[ { "title": "[Spring] Spring6 Http Interface Client - 1", "url": "/posts/devlog-spring6-http-clients-1/", "categories": "DevLog, Spring", "tags": "Java, Kotlin, Spring", "date": "2024-09-13 21:47:00 +0900", "snippet": "1. 어떤 HttpClient을 사용할것인가? Spring을 이용한 Application을 개발할 때, 다른 서버와의 HTTP 통신이 필요할 경우가 많다. 이때 어떤 HttpClient를 사용할지 결정하는 것은 중요한 선택입니다. 다양한 HttpClient 옵션이 있지만, 각 방식마다 장단점이 있다. 여기서는 대표적인 HttpClient들을 비교...", "content": "1. 어떤 HttpClient을 사용할것인가? Spring을 이용한 Application을 개발할 때, 다른 서버와의 HTTP 통신이 필요할 경우가 많다. 이때 어떤 HttpClient를 사용할지 결정하는 것은 중요한 선택입니다. 다양한 HttpClient 옵션이 있지만, 각 방식마다 장단점이 있다. 여기서는 대표적인 HttpClient들을 비교하여 어떤 선택이 가장 적합할지 살펴보자.1.1 순수 HttpClient 라이브러리를 사용하는 방법 순수 HttpClient 라이브러리는 Java 11부터 제공되는 java.net.http.HttpClient를 사용하는 방법이다. 이 라이브러리는 Java 표준 라이브러리로, 외부 라이브러리 의존성 없이 HTTP 요청을 보낼 수 있다. HttpClient는 동기 및 비동기 방식 모두를 지원하며, 간단한 HTTP 통신부터 복잡한 요청까지 다양한 기능을 제공한다.장점 외부 라이브러리 없이 사용 가능 Java 표준 라이브러리의 일환으로 간편한 유지보수 비동기 및 동기 요청 모두 지원단점 Spring과의 통합이 부족 코드가 상대적으로 장황할 수 있음import java.net.URIimport java.net.http.HttpClientimport java.net.http.HttpRequestimport java.net.http.HttpResponsefun httpClientTest() { val client = HttpClient.newBuilder().build() val request = HttpRequest.newBuilder() .uri(URI.create(\"https://www.naver.com\")) .GET() .build() val response: HttpResponse&lt;String&gt; = client.send(request, HttpResponse.BodyHandlers.ofString()) // 추가적으로 Response Model에 대한 Deserialize 필요 logger.info { \"response : ${response.body()}\" }}1.2 RestTemplate RestTemplate는 Spring 3.0에서 처음 도입된 전통적인 HttpClient입니다. 많은 개발자들이 익숙하게 사용해왔으며, 간단한 HTTP 요청을 쉽게 처리할 수 있다. 그러나 RestTemplate는 Spring 5부터는 더 이상 새로운 기능이 추가되지 않고, 유지보수 모드로 전환되었다. 이는 Spring에서 비동기 처리와 반응형 프로그래밍의 중요성이 커지면서, 이를 더 잘 지원하는 클라이언트로의 전환을 권장하고 있기 때문이다.장점 간편한 사용법과 널리 사용된 예제 Spring과의 깊은 통합 동기식 요청에 적합단점 비동기 처리 지원 부족 향후 기능 업데이트 없음fun restTemplateTest() { val restTemplate = RestTemplate() val url = \"https://www.naver.com\" val response = restTemplate.getForEntity(url, String::class.java) if (response.statusCode === HttpStatus.OK) { val responseBody = response.body logger.info { \"response : $responseBody\" } }}1.3 Spring WebClient WebClient는 Spring 5에서 등장한 비동기 및 반응형 HTTP 클라이언트입니다. RestTemplate의 대체로 제안되며, 더 나은 성능과 확장성을 제공한다. WebClient는 비동기 및 블로킹 둘 다 지원하며, 반응형 스트림을 사용해 대용량 데이터를 처리하는데 유리하다.장점 비동기 및 반응형 프로그래밍 지원 다양한 커스터마이징 옵션단점 RestTemplate에 비해 다소 복잡한 설정과 사용법 다른 방식 대비 러닝커브가 존재함.fun webClientTest() { val webClient = WebClient.create(\"https://www.naver.com\") val response = webClient.get() .uri(\"/data\") .retrieve() .bodyToMono(String::class.java) response.subscribe { logger.info { it } }}1.4 Spring Cloud OpenFeign Spring Cloud OpenFeign은 Spring Cloud 프로젝트에서 제공하는 Feign의 확장판이다. Feign은 인터페이스 기반의 선언적 HTTP 클라이언트로, HTTP API 클라이언트를 쉽게 정의할 수 있다.장점: 간결하고 선언적인 HTTP 클라이언트 정의 Spring Cloud와의 통합으로 강력한 기능 제공 Srping MVC의 annotation을 활용 가능단점: Spring Cloud 의존성으로 인해 독립적인 사용은 제한적이다. Spring Cloud OpenFeign은 유지 관리 모드에 있으며 더 이상 적극적으로 개발되지 않으므로 사용을 지양하는 편이 좋다.import org.springframework.cloud.openfeign.FeignClientimport org.springframework.web.bind.annotation.GetMappingimport org.springframework.web.bind.annotation.RestControllerimport org.springframework.beans.factory.annotation.Autowired@FeignClient(name = \"exampleClient\", url = \"https://api.example.com\")interface ExampleClient { @GetMapping(\"/data\") fun getData(): String}@RestControllerclass ExampleController( private val exampleClient: ExampleClient,) { @GetMapping(\"/fetch\") fun fetchData(): String { return exampleClient.getData() }}1.5 Openfeign OpenFeign은 Netflix에서 처음 개발한 인터페이스 기반의 선언적 HTTP 클라이언트 라이브러리이다. OpenFeign은 Java의 인터페이스에 메서드 정의만으로 HTTP 요청을 간결하게 수행할 수 있다. Openfeign은 Spring과는 완벽히 독립적인 라이브러리이다. 과거에는 Non-Blocking 방식을 지원하지 않았는데, 현재는 Reactive Streams Wrapper을 통해 지원한다.장점: 간결하고 선언적인 API 클라이언트 작성 Spring Cloud와의 결합 없이 독립적으로 사용 가능 플러그인 시스템을 통해 다양한 기능 추가 가능단점 Spring Cloud의 부가 기능이 필요할 경우 직접 구현해야 함 Spring Cloud와의 통합 기능은 사용 불가import feign.Feignimport feign.RequestLineinterface ExampleClient { @RequestLine(\"GET /data\") fun getData(): String}val exampleClient = Feign.builder() .target(ExampleClient::class.java, \"https://api.example.com\")val data = exampleClient.getData()println(data)1.6 요약 Spring 기반 Server App에서 다른 서버와 통신하기 위한 다양한 방법을 알아보았다. 위 방법 중 사실 개인적으로 가장 맘에 드는 라이브러리는 1.4 Spring Cloud OpenFeign이다. 하지만 Spring Cloud OpenFeign에는 치명적인 단점이 있다. 첫째, Non-blocking 방식을 지원하지 않는다. 둘쨰, 유지보수 모드에 돌입하여 더이상 유의미한 기능 수정이 이루어지지 않는다는 점이다. 이로 인해 혼동이 가중되었으나, Spring 6가 나오면서 이 부분에 대해 고민이 줄게 되었다. 이제 본격적으로 Spring6 Http Interface Client에 대해 알아보자.2. Spring6 Http Interface Client Spring 6에서 도입된 HTTP Interface는 REST API와의 상호작용을 간소화하기 위한 새로운 기능이다. 이 기능은 REST API를 호출하기 위한 인터페이스 기반의 선언적 방법을 제공한다. 이 기능을 통해 기존의 RestTemplate이나 WebClient보다 간편하게 API 호출을 할 수 있다.2.1 왜 Spring6 Http Interface Client인가? Feign과 같은 선언적 인터페이스 방식으로 관리할 수 있음. ReactorHttpExchangeAdapter 이용해서 구현하는 경우 reactive variants를 지원한다. Spring Framework에서 공식지원 하는 방식이므로, 사용하지 않을 이유가 없다. 아직 초기 단계이므로 그럼에도 미흡한 부분이 있고, 이는 장기적으로 업데이트되거나 Customized를 통해 커버할수 있다. 그럼 본격적으로 사용방법을 알아보도록 하자.2.2 Spring6 Http Interface Client 내부 구조HttpExchangeAdapter spring-web:6.1.11 기준으로 다음과 같은 Adaptor을 제공함. 기존에 사용하던 RestClient, RestTemplate, WebClient와 같은 설정을 토대로 사용이 가능하다.Proxy 객체 생성 방식 개발자는 Spring6 Http Interface의 선언형 방식으로 코드를 정의 내부적으로는 HttpServiceProxyFactory &gt; ProxyFactory &gt; DefaultAopProxyFactory 를 통해 JdkDynamicAopProxy 객체를 생성하여 처리한다. 위 내용을 볼떄 현재는 인터페이스로 정의된 방식만 지원 가능한것으로 보인다. 추후 interface뿐만 아니라 class로써, 특정 API들은 본문을 직접 작성하는 방식을 지원한다거나 하면 그에 맞게 Proxy 방식도 ObjenesisCglibAopProxy 같은 방식을 통해 처리하는것도 가능하지 않을까 예상된다. 3 Spring 6 Http Interface Client 개발하기 Spring6부터는 @HttpExchange 메서드를 사용하여 HTTP 서비스를 인터페이스로 정의할 수 있다. 인퍼테이스를 HttpServiceProxyFactory로 전달하여 RestClient나 WebClient와 같은 HTTP 클라이언트를 통해 요청을 수행하는 프록시를 생성한다.3.1 Https Client Request/Response spring-web에서 제공하는 Annotation을 조합해서 간결하게 ApiClient를 정의할수 있다.Exchange Annotations Exchange 애노테이션은 대체적으로 spring-web과 1:1 맵핑되는 Exchange Annotation을 제공한다. SpringWeb HttpInterface @RequestMapping @HttpExchange @GetMapping @GetExchange @PostMapping @PostExchange @PutMapping @PutExchange @PatchMapping @PatchExchange @DeleteMapping @DeleteExchange Method Parameters 대부분의 spring-web에서 지원하는 annotation을 지원한다. 하나의 Spring Web Controller와 완전히 동일한 방식으로 HttpInterface를 정의할수는 없다. 하지만, 이런부분을 직접 구현함으로써 어느정도 문제를 해결할수는 있다. SpringWeb HttpInterface Description @RequestParam @RequestParam 동일하게 지원 @RequestHeader @RequestHeader 동일하게 지원 @PathVariable @PathVariable 동일하게 지원 @RequestBody @RequestBody 동일하게 지원 @CookieValue @CookieValue 동일하게 지원 @RequestAttribute @RequestAttribute webClient httpService에서만 지원 @RequestPart @RequestPart 동일하게 지원 MultipartFile MultipartFile @RequestPart로 전달하여 동일하게 사용 가능. @ModelAttribute - 지원하지 않음. Return Values 일반적인 Blocking I/O에 대한 응답을 지원한다. HttpHeader를 받는다거나 HttpStatus 및 Header를 포함한 ResponseEntity로 반환받는것을 지원한다. webClient 기반인 경우 Webflux의 Mono나 Flux 도 함께 지원한다. ReturnType Void / Unit HttpHeaders &lt;T&gt; ResponseEntity&lt;Unit&gt; ResponseEntity&lt;T&gt; ResponseEntity&lt;T&gt; Mono&lt;Unit&gt; Mono&lt;HttpHeaders&gt; Mono&lt;T&gt; Flux&lt;T&gt; Mono&lt;ResponseEntity&lt;Unit&gt;&gt; Mono&lt;ResponseEntity&lt;T&gt;&gt; Mono&lt;ResponseEntity&lt;Flux&lt;T&gt;&gt; 3.2 Http Interface Client 정의 HttpInterfaceClient의 Request/Response를 어떻게 정의할지 알았으니 이제 샘플 코드를 살펴보자. User(사용자) Resource 에 대한 REST API가 있다고 하고, 이를 호출하기 위한 HttpClient 코드를 정의하면 다음과 같이 정의할수 있다.package com.starter.core.clients.internal.admin.apiimport com.starter.core.models.user.UserCreateRequestimport com.starter.core.models.user.UserPatchRequestimport com.starter.core.models.user.UserResponseimport com.starter.core.models.user.UserSearchRequestimport org.springframework.web.bind.annotation.ModelAttributeimport org.springframework.web.bind.annotation.PathVariableimport org.springframework.web.bind.annotation.RequestBodyimport org.springframework.web.service.annotation.DeleteExchangeimport org.springframework.web.service.annotation.GetExchangeimport org.springframework.web.service.annotation.HttpExchangeimport org.springframework.web.service.annotation.PatchExchangeimport org.springframework.web.service.annotation.PostExchange@HttpExchange(\"/api/v1/users\")interface UserApiClient : UserApi { @PostExchange override fun create(@RequestBody request: UserCreateRequest): UserResponse @GetExchange override fun getUsers(@ModelAttribute request: UserSearchRequest) : List&lt;UserResponse&gt; @GetExchange(\"/{uuid}\") override fun getUser(@PathVariable uuid: String): UserResponse @PatchExchange(\"/{uuid}\") override fun patchUser( @PathVariable uuid: String, @RequestBody request: UserPatchRequest ) @DeleteExchange(\"/{uuid}\") override fun deleteUser(@PathVariable uuid: String)} 구현코드를 보면 feign과 유사한 형태로 간결하게 작성이 가능한것을 알 수 있다. 다만, 위에서 말한것처럼 webClient 기반으로 정의할 경우 webflux 스펙도 모두 지원하므로 훨씬 더 좋다고 볼수 있다. 사실 글을 작성하는 현재 시점에는 Spring Cloud OpenFeign 말고, 순수 Openfeign을 사용할 경우 reactiveClient로 정의할수 있는 feature가 추가된것으로 보이기는 하나, 나라면 spring 공식지원되는 spring6HttpInterface를 쓸것 같다. 3.3 Http Interface Client ConfigurationHttpServiceProxyFactory 2.3.2에서 HttpInterface를 정의했다면 httpServiceProxyFactory를 통해 Proxy 객체를 만들어 주어야 한다. 공통적으로 사용할수 있도록 Util method를 정의하면 아래와 같이 만들수 있다. HttpServiceProxyFactory Builder에는 더 많은 옵션이 있지만, 내가 필요했던건 customArgumentResolver를 등록하는것뿐이라 아래와 같이 구성했다. 위 Request 정의시 ModelAttribute를 지원하지 않으므로 이를 처리할수 있도록 직접 구현한것으로 보면 된다. import com.starter.core.clients.common.resolver.ModelAttributeArgumentResolverimport org.springframework.web.reactive.function.client.WebClientimport org.springframework.web.reactive.function.client.support.WebClientAdapterimport org.springframework.web.service.invoker.HttpServiceProxyFactoryobject HttpInterfaceProxyFactory { inline fun &lt;reified T&gt; create( webClient: WebClient, resolvers: List&lt;ModelAttributeArgumentResolver&gt; = listOf(ModelAttributeArgumentResolver.DEFAULT_INSTANCE), ): T { val builder = HttpServiceProxyFactory .builderFor(WebClientAdapter.create(webClient)) resolvers.forEach { builder.customArgumentResolver(it) } val httpServiceProxyFactory = builder.build() return httpServiceProxyFactory.createClient(T::class.java) }}Bean Configuration Config Class를 별도로 정의하고, 이를 필요한 곳에서 Import해서 쓸수 있는 방식으로 정의해보았다. HttpInterfaceProxyFactory Util 메소드를 통해 Instance를 만들고 이를 Bean으로 등록해서 사용한다. webClient는 하나의 API서버군을 바라보고 할수 있도록 정의하고, 이를 HttpClient들이 주입받아 Bean을 등록하도록 정의한다. 이렇게 하면 Controller와 HttpClient를 1:1로 관리할수 있어서 코드를 깔끔하게 유지할수 있다. import com.starter.core.clients.common.HttpInterfaceProxyFactoryimport com.starter.core.clients.common.WebClientFactoryimport com.starter.core.clients.internal.InternalClientsPropertiesimport com.starter.core.clients.internal.file.api.FileDownloadApiClientimport com.starter.core.clients.internal.file.api.FileUploadApiClientimport org.springframework.beans.factory.annotation.Qualifierimport org.springframework.boot.context.properties.ConfigurationPropertiesScanimport org.springframework.context.annotation.Beanimport org.springframework.context.annotation.ComponentScanimport org.springframework.context.annotation.Configurationimport org.springframework.web.reactive.function.client.WebClient@Configuration@ConfigurationPropertiesScan(\"com.starter.core.clients.internal.file\")@ComponentScan(\"com.starter.core.clients.internal.file\")class FileClientConfig( private val internalClientsProperties: InternalClientsProperties) { companion object { private const val FILE_WEB_CLIENT = \"fileWebClient\" } @Bean(FILE_WEB_CLIENT) fun fileWebClient(): WebClient { return WebClientFactory .createNettyClient(baseUrl = internalClientsProperties.file) } @Bean fun fileDownloadApiClient(@Qualifier(FILE_WEB_CLIENT) webClient: WebClient): FileDownloadApiClient { return HttpInterfaceProxyFactory.create&lt;FileDownloadApiClient&gt;(webClient) } @Bean fun fileUploadApiClient(@Qualifier(FILE_WEB_CLIENT) webClient: WebClient): FileUploadApiClient { return HttpInterfaceProxyFactory.create&lt;FileUploadApiClient&gt;(webClient) }}3.4 Http Interface CustomizeApi Interface로 Controller와 1:1로 관리하기 내가 만들어둔 프로젝트 아키텍쳐에서는 특정 Client의 API를 담당하는 BFF 모듈을 별도로 만드는것을 목표로 하고 있는데, 이 경우 BFF에서 내부 모듈들의 API를 호출하고, 이를 aggregation하는등의 관리가 필요하다. 이때 Controller와 HttpInterface가 동일한 API임을 나타내기 위해 Api Interface를 정의하고 이를 Controller와 HttpInterface가 구현하도록 구성했다.// FileUploadApi.ktinterface FileUploadApi { fun uploadFile(fileUploadRequest: FileUploadRequest): FileResponse}// FileUploadRequest.ktdata class FileUploadRequest( val fileName: String, override val file: MultipartFile, override val fileType: FileType, override val description: String?, override val uploadDirectoryType: UploadDirectoryType) : FileUploadRequestInterface// FileUploadController.ktpackage com.starter.file.app.file.controllerimport com.starter.core.clients.internal.file.api.FileUploadApiimport com.starter.core.models.file.FileResponseimport com.starter.core.models.file.FileUploadRequestimport com.starter.file.app.file.facade.FileUploadFacadeimport io.swagger.v3.oas.annotations.Operationimport io.swagger.v3.oas.annotations.Parameterimport org.springframework.http.MediaTypeimport org.springframework.web.bind.annotation.PostMappingimport org.springframework.web.bind.annotation.RequestMappingimport org.springframework.web.bind.annotation.RestController@RestController@RequestMapping(\"/api/v1/files/upload\")class FileUploadController( private val fileUploadFacade: FileUploadFacade,) : FileUploadApi { @PostMapping(consumes = [MediaType.MULTIPART_FORM_DATA_VALUE] ) @Operation(description = \"파일 업로드 API\") override fun uploadFile(@Parameter fileUploadRequest: FileUploadRequest): FileResponse { val fileResponse = fileUploadFacade.uploadFile(fileUploadRequest) return fileResponse }}// FileUploadApiClient.kt@HttpExchange(\"/api/v1/files/upload\")interface FileUploadApiClient : FileUploadApi { @PostExchange(contentType = MediaType.MULTIPART_FORM_DATA_VALUE) override fun uploadFile(@ModelAttribute fileUploadRequest: FileUploadRequest): FileResponse} 이런식으로 관리하면 code trace시에도 좀 더 편하게 코드를 추적할 수 있다. 하지만, HttpInterface에서 지원하는 스펙의 한계로 별도의 customize 없이는 모든 케이스를 이렇게 관리할 수 없다.ModelAttributeArgumentResolver 위 코드를 보면 Controller에서는 파일 업로드를 위해 MultipartFile를 포함한 Model를 받고 있다. 별도의 구현이 없다면 HttpInterface에서는 @RequestPart 를 통해 파일을 전달해주어야 한다. multipart/form-data 외에도 RequestParamter가 많은 경우 @RequestParam 을 나열하는 경우를 방지하기 위해 하나의 Request Model Class를 선언하기도 하는데, 이부분에서도 지원하지 않는다. 위의 문제 해결을 위해 Custom ArgumentResolver를 구현해보자.package com.starter.core.clients.common.resolverimport com.starter.core.common.utils.JsonUtilimport com.starter.core.common.utils.ReflectionUtilsimport mu.KLoggingimport org.springframework.core.MethodParameterimport org.springframework.core.io.Resourceimport org.springframework.http.HttpEntityimport org.springframework.http.HttpHeadersimport org.springframework.lang.Nullableimport org.springframework.web.bind.annotation.ModelAttributeimport org.springframework.web.multipart.MultipartFileimport org.springframework.web.service.invoker.HttpRequestValuesimport org.springframework.web.service.invoker.HttpServiceArgumentResolverimport kotlin.reflect.KFunction1class ModelAttributeArgumentResolver( val argumentToMapFunc: KFunction1&lt;Any, Map&lt;String, Any?&gt;&gt; = ReflectionUtils::objectToMap,) : HttpServiceArgumentResolver { companion object : KLogging() { val DEFAULT_INSTANCE = ModelAttributeArgumentResolver() val INSTANCE_WITH_JSON = ModelAttributeArgumentResolver( JsonUtil::toMap ) } override fun resolve( argument: Any?, parameter: MethodParameter, requestValues: HttpRequestValues.Builder ): Boolean { parameter.getParameterAnnotation(ModelAttribute::class.java) ?: return false if(argument == null) { return false } val argumentMap = argumentToMapFunc(argument) val isPartRequest = isPartRequest(argumentMap) logger.debug { \"argumentMap: $argumentMap, isPartRequest: $isPartRequest\" } argumentMap .filter { it.value != null } .forEach { if(isPartRequest) { addRequestPartValues(name = it.key, value = it.value!!, requestValues = requestValues) } else { addRequestParamValues(name = it.key, value = it.value!!, requestValues = requestValues) } } return true } @Nullable private fun isPartRequest(argumentMap: Map&lt;String,Any?&gt;): Boolean { return argumentMap.values.any { it is MultipartFile } } private fun addRequestParamValues(name: String, value: Any, requestValues: HttpRequestValues.Builder) { if(value is Collection&lt;*&gt;) { requestValues.addRequestParameter(name, value.joinToString()) } else { requestValues.addRequestParameter(name, value.toString()) } } private fun addRequestPartValues(name: String, value: Any, requestValues: HttpRequestValues.Builder) { if(value is Collection&lt;*&gt;) { requestValues.addRequestPart(name, value.joinToString()) } else if (value is MultipartFile) { val value = toMultipartFileHttpEntity(name, value) requestValues.addRequestPart(name, value) } else { requestValues.addRequestPart(name, value.toString()) } } private fun toMultipartFileHttpEntity(name: String, multipartFile: MultipartFile): HttpEntity&lt;Resource&gt; { val headers = HttpHeaders() if (multipartFile.originalFilename != null) { headers.setContentDispositionFormData(name, multipartFile.originalFilename) } if (multipartFile.contentType != null) { headers.add(HttpHeaders.CONTENT_TYPE, multipartFile.contentType) } return HttpEntity&lt;Resource&gt;(multipartFile.resource, headers) }} 위 구현코드는 @ModelAttribute 애노테이션이 달린 class에 대해서 Controller와 동일하게 동작할수 있도록 직접 구현한 코드이다. 현재는 이에 대한 공식적인 ArgumentResolver가 없기 떄문이 이를 등록해서 사용할수 있다. 만약 공식 지원이 된다면 그때 삭제하거나 ModelAttribute에 대해 지원을 하지만, 의도와 다른 게 있다면 별도의 annotaiton으로 정의해서 사용할수 있을것이다. argument을 request에 넣어주기 위해 map으로 변환한 뒤 체크를 하는데, 필요에 따라 reflection 혹은 json 방식으로 사용할 수 있다. 4. 결론 Spring 기반 웹 애플리케이션을 개발할때 필연적으로 API를 호출하기 위한 HttpClient가 필요하다. 이 떄, 우리는 무엇을 어떻게 쓸지를 고민하는데 Spring 6 이후부터는 큰 고민 없이 Http Interface Client 사용을 고려할 수 있을것 같다. 하지만, 위에서 Customize 한 것 이외에 부가적인 문제 해결이 필요한 부분이 아직 남아있는데, 이 부분은 이후 포스팅을 통해 공유하려고 한다. 관련 코드는 starter/1.240914 에서 확인할수 있다.Refernece Spring Http Interface Client 공식문서 Spring Cloud OpenFeign OpenFeign" }, { "title": "[엘라스틱서치 바이블] 8. 엘라스틱서치의 내부 동작 상세", "url": "/posts/es-bible-8/", "categories": "DevLog, Elasticsearch", "tags": "Elasticsearch", "date": "2024-02-03 21:45:00 +0900", "snippet": "[엘라스틱서치 바이블] 8. 엘라스틱서치의 내부 동작 상세 ES의 기본 구조와 핵심 동작 양상, 주요 기능, 서비스 환경을 구축하고 운영하며 장애에 대응하는 방법을 학습한것만으로도 ES를 활용한 개발에 큰 어려움이 없을것이다. 다만, 고도화가 필요하다면, 내부 동작을 구체적으로 이해하는것이 중요하다.8.1 엘라스틱서치의 데이터 분산 처리 과정 데...", "content": "[엘라스틱서치 바이블] 8. 엘라스틱서치의 내부 동작 상세 ES의 기본 구조와 핵심 동작 양상, 주요 기능, 서비스 환경을 구축하고 운영하며 장애에 대응하는 방법을 학습한것만으로도 ES를 활용한 개발에 큰 어려움이 없을것이다. 다만, 고도화가 필요하다면, 내부 동작을 구체적으로 이해하는것이 중요하다.8.1 엘라스틱서치의 데이터 분산 처리 과정 데이터 읽기와 쓰기 작업 요청이 들어왔을떄 ES 내부가 어떤 단계를 거쳐 동작하는지 살펴본다.8.1.1 쓰기 작업 시 엘라스틱서치 동작과 동시성 제어 쓰기 작업은 조정 단계(coordination stage), 주 샤드 단계(primary stage), 복제 단계(replica stage)의 3단계로 수행된다.조정 단계 먼저 라우팅을 통해 어느 샤드에 작업해야 할지 파악하고, 적절한 주 샤드를 찾아 요청을 넘겨준다.주 샤드 단계 주 샤드에 작업 요청이 넘어오면 요청이 문제 없는지 검증하고, 쓰기 작업을 수행한다. 작업이 완료되면 각 본제본 샤드로 요청을 넘긴다.복제 단계 마스터 노드는 작업을 복제받을 샤드 목록을 관리하는데 이를 in-sync 복제본이라 하고, 이 샤드는 주 샤드에게 받은 요청을 로컬에서 수행하고, 주 샤드에게 작업이 완료됐음을 보고하는데 이 과정을 복제 단계라고 한다.각 단계의 종료 시점 각 단계의 실행은 조정 -&gt; 주 샤드 -&gt; 복제 단계로 실행되지만 종료는 역순이다. 복제 단계가 완료되어 주 샤드에게 응답을 모두 보내야 주 샤드 단계가 종료된다. 조정 단계도 주 샤드 단계에서 최초 요청을 받아 전달했던 노드에게 작업 완료결과를 보내야 조정 단계가 종료된다.쓰기 작업의 전체 흐름낙관적 동시성 제어 주 샤드 단계에서 작업을 각 복제본 샤드에 병렬적으로 보내는데, 이때 여러 작업을 병렬적으로 보내면 메시지 순서의 역전이 일어날 수 있다. ES에서는 낙관적 동시성 제어를 통해 이를 처리한다. 변경 내용이 복제본 샤드에 완전히 적용되기 전에 다른 클라이언트로부터 주 샤드에 같은 문서의 필드 값을 변경하는 요청이 발생하면 분산시스템 특성상 어떤 요청이 먼저 본제본 샤드로 들어올지는 보장할 수 없다. ES에서는 이러한 동시성 문제를 방지하기 위해 _seq_no가 존재한다. _seq_no는 각 주 샤드마다 들고 있는 시퀀스 숫자값이며, 매 작업마다 1씩 증가한다. ES에서 문서를 색인할 때 이 값을 함게 저장하는데, eS에서는 이 값을 역전 시키는 변경을 허용하지 않음으로써 요청 순서의 역전 적용을 방지한다. 주 샤드를 들고 있는 노드에 문제가 발생하여 해당 노드가 클러스터에서 빠지는 경우에 대한 처리를 위한 방안도 존재한다. _primary_term이라는 이전 주샤드에서 수행했던 작업과 새로 임명된 주 샤드에서 수행했던 작업을 구분하기 위한 값이 존재하고, 주 샤드가 새로 지정될 때 1씩 값을 증가시킨다. ES에서는 이러한 방법으로 낙관적 동시성 제어(optimistic concurrency control)을 수행한다._seq_no와 primary_term이 배정되는 과정### 인덱스 생성PUT /concurrency_testHost: localhost:9200Content-Type: application/json{ \"settings\": { \"number_of_shards\": 2 }}### 문서 색인PUT /concurrency_test/_doc/1Host: localhost:9200Content-Type: application/json{ \"views\": 1}{ \"_index\": \"concurrency_test\", \"_id\": \"2\", \"_version\": 1, \"result\": \"created\", \"_shards\": {\t\"total\": 2,\t\"successful\": 1,\t\"failed\": 0 }, \"_seq_no\": 0, \"_primary_term\": 1} _seq_no는 주 샤드마다 따로 매긴다. 데이터가 다른 샤드에 색인되면 _seq_no는 0부터 다시 매기는것을 확인할 수 있다. 로컬에서 테스트할때는 모두 같은 샤드에 색인되는것까지만 확인하긴 했음. if_primary_term=, if_seq_noPUT /concurrency_test/_doc/1?if_primary_term=1&amp;if_seq_no=1Host: localhost:9200Content-Type: application/json{ \"views\": 3} 문서 색인시 지정한 값과 같은 seq_no, primary_term인 경우에만 색인 작업이 실행되도록 할 수 있다.버전(_version) 위 색인 테스트에서 _version이라는 값이 증가하는 것도 확인 할 수 있는데, 이는 _seq_no와 _primary_term 처럼 동시성을 제어하기 위한 메타데이터로 모든 무서마다 붙는다. 기본적으로 1부터 시작해서 업데이트나 삭제 작업을 수행하면 1씩 증가한다. _seq_no나 _primary_term 과 다른 점은 클라이언트가 문서의 _version 값을 직접 지정할 수 있다는 점이다. version_type을 external / external_gte로 설정하면 클라이언트가 직접 _version을 지정해 색인할 수 있다. ### 버전PUT /concurrency_test/_doc/10Host: localhost:9200Content-Type: application/json{ \"views\": 0}### 버전 external 지정PUT /concurrency_test/_doc/10?version=15&amp;version_type=externalHost: localhost:9200Content-Type: application/json{ \"views\": 1}{ \"_index\": \"concurrency_test\", \"_id\": \"10\", \"_version\": 15, \"result\": \"updated\", \"_shards\": {\t\"total\": 2,\t\"successful\": 2,\t\"failed\": 0 }, \"_seq_no\": 10, \"_primary_term\": 3} version은 long 범위 이내 정수로 지정할 수 있다. 마지막 버전보다 낮은 값으로 재색인하려는 경우 version_conflict_engine_exception 이 발생한다. version_type은 internal / external(external_gt) / external_gte가 있다. internal은 별도로 지정하지 않고 ES에서 자동으로 매겨주는 타입이다. external은 기존 문서의 버전보다 더 클 떄에만 색인이 수행된다. external_gte는 기존 문서의 버전보다 새 문서의 버전이 더 크거나 같을 떄에만 색인이 수행된다. 8.1.2 읽기 작업시 엘라스틱서치 동작 클라이언트로부터 읽기 작업을 요청받은 조정도느는 라우팅을 통해 적절한 샤드를 찾아 요청을 넘긴다. 쓰기와 다르게 주 샤드가 아니라 복제본 샤드로 요청이 넘어갈 수 있다. 요청을 넘겨받은 각 샤드는 로컬에서 읽기 작업을 수행한 뒤 그 결과를 조정 노드로 돌려준다. 조정 노드는 이 결과를 모아서 클라이언트에 응답한다. 이때, 주 샤드에는 색인이 완료됐지만, 특정 본제본에는 반영이 완료되지 않은 상탤태로 데이터를 읽을 수 있다는 점을 염두에 둬야 한다.8.1.3 체크포인트와 샤드 복구 과정 문제가 생겨서 특정 노드가 재기동되었다면 그 노드가 들고 있던 샤드에 복구 작업이 진행되는데, 이 과정에서 복구 중인 샤드가 현재 주 샤드의 내용과 일치하는지를 파악할 필요가 있다. 재기동되는 동안 주 샤드에 색인 작업이 있었다면, 그 내용을 복구중인 샤드에도 반영해야 할 것이다. 문제가 발생했던 노드가 주 샤드를 들고 있었다면 다운타임 중에 선출된 주 샤드가 미처 반영하지 못한 작업 내용이 복구 중인 샤드에 포함돼 있을수도 있다. ES에서는 이런 부분을 고려하여 샤드 복구를 수행한다. 수행 과정과 체크포인트 업데이트 흐름 ES에서는 앞서 다루었던 _primary_term과 _seq_no 을 조합하면 샤드와 샤드 사이에 어떤 반영 차이점이 있는지 알 수 있다. 각 샤드는 로컬에 작업을 수행하고 나면 몇번 작업까지 순차적으로 빠짐없이 수행을 완료했는지를 로컬 체크포인트로 기록한다. 복제본 샤드는 로컬 체크포인트 값이 업데이트되면 이를 주 샤드에 보고한다. 주 샤드는 갹 복제본 샤드로부터 받은 로컬 체크포인트를 비교하여 가장 낮은 값을 글로벌 체크포인트 값으로 기록하고, 체크포인트를 비교하여 가장 낮은 값을 글로벌 체크 포인트 값으로 기록한다.복구 방식 문제가 발생해 샤드를 복구해야 할 경우 ES는 샤드 간의 글로벌 체크포인트를 비교한다. 주 샤드와 보제본 샤드의 글로벌 체크포인트가 같다면 이 샤드는 추가 복구 작업이 필요없다. 글로벌 체크 포인트가 차이가 난다면 두 샤드 간 체크포인트를 확인해서 필요한 작업만 재처리하여 복구한다. 재처리할 내용을 추적하는 매커니즘을 ES에서는 샤드 이력 보존(shard history retention leases)라고 부른다. index.soft_deletes.retention_lease.period 기간을 설정하여 보존 기간을 관리할수 있고, default 12시간이다. 이 기간을 넘겨서 샤드 이력이 만료된 이후 수행되는 복구 작업에는 작업 재처리를 이용하지 않고, 세그먼트 파일을 통째로 복사하는 방법을 사용한다. 8.2 엘라스틱서치의 검색 동작 상세 ES 레벨에서 어떻게 검색을 수행하는지 각 샤드 레벨, 즉 루씬 레벨에서는 매칭되는 문서를 어떻게 판정하고 점수를 계산하는지, 캐시는 어떻게 동작하는지 그 내부를 깊게 살펴보자.8.2.1 엘라스틱서치 검색 동작 흐름ES 검색 동작 흐름 개괄 검색 뿐만 아니라 다른 API도 아래와 같은 기본적인 흐름을가지고 있음. RestSearchAciton : REST 요청을 ES 내부에서 사용할 요청으로 변경하여 다음 단계로 요청하는 과정 TransportSearchAction : 검색 요청과 현재 클러스터 상태를 분석해 상황에 맞는 적절한 검색 방법과 대상을 확정하고 검색 작업을 다음 단계로 넘기는 과정 Query Phase : 쿼리에 매칭되는 상위 문서들을 각 샤드들이 판단하여 조정 노드로 반환하는 과정 Fetch Phase : 각 샤드의 검색 결과를 받아 fetch를 수행할 문서를 확정하는 과정ES 검색 동작 흐름 상세TransportSearchAction 다양한 방법을 통해 여러 샤드 중에서 어떤 샤드에 우선해서 요청을 보낼것인지 순서를 확정한다. 우선순위가 가장 높은 샤드 하나에 요청을 보냈을 때 실패할 수도 있으므로 요청 순서 목록을 작성한다. (ShardIterator) 어떤 노드와 샤드를 우선해서 검색할 것인지 지정하는 매개변수 preference 값을 먼저 읽고 처리한다. preference가 지정되지 않았다면 ES는 해당 샤드를 가진 노드 중 적절한 노드를 선정한다. 조정 노드에서 보낸 이전 요청에 대한 응답 속도나 이전 검색 요청에서 시간이 얼마나 소요됐는지 등 그간의 통계 데이터와 현재 검색 스레드 풀 상황이 어떠한지를 고려해 가장 응답을 빨리 돌려줄것으로 예상되는 노드를 보통 선정한다. 만약 pit가 지정됐다면 처음부터 pit 값으로 검색 문맥을 가져오고 거기에서 검색 대상 인덱스와 샤드를 가져온다.preference preference 관련 트래블슈팅 사례 _only_local : 로컬에 들고 있는 샤드만을 대상으로 검색을 수행한다. _local : 가능하면 로컬에 들고 있는 샤드를 우선으로 검색을 수행한다. 불가능하다면 다른 노드의 샤드를 대상으로 지정한다. _only_nodes:&lt;node-id&gt;, &lt;node-id&gt; : 지정한 노드 ID가 들고 있는 샤드를 대상으로만 검색을 수행한다. _prefer_nodes:&lt;node-id&gt;,&lt;node-id&gt; : 지정한 노드 ID가 들고 있는 샤드를 우선해서 검색한다. _shards:&lt;shard&gt;, &lt;shard&gt; : 샤드 번호를 직접 지정해 해당 샤드 대상으로만 검색을 수행한다. custom-string : _로 시작하지 않는 문자열 지정으로, 지정한 문자열의 해시 값으로 노드 우선순위를 지정한다. 같은 노드가 요청을 받도록 해서 캐시를 최대한 활용하고자 할 떄 이방법을 사용한다.CanMatchPreFilterSearchPhase 검색 요청의 search_type이 query_then_fetch이고, 몇몇 특정 조건을 만족하면 ES는 본격적인 검색 작업에 들어가기 전에 이 과정을 거치며 몇가지 최적화를 수행한다. 검색 대상의 샤드 수가 128개를 초과하거나 검색 대상이 읽기 전용 인덱스를 포함하거나, 첫번째 정렬 기준에 색인된 필드가 지정된 경우 수행된다. 검색 대상 샤드에서 주어진 쿼리로 단 하나의 문서라도 매치될 가능성이 있는지 사전에 점검을 수행해 확실히 검색 대상이 될 필요가 없는 샤드를 사전에 제거한다. 인덱스의 메타데이터를 이용해 타임스탬프 필드 범위상 매치되는 문서가 확실히 없는지를 체크한다거나 색인된 필드가 지정되어 있으면 각 샤드의 최솟값과 최대값을 가지고 샤드를 정렬해 상위에 올라올 문서를 보유한 샤드가 먼저 수행되도록 최적화하는 등 다양한 방법이 사용된다.AbstractSearchAsyncAction search_type의 기본값인 query_then_fetch로 수행하는 경우 각 샤드에서 검색 쿼리를 수행하고, 매치된 상위 문서를 수집할 때 유사도 점수 계산을 끝내는 가장 일반적인 형태의 검색인데, 이 경우 이 단계에서 전체적인 흐름을 제어한다.(SeachQueryThenFetchAsyncAction) search_type을 dfs_query_then_fetch로 지정하면 모든 샤드로부터 사전에 추가 정보를 모아 정확한 유사도 점수를 계산한다. 정확도는 올라가지만 성능은떨어진다.(SearchDfsQueryThenFetchAsyncAction) dfs : distributed frequency search SearchPhase-조정 노드에서 수행하는 작업과 각 샤드 레벨에서 수행하는 작업을 구분해서 봐야 한다. QueryPhase, DfsPhase, FetchPhase 등은 각 샤드에서 수행되고 나머지는 조정노드에서 수행된다.SearchDfsQueryThenFetchAsyncAction 점수 계산에 사용할 추가 정보를 각 샤드에서 가져오기 위해 샤드별 요청을 만들어서 분산 전송한다. 이 과정에서 각 샤드는 검색 쿼리에 매치되는 텀이나 문서 빈도(document frequency), 여러 통계 데이터 등을 구해 반환한다. 이후 조정노드에서 DfsQueryPhase를 수행한다.DfsQueryPhase 각 샤드에서 보낸 DfsPhase 작업 결과로부터 샤드별 본 검색 요청을 만들어 다시 각 노드로 분산 전송한다. QueryPhase에서 본격적인 쿼리 매치 작업을 수행하고 이 작업 결과를 수신하면 FetchSearchPhase로 넘어간다.SearchQueryThenFetchAsyncAction query_then_fetch search_type인 경우 사전 작업 없이 바로 샤드별 검색 요청을 만들어 전송한다. 샤드별 결과를 수신하면 FetchSearchPhase로 넘어간다.QueryPhase 이 단계에서는 샤드 요청이 캐시 가능한 요청인지 확인하고, 캐시 가능한 요청이라면 캐시에서 값을 불러와 바로 응답을 반환한다.(샤드 레벨 저장 캐시) 캐시에 값이 없다면 QueryPhase의 주 작업을 수행하고 캐시에 결과를 저장한다. QueryPhase에서는 크게 검색, 제안(suggest), 집계의 세 작업을 수행한다. 제안(suggest) 작업은 오타 교정이나 자동완성 등에 사용하는 기능이다.FetchSearchPhase와 FetchPhase 각 샤드가 수행한 QueryPhase 작업의 결과가 조정 노드에 모이면 FetchSearchPhase로 넘어간다. FetchSearchPhase 단계에서는 각 샤드에 요청할 fetch 요청을 생성해 분산 전송한다. FetchPhase에서는 요청에 지정한 번호의 문서의 내용을 실제로 읽어서 반환한다. 각 샤드의 FetchPhase의 작업 결과가 조정 노드에 모이면 ExpandSearchPhase로 넘어간다.FetchSubPhase FetchSubPhase는 문서 내용을 읽어 SearchHit을 만드는 과정에서 수행하는 여러 하위 작업을 처리하는 단계이다. 커스텀 플러그인을 통해 직접 만든 FetchSubPhase를 등록할 수도 있다. _soruce를 읽거나 _score을 다시 계산하거나, 검색 수행 중간 과정과 부분 유사도 점수를 상세 설명하는 _explanation을 만드는 과정들이 포함된다.ExpandSearchPhase collapse를 이용한 검색 축소 ExpandSearchPhase에서는 필드 collapse을 수행한다. 이는 지정한 필드 값을 기준으로 검색 결과를 그룹으로 묶은 뒤 그 안에서 다른 기준으로 상위 문서를 지정한 개수만큼 뽑을 떄 사용하는 특수한 기능이다. 필드 collapse을 수행하기 위해 본 검색의 hit 수만큼 새 검색 요청을 만들어서 로컬에 전송하고, 조정 노드 자신이 이를 받는다. 이 과정을 마무리하면 결과를 모아서 최종 검색 결과를 만들어 반환하고 검색 작업을 조욜한다. 필드 collapse을 사용하지 않는다면 바로 응답을 반환한다. 8.2.2 루씬 쿼리의 매칭과 스코어링 과정 위에서 학습한 QueryPhase에서 쿼리에 매칭되는 상위 문서를 수집하는 작업은 각 샤드 레벨, 즉 루씬 레벨에서 수행하는 작업이다. 루씬이 어떻게 검색 작업을 수행하는지 흐름을 이해하면 어떤 쿼리가 무거운 쿼리이고 문제가 되는 경우를 이해하기 쉽다.IndexSearcher IndexSearcher 는 루씬 인덱스 내의 문서를 검색할 때 사용하는 클래스QueryBuilder ES 레벨의 쿼리를 정의하는 인터페이스로 쿼리 이름, DSL 파싱, 직렬화 여부 등을 정의한다.Query 루씬 쿼리를 정의하는 추상클래스Weight Query 내부 동작을 구현하는 추상클래스로, IndexSearcher에 의존ㄴ성 있는 작업이나 상태를 담당한다.Scorer 유사도 점수 계싼을 담당하는 추상 클래스로BulkScorer 여러 문서를 대상으로 한 번에 유사도 점수를 계산하는 추상 클래스 Weight의 BulkScorer 메소드를 따로 오버라이드하지 않았다면 기본 구현인 DefaultBulkScorer 클래스를 반환하여 사용한다.DocIdSetIterator ocIdSetIterator는 매치된 문서의 순회를 담당하는 추상 클래스이다. hello 라는 질의어로 term 쿼리를 수행했다고 가정해보면 루씬은 해당 필드의 역색인을 읽어 어떤 문서가 hello term을 가지고 있는지 목록을 불러오고, 이 목록을 DocIdSetIterator로 순회한다. 일반적인 경우는 next로 순회할 수 있으나, 일부 문서들을 수집 후보에서 건너뛸 수 있다거나 한 경우 advance(target)을 호출하여 순회한다.TwoPhaseIterator 매치 여부를 판단하는 작업이 무거운 쿼리의 매치 작업을 두 개 페이즈로 나누어 진행하도록 하는 추상 클래스이다. 비용이 저렴한 간략한 매치를 먼저 수행해 후보를 좁히고 난 뒤 문서 수집 과정에서 최종 매치를 수행한다.Collector, LeaftCollector Collector는 검색 결과를 수집하는 동작을 정의하는 인터페이스이다. 유사도 점수나 정렬 기준 등을 계산하거나 확인하며, 상위 결과를 수집하는 동작 등을 수행한다.bool 쿼리의 검색 동작 순서와 DocIdSetIterator 순회 bool 쿼리는 must, filter, must_not, should 절 하위에 다양한 쿼리르 갖고 있는데, 이 중 어떤 쿼리를 먼저 수행된다는 알기 쉬운 정해진 규칙이 없다. 이를 루씬의 주요 동작과 연결시켜 상세히 알아보자.rewrite, cost ES는 검색 요청을 받으면 내부적으로 쿼리를 루씬의 여러 쿼리로 쪼갠 뒤 조합해 재작성한다.(Query.rewrite()) 그 뒤 쪼개진 각 쿼리를 수행시 얼마나 비용이 소모되는지 내부적으로 추정한다. 비용의 효과를 추정하고 유리할 것으로 생각되는 부분을 먼저 수행한다. (DocIdSetIterator.cost()) bool 쿼리를 담당하는 BooleanWeight는 쿼리의 세부 내용에 따라 다양한 종류의 최적화된 Scorer를 만들어서 반환한다.conjunction 검색과 DocIdSetIterator 순회 conjunction 검색은 AND 성격의 검색으로 주어진 쿼리의 매치 조건을 모두 만족해야 최종 매치된 것으로 판정한다. disjunction 검색은 OR 성격의 검색으로 주어진 쿼리의 매치 조건 중 하나만 만족해도 최종 매치된 것으로 판단한다. 위 2가지 방식 모두 하위 Query마다 Weight에 이어 Scorer를 미리 만들어 둔 다음 하위 Scorer들을 가지고 최종 Scorer를 만든다. conjunction 방식에서는 각 항목에 대한 DocIdSetIterator을 뽑고, cost순으로 정렬 한 뒤 제일 작은 cost를 가진 Iterator순으로 순회한다. 첫번쨰 항목에 대한것을 수행하고, doc을 기준으로 다음것을 수행하여 두번쨰 항목에서도 가리키는지를 확인하고, 모든 항목에 대해서 같은 doc을 바라보는지 확인하여 모두 일치하다면 매치된것으로 판단한다. disjunction 검색과 건너 뛰기 disjunction 검색도 하위 Scorer와 Iterator을 이용해 최상위 레벨의 Scorer와 Iterator를 생성한다. OR 검색이므로 개념적으로 결과 중 최솟값을 반환한다. 구현은 하위 Iterator를 현재 doc ID를 기준으로 하는 최소 힙을 이용한다. 유사도 점수를 계산해서 상위 k개의 문서를 수집하면 되는 상황이라면 현재까지 수집된 k번쨰 문서보다 점수 경쟁력이 떨어지는 문서나 문서의 블록을 적극적으로 건너띄는 여러 가지 최적화 방법을 함께 사용한다.쿼리 문맥과 필터 문맥 쿼리 문맥의 쿼리든 필터 문맥의 쿼리든 수행 순서와는 관련이 없다. DocIdSetIterator 인스턴스생성이 끝난 후에야 LeafCollector가 순회하며 데이터를 수집하고, 그 안에서 socre를 계산한다. 이러한 흐름 자체가 쿼리 문맥, 필터 문맥 상관 없이 하위 쿼리에 매치되는 후보군 자체를 일단 다 뽑는다는 것을 의미한다. 필터 문맥의 쿼리는 이때 score를 호출하여 점수를 계싼하는 비용을 아끼는것일뿐 먼저 수행되는것은 아니다. 매치되는 단일 문서마다 유사도 점수를 계산하며 수집한다는 것은 하위 쿼리의 동작 일부가 병렬적으로 수행된다는 것을 의미한다.8.2.3 캐시 동작 검색 성능이 아쉽다면 캐시를 잘 활용하고 있는지 검토해 볼 필요가 있다.샤드 레벨 요청 캐시 샤드 레벨 요청 캐시(request cache)는 검색 수행 시 query 페이즈에서 수행된 작업을 샤드 레벨에 저장하는 캐시이다.캐시 수행 위치 SearchQueryThenFetchAsyncAction 작업 이후 QueryPhase 작업에 들어갈 떄 동작한다. ShardSearchRequest와 대상 인덱스 설정을 보고 캐시 가능한 요청인지 여부를 파악한다.캐시 조건 ES 검색 수행 시 요청 캐시를 활용하려면 다음 조건을 만족해야 한다. search_type이 query_then_fetch이어야 한다. scroll 검색이 아니어야 한다. profile 요청이 아니어야 한다. ( Profile API ) 검색 요청의 개별 구성 요소 실행에 대한 자세한 타이밍 정보를 제공받기 위한 요청, 디버깅용 now가 포함된 날짜 시간 계싼 표현이나 Math.random() 같은 명령이 포함된 스크립트가 아니어야 한다. api 호출시 requestCache 매개변수를 명시적으로 지정한 경우 true이어야 한다. 매배견수를 지정하지 않는 경우 index.requests.cache.enable 인덱스 설정이 true여야 한다.(default : true) 검색 요청의 size 매배견수가 0이어야 캐시 대상이 될 수 있다. 명시적으로 requestCache를 true로 지정하면 이 여부와 관계 없이 캐시를 수행한다. 캐시 키 요청이 캐시 대상이라는 사실을 확인하고 나면 캐시 키를 만든다. 인덱스, 샤드 번호와 검색 요청의 본문 내용 등 사실상 완전히 같은 검색 요청이어야 캐시가 적중되는 구조이다. 인가되지 않은 사용자가 캐시에 올라간 데이터를 받아가지 않게 하기 위한 추가적인 로직이 있음.캐시 대상 값 캐시 키를 만든 후에는 이 키로 캐시에 올라간 데이터가 있는지 찾는다. 캐시가 적중했다면 쿼리 매치 작업을 수행하지 않고 캐시된 값을 바로 이용한다. 제안 결과, 집계 결과, 매치된 문서 수, 최대 점수, 매치된 상위 문서 등 모든 응답 값이 캐시된다.샤드 레벨 요청 캐시 활용 방향 샤드 레벨 요청 캐시의 주 목적 자체가 size를 0으로 지정해 요청하는 집계 결과를 캐시하는 것이 목적이다. 매치된 상위 문서가 무엇인지, 유사도 점수가 얼마인지는 캐시에서 가져올 수 있지만, 그 문서의 _source는 FetchPhase를 통해서 가져와야 한다. 그러므로 집계를 지정하지 않은 일반적인 검색 요청의 쿼리 매치 작업 결과를 캐시하는 것은 집계 작업을 캐시하는 것보다 효율성이 떨어진다.(기본 동작이 size = 0 인 요청하는 캐시하는 이유) 서비스 특성상 ES 클러스터의 주 사용 용도가 집계가 아닌 일반 검색이고, 동일한 쿼리가 여러 번 인입될 가능성이 있다면 요청 캐시를 적극적으로 사용하는 것도 좋다. 이 경우 requestCache 매개변수를 true로 지정해서 요청하는 방식을 사용하자. FetchPhase은 무조건 수행해야 하기는 하지만, QueryPhase의 작업을 생략할 수 있는 것만으로도 성능 차이가 클 수 있다. 캐시가 적재되는 위치 ES 노드가 기동하는 과정에서 Node 클래스, IndicesService 인스턴스가 생성되고, 이 내부의 IndicesRequestCahe 인스턴스를 멤버로 관리하며 실제 Cache는 여기서 관리된다. 즉 샤드 레벨 요청 캐시는 노드와 생명주기를 같이 한다고 보면 된다.캐시 상태 확인GET /_status/request_cache/human 위와 같이 인덱스별 혹은 전체 인덱스의 캐시 적중, 부적중, 퇴거(eviction) 회수, 캐시 크기 등의 주요 정보를 확인 할수 있다.캐시 크기 지정과 캐시 무효화 기본 설정으로는 총 힙의 1%까지 샤드 레벨 요청 캐시로 사용한다. config/elasticsearch.yml의 indices.requests.cache.size을 변경하고 재기동하여 이 값을 변경할 수 있다. ex. indices.requests.cache.size:2% 샤드 레벨 요청 캐시는 인덱스 refresh를 수행할 때마다 무효화된다. 수동으로 무효화하려면 아래 API를 호출할 수 있다. POST [인덱스 이름]/_cache/clear?request=true 노드 레벨 캐시 노드 레벨 캐시는 필터 문맥으로 검색 수행시 쿼리에 어떤 문서가 매치됐는지를 노드 레벨에 저장하는 캐시이다.캐시 수행 위치 QueryPhase에서 검색 수행 시 IndexSearcher.search()을 수행하고, 여기서 유사도 점수를 계산하지 않는 쿼리라면 쿼리 캐시를 적용하는 CachingWeightWrapper로 감싸 최종 Wieght를 반환하고, 검색을 수행하는 과정에서 캐시된 DocIdSet을 순회하는 Iterator을 반환한다.캐시 대상 값 캐시에 적재하는 값인 DocIdSet 추상 클래스는 쿼리에 매치된 문서 목록을 나타낸다. 여러 DocIdSet의 구현체가 있지만, 비트 배열로 구현한다. (ex. 4번 문서가 매치된 경우 배열의 비트를 1로 처리한다.캐시 키 캐시의 키는 Query이다. Query의 equals와 hashCode 구현상 같은 Query로 취급되는 경우 캐시를 적중시킬 수 있다.bool 쿼리에서 노드 레벨 쿼리 캐시 활용 쿼리 문맥과 필터 문맥을 모두 가지고 있는 bool 쿼리는 쿼리 중 일 부분인 필터 문맥 부분만 잘 분리하여 캐시한다. 여러 bool 쿼리의 내용을 바꿔가며 수행하더라도 필터 문맥의 하위 쿼리 중 일부가 겹친다면 부분적으로 캐시의 혜택을 볼 수 있다. 쿼리 문맥은 노드 레벨 쿼리 캐시가 안된다고 보면 된다. 캐시 조건 캐시를 활용하기 위해서는 유사도 점수를 계산하지 않는 쿼리여야 한다는 점 외에도 조건이 많다. 1만개 이상의 문서가 있으며 동시에 샤드 내 문사ㅓ의 3% 이상을 보유하고 있는 세그먼트가 대상일 떄만 캐시한다. 내부 테스트 목적으로 설정의 해당 조건을 아예 제거할수는 있지만, 1만과 3%의 숫자 값은 바꿀 수 없다. 이 외에도 쿼리 캐시 없이도 충분히 빠르다고 판단하는 쿼리는 캐시하지 않는다. 최근 해당 쿼리를 얼마나 수행했는지 이력을 추적하여 루씬 내부적인 기준으로 무겁다고 판단한 쿼리는 2회 이상, 그 외는 4~5회 이상 반복 수행해야만 캐시를 수행한다.락 획득 노드 레벨 쿼리 캐시는 읽기 작업과 쓰기 작업에 모두 락 획득을 필요로 한다. 읽기 작업 도중 락 획득은 무작정 기다리지는 않고, 락 획득 시도가 실패하면 바로 일반 검색 작업을 진행한다.쿼리 캐시 상태 확인GET /_stats/query/_cache?human 이 요청을 통해 인덱스별 혹은 전체 인덱스의 캐시 적중, 부적중, 퇴거 회수, 캐시 크기 등 주요 노드 레벨 쿼리 캐시의 상태를 확인할 수 있다.캐시 크기 지정 기본 설정으로는 총 힙의 10%까지 노드 레벨 쿼리 캐시를 사용한다. config/elasticsearch.yml의 indics.queries.cache.size: 5% 와 같이 지정해서 값을 변경할 수 있다.쿼리 캐시 무효화 쿼리 캐시도 인덱스 refresh를 수행할떄 캐시가 무효화되고, 수동으로 무효화 하는 경우 아래 API를 호출할 수 있다. 파라미터가 request / query 로 구분해서 달리 처리할 수 있다. POST [인덱스 이름]/_cache/clear?query=trueES의 캐시 적재 위치 구성샤드 레벨 요청 캐시와 노드 레벨 캐시 비교   요청 캐시 쿼리 캐시 수행 위치 QueryPhase 작업 들어가기 직전 래핑한 Weight에서 BulkScorer, Scorer 등 생성시 키 SharedSearchRequest Query 값 매치된 상위 문서와 점수의 목록, 제안, 집계 등을 포함한 QueryPhase의 수행 결과 매치된 문서 목록을 나타내는 비트 배열 주요 캐시 조건 1. 요청에 requestCache를 명시적으로 지정, 2. index.requests.cache.enabled=true &amp;&amp; size가 0인 집계 쿼리, 그 외 확정적인 검색 등 기본 조건 필터 문맥에서 쿼리를 많이 자주 수행해야 하고, 빠른 쿼리는 제외되며 세그먼트에 일정 이상의 문서가 있는 경우 적재 위치 IndicesService.IndicesRequestCache IndicesService.IndicesQueryCache 캐시 접근 범위 샤드 레벨 노드 레벨 운영체제 레벨 페이지 캐시 위 캐시 외에 OS 레벨에서 기본적으로 동작하는 페이지 캐시가 있다. OS는 디스크에서 데이터를 읽은 후 데이터를 메모리에 넣어 두었다가 다시 이 데이터를 읽을 일이 있으면 디스크가 아니라 메모리에서 읽어 반환한다. ES는 페이지 캐시를 잘 활용하고 있으므로 시스템 메모리의 절반 이상은 캐시로 사용하도록 설정하는 것이 좋다.Refernece preference 관련 트래블슈팅 사례 collapse를 이용한 검색 축소 Profile API" }, { "title": "[엘라스틱서치 바이블] 5. 서비스 환경에 클러스터 구성", "url": "/posts/es-bible-5/", "categories": "DevLog, Elasticsearch", "tags": "Elasticsearch", "date": "2024-01-06 19:53:00 +0900", "snippet": "[엘라스틱서치 바이블] 5. 서비스 환경에 클러스터 구성5.1 운영 환경을 위한 설정과 클러스터 구성5.1.1 노드 설정과 노드 역할 ES 클러스터 내에서 각 노드가 수행하는 역할의 종류에 대해 알아보자노드 역할 클러스터 구성시 반드시 노드에 역할을 지정해야 한다. 지정된 역할에 따라 노드가 클러스터 내에서 어떤 작업을 담당할지 정해진다.마스터 ...", "content": "[엘라스틱서치 바이블] 5. 서비스 환경에 클러스터 구성5.1 운영 환경을 위한 설정과 클러스터 구성5.1.1 노드 설정과 노드 역할 ES 클러스터 내에서 각 노드가 수행하는 역할의 종류에 대해 알아보자노드 역할 클러스터 구성시 반드시 노드에 역할을 지정해야 한다. 지정된 역할에 따라 노드가 클러스터 내에서 어떤 작업을 담당할지 정해진다.마스터 후보(master-eligible) 노드 노드의 역할에 master를 지정하면 해당 노드는 마스터 후보 노드가 된다. 마스터 후보 노드 중에서 선거를 통해 마스터 노드가 선출된다. 마스터 노드는 클러스터를 관리하는 역할을 수행한다. 인덳 생성이나 삭제, 어떤 샤드를 어느 노드에 할당할 것인지 등 중요한 작업을 수행한다.데이터 노드 실제 데이터를 들고 있는 노드이다. CRUD, 검색, 집계와 같이 데이터와 관련된 작업을 수행한다.인제스트(ingest) 노드 데이터가 색인되기 전에 전처리를 수행하는 인제스트 파이프라인을 수행하는 노드이다. 간략히 살펴보면 데이터(Document)가 파이프라인을 통해 들어올떄 lowercase와 같은 전처리 등을 수행한다고 보면 된다. 자세한 내용은 ES Doc Ingest 을 참고하자.조정 노드 클라이언트의 요청을 받아서 다른 노드에 요청을 분배하고, 클라이언트에게 최종 응답을 돌려주는 노드이다. 기본적으로 모든 노드가 조정 노드 역할을 수행한다. 마스터나 데이터 등 주요 역할을 수행하지 않고, 조정 역할만 수행하는 노드는 조정 전용 노드라고 한다.원격 클러스터 클라이언트 노드 다른 ES 클러스터에 클라이언트로 붙을 수 있는 노드이다. 노드 역할에 remote_cluster_client을 추가해 지정한다. 키바나의 스택 모니터링 기능을 활용해서 모니터링 전용 클러스ㅓ터를 구축한 뒤 얼럿 메시지를 보내도록 구성하거나 유료 기능인 클러스터간 검색 기능 등을 활용할 때 사용된다.데이터 티어 노드 데이터 노드를 용도 및 성능별로 hot-warm-cold-frozen 티어로 구분해 저장하는 데이터 티어 구조 채택시 사용하는 역할이다. data 역할 대신에 data_content, data_hot, data_warm, data_cold, data_frozen 역할을 선택한다.config/elasticsearch.yml 클러스터 구성시 위 confg 파일을 통해 설정을 할수 있다.node.roles: [data, master, ingest]cluster.name: test-esnode.name: test-es-node01http.port: 9200transport.port: 9300-9400network.host: my_server_ippath: data: /var/lib/elasticsearch/logs logs: - /var/log/elasticsearch/data1 - /var/log/elasticsearch/data2network.host: 10.0.0.1network.bind_host: 0.0.0.0discovery.seed_hosts: [\"10.0.0.1\", \"10.0.0.2\", \"some-host-name.net\"]cluster.initial_master_nodes: [\"node_name-1\", \"node_name-2\" \"node_name-3\"]xpack.monitoring.enabled: falsenode.roles 노드 역할을 지정한다. (master, data ,ingest 등) 0개 이상을 조합할 수 있다. []와 같이 비워두면 조정 전용 노드가 된다. 하나의 노드가 여러 역할을 할수 있으나, 운영 환경에서는 마스터 후보 역할과 데이터 역할을 분리하는 것이 좋다.discovery.seed_hosts 마스터 노드로 동작할 수 있는 노드 목록을 지정한다.cluster.initial_master_nodes 클러스터를 처음 기동할 떄 첫 마스터 선거를 수행할 후보 노드 목록을 지정한다. (node.name에 기입한 값)network.bind_host, network.publish_post bind_host는 ES에 바인딩할 네트워크 주소를 지정한다. publish_post는 클러스터의 다른 노드에게 자신을 알릴 떄 쓰는 주소를 지정한다. network.host에 지정한 값을 입력한다.tansport.port transport 통신을 위해 사용하는 포트를 지정한다.path.data 데이터 디렉터리를 지정할 수 있다. 여러 경로를 지정할수 있는 부분은 7.13버전부터 지원 중단되었다.5.1.2 그 외 필요한 주요 설정힙 크기 적절한 힙 크기 지정은 ES 클러스터 운영에서 아주 중요하다. 힙 크기는 config/jvm.options를 열어 다음과 같은 방식으로 지정한다.vim config/jvm.options.d/heap-size.options-Xms32736m-Xmx32736m힙 크기 지정 관련 대원칙 최소한 시스템 메모리의 절반 이하로 지정해야 한다. 시스템 메모리의 절반은 운영체제가 캐시로 쓰도록 놔두는것이 좋다. 루씬이 커널 시스템 캐시를 많이 활용하기 떄문이다. 힙 크기를 32GB 이상 지정하지 않아야 한다. ES는 512GB 이상의 고용량 메모리를 갖춘 서버를 사용하더라도 힙 크기는 32GB 이하로 가이드한다. JVM이 힙 영역에 생성된 객체에 접근하기 위한 포인터(Ordinary Object Pointer)을 통해 처리되는데, 32비트 환경에서는 포인터 1개를 32비트로, 64비트 환경에서는 64비트로 표현하므로, 4GB를 넘어서는 힙을 사용한다면 32비트 OOP로는 불가능하고, 64비트 OOP를 사용하는 경우에도 최대 32GB까지만 가능하다. 다만, 32GB 이내 힙 영역에만 접근한다면 CompressedOOPs라는 기능을 적용해 포인터를 32비트로 유지할 수 있다. 하지만 정확한것은 확인을 통해 CompressedOOPs 값이 true가 되는 경계값으로 힙 크기를 지정해야 한다. java -Xmx32736m -XX:+PrintFlagsfinal 2&gt; /dev/null | grep UseCompressedOopsjava -Xmx32737m -XX:+PrintFlagsfinal 2&gt; /dev/null | grep UseCompressedOopsjava -XX:+UnlockDiagnosticVMOptions -Xlog:gc+heap+coops=debug -Xmx30721m -versionjava -XX:+UnlockDiagnosticVMOptions -Xlog:gc+heap+coops=debug -Xmx30720m -version[0.002s][debug][gc,heap,coops] Protected page at the reserved heap base: 0x0000000280000000 / 16777216 bytes[0.002s][debug][gc,heap,coops] Heap address: 0x0000000281000000, size: 30720 MB, Compressed Oops mode: Non-zero based: 0x0000000280000000, Oop shift amount: 3openjdk version \"17.0.6\" 2023-01-17 LTSOpenJDK Runtime Environment Zulu17.40+19-CA (build 17.0.6+10-LTS)OpenJDK 64-Bit Server VM Zulu17.40+19-CA (build 17.0.6+10-LTS, mixed mode, sharing) 엄밀히 테스트하기 어렵다면 Compressed Oops mode 가 Zero based 선까지 낮추는것을 권장한다.스와핑 ES는 스와핑을 사용하지 않도록 강력히 권고한다. 스와핑은 성능과 노드 안정성에 큰 영향을 미친다. ms단위로 끝나야 할 GC를 분 단위 시간까지 걸리게 만들기도 한다. 스와핑을 켜는것보다 차라리 OS가 노드를 kill 시키도록 놔두는것이 낫다고 가이드한다.스와핑을 완전히 끄기 아래 명령어를 통해 OS의 스와핑을 완전히 끌수 있다. 이 명령 전후에 ES 노드를 재기동하지 않아도 되나, OS 재부팅 후에도 설정을 유지하고 싶다면 /etc/fstab 내부의 swap 부분을 제거해야 한다.sudo swapoff -asudo vim /etc/fstabswapiness 설정 스와핑을 완전히 끌 수 없는 특수한 상황이라면 swapiness 값을 1로 설정하여 스와핑 경향성을 최소화할 수 있다.sysctl vm.swappinessvm.swappiness = 30sudo sysctl -w vm.swappiness=1# os 재부팅 후에도 유지시키고 싶다면 아래 파일 수정sudo vim /etc/sysctl.d/98-elasticsearch.confvm.swappiness = 1bootstrap.memory_lock 설정 bootstrap.memory_lock: true 설정을 지정하여 프로세스의 주소 공간을 메모리로 제한시키고, 스와핑되는것을 막을 수 있다.-그러나 이방법은 애플리케이션 레벨에서만 의미가 있는 제한적인 기술이다.(OS 차원에서 이 설정을 무시하고 스와핑이 발생할 수 있다.)vm.max_map_count vm.max_map_count 값은 프로세스가 최대 몇 개까지 메모리 맵 영역을 가질수 있는지를 지정한다. 루씬은 mmap을 적극적으로 활용하기 떄문에 vm.max_map_count 값을 높일 필요가 있따. 만약 이 값이 262144보다 작은 값으로 셋팅되어 있다면 ES는 기동 자첼가 되지 않을수 있다. 이 값의 설정은 최대치를 규정할 뿐이며, 값을 매우 높게 하더라도 OS 메모리 사용량이나 성능상 손해는 없다고 알려져있다. 일반적으로 추천되는 사이즈는 시스템의 메모리 / 128KB의 값으로 셋팅하는것을 추천한다. 대부분의 경우 262144로 지정해 대규모 ES 서비스를 운영해도 별 문제는 없다.sysctl vm.max_map_countvm.max_map_count = 65530sudo sysctl v-w m.max_map_count=262144# 재부팅 후에도 설정을 유지하려면 아래 파일 수정sudo vim /etc/sysctl.d/98-elasticsearch.confvm.max_map_count = 262144파일 기술자 ES는 많은 파일 기술자(file descriptor)를 필요로 한다. ES는 이값을 최소 65535 이상으로 지정하도록 가이드한다.ulimit -a | grep \"open files\"# OSX에서는 아래설정으로 확인ulimit -a | grep 'file descriptor'# 값 설정sudo ulimit -n 65535# 영구 지정을 위해 파일 수정sudo vim /etc/security/limits.confusername - nofile 65535JVM 지정과 설정 특별한 사유가 없다면 내장 JDK를 사용하는것이 좋으며, 힙 크기와 JVM 설정은 굳이 변경하지 않도록 권고한다. 7.12 버전 미만을 사용한다면 의도치 않은 JAVA_HOME 환경 변수가 설정됐는지 사전에 확인해야 하는 점도 동일하다.5.2 클러스터 구성 전략5.2.1 마스터 후보 노드와 데이터 노드를 분리 어느 정도 규모가 있는 서비스를 위해 클러스터를 운영한다면 마스터 후보 노드를 데이터 노드와 분리시켜야 한다. 마스터 노드는 클러스터를 관리하는 중요한 역할을 하는데 운영 중 발생하는 문제를 보면 상대적으로 데이터 노드가 죽을 확률이 높으므로 마스터 후보와 데이터 노드 역할을 동시에 수행한다면 문제가 생길 가능성이 높다. 또 장애 대응을 위해 특정 노드를 rolling restart 할때에도 장애 원인에 따라 마스터 노드를 재시작하거나 데이터 노드를 재시작해야 하는 상황이 다른데 이러한 fail-over 과정도 복잡해질 수 있다. 데이터 노드가 하나 죽은 경우라면 마스터 노드가 주 샤드가 사라진 샤드의 복제본 샤드를 새로운 주 샤드로 변경 시키고, 복제본 개수를 새로 채우는 과정을 수행하며 자연스럽게 fail-over를 할 수 있다. 마스터 후보 노드는 데이터 노드보다 상대적으로 성능이 많이 낮은 서버를 사용해도 무방하다. 데이터 노드처럼 디스크나 메모리를 많이 필요로 하지 않다. 5.2.2 마스터 후보 노드와 투표 구성원 클러스터가 운영되는 동안 항상 마스터 노드가 선출되어 있어야 한다. 이러한 마스터 노드를 선출하는 집단이 바로 투표 구성원이다. 투표 구성원은 마스터 후보 노드 중 일부 혹은 전체로 구성된 마스터 후보 노드의 부분 집합으로 마스터 선출이나 클러스터 상태 저장 등의 이사결정에 참여하는 노드의모임이다. 마스터 후보 노드가 클러스터에 참여하거나 떠나면 ES에서 안정성을 추가하는 방향으로 투표 구성원을 자동 조정한다. 이 조정 동작에는 어느정도 시간이 필요하므로, 조정이 필요한 경우 1대씩 천천히 진행하는것이 좋다. 동시에 절반 이상이 멈추면 클러스터는 의사결정을 내릴 수 없는 상황에 빠질 수 있다. 마스터 후보 노드는 홀수대를 준비하는 것이 비용 대비 효용성이 좋다. ES 7 버전 이상부터는 투표 구성원을 홀수로 유지하고, 후보 노드를 뺴놓는 방식으로 처리된다. 2K + 2대가 운영되더라도 투표 구성원은 2K + 1대만 유지하고, 1대가 죽거나 할 경우 대기중이던 1대를 투입시켜서 홀수를 유지시킨다. 5.2.3 서버 자원이 많지 않은 경우 적어도 3대가 확보돼야 클러스터를 구성하는 의미가 있다. 마스터 후보 노드 최소 3대, 데이터 노드 최소 3대가 확보돼야 기본적인 고가용성이 제공되기 떄문이다.최소 구성, 장비 3대 최소의 서버 자원으로 ES 클러스터를 구성한다면 3대의 노드가 모두 마스터 후보 역할과 데이터 역할을 겸임하는 구성이 된다.사양이 낮은 장비 4~5대가 있는 경우 3대는 마스터 후보 역할과 데이터 역항을 겸임하게 지정하고, 나머지 1~2대를 데이터 노드 전용으로 지정한다.사양이 높은 장비 4~5대가 있는 경우 같은 비용을 지불해서 사양이 낮은 장비 3대와 사양이 높은 장비 3~4대 구성이 가능한지 확인한다. 변경할 수 있다면 사양이 낮은 장비 3대에 마스터 후보 역할을 주고, 사양이 높은 나머지 장비에 데이터 노드 역할을 준다.장비 6~7대 마스터 후보 노드와 데이터 노드를 완전히 분리할 수 있다. 서비스 안정성 면에서 훨씬 이득이다.5.2.4 서버 자원이 굉장히 많이 필요한 경우 한 클러스터에 동원해야 하는 물리 서버가 200대 이상인 경우 서비스 요건이 빠듯하고, 동원할 수 있는 서버 자원이 굉장히 많은 상황이 있다. 이런 상황에서는 먼저 용도나 중요도별로 클러스터를 더 잘게 쪼갤 수 있는지 검토해야 한다. ES가 선형적 확장을 잘 지원하는 구조로 설계됐다 하더라도 선출하는 마스터 노드의 수 자체를 늘릴 수는 없기 떄문에 한계가 있다. 클러스터를 나눌수 없다면 마스터 노드도 높은 사양의 서버로 준비하는 것이 좋다. 그럼에도 결국 한계가 있기 떄문에 클러스터를 쪼개고, 클러스터간 검색(cross-cluster search) 도입을 고려해보는것이 좋다.5.2.5 사양이 크게 차이나는 서버 자원을 활용해야 하는 경우 가용한 서버 자원 사이의 사양 차이가 크다면 같은 운영 비용으로 성능 차이가 적은 서버로 일원화해서 교체 운영할 수 있는지 먼저 검토하자. 만약 그것이 어려운 상황이고 성능 차이가 좀 나더라도 남은 서버 자원을 긁어모아 활용해야 한다면 데이터 티어 구조의 도입을 고려할 수 있다. 성능이 높은 서버를 data_content와 data_hot 역할로, 성능이 낮은 서버를 data_warm, data_cold, data_frozen 역할로 지정한다. 이후 인덱스 생명 주기 관리 정책을 도입해 오래된 시계열 데이터 인덱스를 낮은 티어의 노드로 이동시키며 운영할 수 있다. 다만, 이때 데이터 티어 노드를 이동하는 것 역시 작업이며, 클러스터에 부하를 줄수 있으므로 상황을 잘 판단하여 신중하게 설계해야 한다.5.2.6 조정 전용 노드 안정적인 클러스터 운영을 위해서는 조정 전용 노드를 두는 것이 좋다.(node.roles: []로 지정해 조정 역할만 수행하는 노드) 각 데이터 노드에서 작업한 결과물을 모아서 돌려주는 작업은 생각보다 큰 부하를 줄 수 있다. 특히, 운영환경에서 키바나를 통해 큰 부하를 주는 경우가 발생할 수 있는데, 이떄 키바나를 조정 전용 노드만을 바라보도록 설정할 수 있다. 서버 자원에 여유가 있다면 읽기 작업을 담당하는 조정 전용 노드와 쓰기 작업을 담당하는 조정 전용 노드를 분리하는 것도 좋은 방법이다. 조정 전용 노드는 데이터 노드보다 사양이 낮은 장비로 구성해도 문제 없다. ES7버전 이상부터 도입된 실제 메모리 서킷 브레이커가 조정 전용 노드와 궁합이 좋은데, 조정 전용 노드의 메모리 사용량이 높아지면 서킷 브레이커가 해당 요청을 차단한다. 5.2.7 한 서버에 여러 프로세스 띄우기 128G 이상의 메모리를 사용하는 고스펙 서버에서 32GB 정도의 힙을 설정한 ES 프로세스를 여러 개 띄우는 방법이 있는데, 이 방법으로 성능을 최대한 끌어내고자 한다면 반드시 실제와 비슷한 환경으로 테스트를 하고 방향을 결정해야 한다. 한 서버에서 여러 프로세스를 띄우는 방법과 유의사항을 좀 더 알아보자유의사항 마스터 후보 역항을 하는 프로세스는 다중 프로세스 대상으로 고려하지 말아야 한다.(클러스터 안정성이 매우 떨어짐) 다중 프로세스 기동을 위해서는 cluster.name은 같게, node.name은 다르게 설정해야 하고, http, transport, path.logs, path.data같은 설정은 독립적인 설정을 가져야 한다. 프로세스 기동을 위해서는 cluster.routing.allocation.same_shard.host: true 설정을 지정해야 한다.(기본값 : false) 또 OS의 file descriptor, mmap max_map_count, 네트워크 대역폭 등 자원도 공유한다는 사실과 서버에 하드웨어적인 문제가 발생하면 영향도가 N배가 될수 있다는 리스크를 염두해두어야 한다.5.3 보안 기능 적용 운영환경에서는 보안이 굉장히 중요한 이슈이다. 기본적으로 특별한 설정을 하지 않으면 통신 암호화를 하지 않는다. ES나 키바나가 인터넷 환경에 노출되어 있다면 인증과 권한 부여, 분리와 관련된 기능도 중요하고, 보안 기능을 적용 해야 한다. 기본적으로는 xpack.security.enabled: true 설정을 바꿔주면 보안 기능을 클러스터에 적용할 수 있다.5.3.1 모든 보안 기능을 적용하지 않은 상태 접근하는 사용자와 클라이언트 전수를 명확하게 제어할 수 있는 상황이라면 xpack.security.enabled: false로 두고 운영하는 선택도 가능하다.cluster.initial_master_nodes: [\"test-es-node-01\",\"test-es-node-02\",\"test-es-node-03\"]xpack.security.enabled: false5.3.2 TLS 부트스트랩 체크 본격적인 보안 설정을 진행하기 전에 먼저 TLS 부트스트랩 체크를 알아야 한다. xpack.security.enabled: false 으로 구동할 경우 TLS 부트스트랩 체크를 건너뛴다. 클러스터 기동 과정에서 노드간 transport 통신에 TLS를 적용하지 않았다면 기동을 거부한다. 노드간 TLS 통신을 먼저 적용해 클러스터를 기동하고, 그 이후에 내부 계정 초기화 작업을 수행해야 한다.5.3.3 클러스터 최초 기동 시 자동 보안 설정 이용 자동 보안 설정 기능은 ES8 버전부터 추가된 기능이다. 8버전 미만을 사용하면 수동 설정으로 진행해야 한다. 전용 CA(Certificate Authority)를 생성하고, 그 CA로 서명된 인증서를 자동 발급한다. 노드간 transport 레이어와 REST API에 사용하는 HTTP 레이어에 TLS 통신을 적용한다.TLS 부트스트랩 체크를 피해 최초 기동node.roles: [ master, data ]cluster.name: test-esnode.name : test-es-node01path: logs: /my/path/to/elasticsearch/logs datas: - /my/path/to/elasticsearch/data1 - /my/path/to/elasticsearch/data2network.host: 10.0.0.1network.bind_host: 0.0.0.0discovery.seed_hosts: [\"10.0.0.1\", \"10.0.0.2\", \"10.0.0.3\"]discovery.type: \"single-node\" 먼저 클러스터 중 한 대의 노드를 띄우고 아래 설정으로 실행한다. 단일 노드로 클러스터가 기동되고 나면 자동 설정이 수행되는데, 이떄 elasti usre password나 enrollment token 등이 sdtout으로 출력된다. 또 프로세스를 종료한 후 config 디렉토리 하위에 certs라는 디렉토리가 생겼고, 그 아래에 CA와 인증서 파일이 자동 생성됨을 확인할 수 있다. 이 메시지를 적당한 곳에 복사해두고, 이를 이용해서 클러스터 노드 추가, 키바나를 붙이는 작업 등을 수행할 수 있다.xapck.security.enrollment.enabled: truexapck.security.http.ssl: enabled: true keystore.path: certs/http.p12xapck.security.transport.ssl: enabled: true verification_mode: certificate keystore.path: certs/http.p12 truststore.path: certs/http.p12enrollment 토큰으로 클러스터에 노드 추가 단일 노드 클러스터로 띄웠던 노드의 config/elasticsearh.yml의 discovery.type 설정을 제거해주고 다시 기동한다. 만약 단일 노드 최초 기동시 발급했던 enrollment 토큰을 분실했다면 다음과 같이 새로운 enrollment 토큰을 발급할 수 있다.bin/elastisearch-create-enrollment-token -s node 새롭게 투입할 노드는 아래와 같은 설정으로 기동할 수 있다. 이때 cluster.initial_master_nodes, xpack.security.enabled 설정과 discovery도 기입하지 않는다.node.roles: [ master, data ]cluster.name: test-esnode.name : test-es-node02path: logs: /my/path/to/elasticsearch/logs datas: - /my/path/to/elasticsearch/data1 - /my/path/to/elasticsearch/data2network.host: 10.0.0.1network.bind_host: 0.0.0.0 설정 확인 후 발급한 enrollment 토큰을 지정해 노드를 기동한다.bin/elasticsearch -d --enrollment-token 노드 기동이 완료되면 클러스터에 새 노드가 합류하고, 이후 추가된 노드의 config/certs에도 인증서가 복사되고, 자동으로 보안 설정이 지정된것을 확인 할 수 있다.초기 비밀번호 변경# 합류 노드 아무곳에서나 한번 수행해서 최고 관리자 계정의 초기 비밀번호를 설정할 수 있다.bin/elasticsearch-reest-password --interactive -u elasticenrollment 토큰으로 키바나 기동bin/elasticsearch-create-enrollment-token -s kibana 키바나를 기동하기 전에 config/kibana.yml의 설정을 검토한다.server.port: 5601server.host: \"10.0.0.3\"server.publicBaseUrl: \"http://10.0.0.3:5601\" enrollment를 이용한 최초 기동을 위해서는 elasticsearch.hosts를 지정하지 않아야 한다. 그리고 키바나를 기동하면 웹에서 enrollment 토큰을 입력하는 창이 나타나고, 여기에 토큰을 입력하면 키바나 설정이 진행된다. 버전(상황)에 따라 키바나에서 발급하는 추가 인증 코드를 입력하라는 안내가 나올 수 있는데 그런 경우 키바나에서 인증 코드를 발급받아 이를 키바나 웹 UI에 입력하면 초기 설정이 재개된다.bin/kibana-verification-codeYour verification code is : 856 595추가 작업 보안 설정을 끝낸 후에는 설정을 의도한 상황에 맞게 조금 정리해줄 필요가 있다. ES 노드의 config/elasticsearch.yml을 보면 discovery.seed_hosts 설정이 원하는 형태가 아닌데, 이런 부분을 마스터 후보 노드를 정확히 가리키도록 변경해야 한다. 또 node.role 설정도 의도와 다르게 설정된 노드가 있다면 변경해주어야 한다. config/kibana.yml의 elasticsearch.hosts 설정도 원하는 리스트로 수정한다. 최종적으로 아래와 같은 구조가 자동 구성된다.5.3.4 키바나와 브라우저 사이에 TLS 적용 위 구성을 하더라도 아직 키바나와 브라우저 사이에는 TLS가 적용되지 않았다. 여기에는 신뢰할 수 있는 공인된 CA가 서명한 인증서를 발급받고 이를 이용해 CSR과 개인키를 생성하고 TLS를 적용하는게 필요하다.bin/elasticsearch-certutil csr -name kibana-server -dns example.com,www.example.com 위에서 생성된 파일의 압축을 풀면 .csr 파일과 .key 파일이 나오는데 이를 설정에 추가할 수 있따. .csr 파일은 인증서 발급 기관으로 보내 인증서를 발급받도록 한다. 인증서가 발급되면 kibana/config 디렉토리 하위에 파일을 위치 시키고 설정을 해주면 된다. server.publicBaseUrl: \"https://example.com:5601\"server.ssl.enabled: trueserver.ssl.certificate: \"/path/to/kibana/config/certificate.pem\"server.ssl.key: \"/path/to/kibana/config/privkey/kibana-server-privkey.key\"5.3.5 수동으로 엘라스틱 서치 노드 간의 통신에 TLS 적용 수동으로 ES 노드간의 통신에 TLS를 적용할 수 있지만, 이 과정은 매우 길고 복잡하며 실수하기 쉽다. 따라서 일반적으로 자동 보안 설정을 이용하는 편이 낫다.절차 CA와 self-signed 인증서 생성 인증서 파일 복사 ES 설정 변경5.3.6 수동으로 기본 인증 설정 ES는 계정과 역할 기반으로 인증, 권한 부여와 분리 기능을 제공한다. 역할에는 읽기, 쓰기, 모니터링, 스냅샷 등 ES에서 수행할 수 있는 여러 작업의 권한이 매우 세부적으로 나뉘어져 있다. 이 외에는 키바나에 스페이스(space)라는 개념을 통해 데이터 뷰, 비주어라이즈, 대시보드 등을 스페이스마다 독립적으로 운영할 수 있다.최소 보안 노드간의 통신에 TLS를 적용하지 않았다면 ES는 애초에 기동되지 않으므로 내부 계정을 활성화시키는 단계로 진입할 수가 없다. 하지만 단일 노드로 구성한 개발 모드는 TLS 부트스트랩 체크를 건너뛰므로 노드간 TLS 적용 없이 기본 인증만 설정해 사용하는것이 가능하다. 기본 인증만 있는 경우 최소 보안만 설정된 상태라고 볼수 있다.내부 계정의 초기 비밀번호 설정bin/elasticsearch-reset-password --interactive -u elasticbin/elasticsearch-reset-password --interactive -u kibana_system# es에 자동으로 password 생성하도록 지시bin/elasticsearch-reset-password --auto -u kibana_system-위 비밀번호 설정을 통해 계정의 비밀번호를 얻어낸다면 이를 이용하여 ES 기본인증을 사용할 수 있다.crul -H \"Authorization: Basic ${accessKoten}\" -XGET \"http://localhost:9200?pertty=true\"키바나 설정 변경 키바나에서 kibana_system 계정을 사용하기 위해 config/kibana.yml 설정을 변경한다.elasticsearch.username: \"kibana_system\"elasticsearch.password: 계정의 비밀번호를 키바나에게 알려주기 위해 elasticsearch.password를 설정할수도 있지만, 키스토어를 이용하는것이 안전하다.bin/kibana-keystore createbin/kibana-keystore add elasticsearch.password 위 설정을 추가하고 키바나를 기동하면 접속시 로그인을 요구하는 창이 등장하게 된다.5.3.7 수동으로 REST API 호출에 TLS 적용 완벽히 격리된 인트라넷 안에서만 사용하는것이 아니라면 결국 transport 레이어가 아니라 HTTP 레이어에도 TLS를 적용해야 한다. ES의 http 레이어, 키바나에서 ES를 호출할떄 TLS 적용, 키바나와 브라우저 사이에 TLS를 수동으로 적용하는 절차를 알아보자인증서 선택 기본적으로 인증서 발급, 갱신, 관리에는 비용이 들어간다. 노드마다 도메인이 모두 발급되어있고, 모든 클라이언트가 이 도메인을 통해 REST 요청을 한다면 이들을 묶을수 있는 와일드카드 인증서를 하나 발급받아 모든 노드에서 사용할 수 있다.ES의 HTTP 레이어에 TLS 적용 예시에서는 직접 발급한 CA를 신뢰할 수 있도록 추가하고 작업한다. bin/elasticsearch-certutil http을 통해 인증서를 생성한다. 생성된 zip 파일의 압축을 풀어 설정을 추가한다. 파일은 지정한 노드별로 생성된다. kibana/elasticsearch-ca.pemelasticsearch/es01/http.p12elasticsearch/es02/http.p12elasticsearch/es03/http.p12 위에 서 생성된 파일을 기준으로 다음과 같이 TLS 수동 적용을 할 수 있다.xpack.security.http.ssl.enabled: true[xpack.sec](xpack.security.http.ssl.keystore.path: http.p12 그리고 다음을 실행해서 안내에 따라 ES 키스토어에 http.p12 파일의 암호를 추가한다.bin/elasticsearch-keystore add xpack.security.http.ssl.keystore.secure_password키바나가 ES를 호출할 떄 TLS 적용server.publicBaseUrl: \"https://10.0.0.3:5601\"elasticsearch.hosts: [\"https://10.0.0.3:9200\"]elasticsearch.ssl.certificateAuthorities: [\"/path/to/kibana/config/elasticsearch-ca.pem\"] 위 설정을 보면 elasticsearch.hosts에도 https 로 명시했는데, 이 후 키바나를 기동하면 키바나와 ES 사이에도 TLS로 통신함을 알 수 있다.Reference ." }, { "title": "[엘라스틱서치 바이블] 4. 데이터 다루기 -3(ES Client 라이브러리)", "url": "/posts/es-bible-4-3/", "categories": "DevLog, Elasticsearch", "tags": "Elasticsearch", "date": "2023-12-31 18:35:00 +0900", "snippet": "4. 데이터 다루기 -3(ES Client 라이브러리)4.5 서비스 코드에서 엘라스틱서치 클라이언트 이용 지금까지 배웠던 REST API를 호출해서 ES에 작업을 요청할수도 있겠으나, 이런 방식보다는 ES에서 공식적으로 제공하는 Client 라이브러리를 활용하면 좀 더 간편하게 코드를 작성할 수 있고, 유지보수도 더 쉽다. Java, Javascr...", "content": "4. 데이터 다루기 -3(ES Client 라이브러리)4.5 서비스 코드에서 엘라스틱서치 클라이언트 이용 지금까지 배웠던 REST API를 호출해서 ES에 작업을 요청할수도 있겠으나, 이런 방식보다는 ES에서 공식적으로 제공하는 Client 라이브러리를 활용하면 좀 더 간편하게 코드를 작성할 수 있고, 유지보수도 더 쉽다. Java, Javascript, Python, Go, .NET 등 다양한 언어에서 사용할수 있는 클라이언트 라이브러리를 제공함.JVM에서 지원하는 클라이언트transport 클라이언트 Deprecated된지 오래됨. 8버전부터는 완전히 제거됨.저수준 Rest Client 모든 버전 호환되나, 단순 httpClient 수준으로만 제공됨.고수준 Rest Client 7.15 버전부터 지원 중단 선언되었고, 자바 클라이언트로 전환이 예고되었다.자바 클라이언트 개발 초기 단계이고, 아직 지원하지 않는 기능이 많으나 최신버전의 ES로 신규 구축한다면 사용을 권장한다.4.5.1 저수준 Rest Client HttpComponents를 이용해서 HTTP로 통신한다. 요청을 ES API에 맞게 만들고 응답을 역직렬화하는 등의 작업은 클라이언트 사용자가 직접 해야 한다. 직접 만들어 호출하기 때문에 모든 ES버전과 호환된다.(실질적으로 api interface에 맞게 다 직접 만드는것이라 호환된다고 보기는 어렵다)저수준 RestClient Configdependencies { val esVersion = \"8.11.1\" // es 저수준 rest client implementation(\"org.elasticsearch.client:elasticsearch-rest-client:$esVersion\")}@Configurationclass EsConfig { companion object { private const val ES_CONNECTION_TIMEOUT = 5000 // 5s private const val ES_SOCKET_TIMEOUT = 5000 // 5s } @Bean(\"lowLevelEsRestClient\") fun lowLevelEsRestClient(): RestClient { return RestClient .builder( HttpHost(\"hosts01\", 9200, \"http\"), HttpHost(\"hosts02\", 9200, \"http\"), HttpHost(\"hosts03\", 9200, \"http\"), ) .setRequestConfigCallback { it.setConnectTimeout(ES_CONNECTION_TIMEOUT) it.setSocketTimeout(ES_SOCKET_TIMEOUT) } .build() }}저수준 RestClient 사용@Serviceclass LowLevelEsService( private val lowLevelEsRestClient: RestClient,) { companion object : KLogging() fun getClusterSettings(): String { val req = Request(\"GET\", \"/_cluster/settings\") req.addParameters( mapOf( \"pretty\" to \"true\", ), ) logger.debug { \"[ES TEST] start request\" } val res = lowLevelEsRestClient.performRequest(req) logger.debug { \"[ES TEST] response : $res\" } return getBody(res) } fun asyncUpdateSetting() { val req = Request(\"PUT\", \"/_cluster/settings\") val requestBody = \"\"\" { \"transient\": { \"cluster.routing.allocation.enable\": \"all\" } } \"\"\".trimIndent() req.entity = NStringEntity(requestBody, ContentType.APPLICATION_JSON) logger.debug { \"[ES TEST] start request\" } val cancellable = lowLevelEsRestClient.performRequestAsync( req, object : ResponseListener { override fun onSuccess(response: Response) { logger.debug { \"[ES TEST] response : $response\" } } override fun onFailure(ex: Exception) { logger.error(\"[ES TEST] es error\", ex) } }, ) logger.debug { \"[ES TEST] thread sleep\" } Thread.sleep(1000L) cancellable.cancel() } private fun getBody(response: Response): String { val statusCode = response.statusLine.statusCode val responseBody = EntityUtils.toString(response.entity, StandardCharsets.UTF_8) logger.debug { \"[ES TEST] statusCode : $statusCode, body : $responseBody\" } return responseBody }} 위 예제에서 동기식 / 비동기식으로 API를 호출해보는것을 테스트했다. 비동기식인 경우 전달받은 Cancellable 객체의 cancel()을 통해 요청을 취소할수 있다. 저수준 RestClient는 thread-safe하므로 Spring 환경에서 위처럼 bean으로 한번 등록해두고 재사용할 수 있다. 저수준 RestClient는 단순 httpClient을 쓰는것과 차이가 없다. 즉 ES 기능을 사용하려면 Document를 보면서 API 스펙에 맞게 직접 구현해줘야해서 불편할수 있다.4.5.2 고수준 Rest Client 고수준 RestClient는 ES API를 라이브러이의 API로 노출한다. 아예 클라이언트 라이브러리가 ES에 딱 맞게 설계되어있다. 다만, 위와 같은 특징때문에 ES 버전과 강하게 결합되어 있어 버전 호환성 이슈가 존재한다. 사용하는 ES 클러스터의 버전과 클라이언트 라이브러리 버전을 맞춰야만 호환성 이슈로부터 안전하다. 고수준 Rest Client 버전 관련 정보 ES 7.15 버전부터 고수준 RestClient 지원 중단 선언됨.(JavaClient로 대체 필요) ES5 ~ 7.15 사이의 버전을 사용한다면 고수준 RestClient을 사용하면 된다. ES8 이상의 버전을 싸용한다면 자바클라이언트를 사용하는것이 좋다.고수준 Rest Client Configdependencies { val esHighLevelClientVersion = \"7.17.16\" // es 고수준 rest client implementation(\"org.elasticsearch.client:elasticsearch-rest-high-level-client:$esHighLevelClientVersion\") // 수동 jar 설치 후 적용하는 방식// implementation(fileTree(mapOf(\"dir\" to \"manual-build\", \"includes\" to listOf(\"*.jar\"))))// implementation(\"org.elasticsearch:elasticsearch:$esVersion\")}@Configurationclass EsConfig { companion object { private const val ES_CONNECTION_TIMEOUT = 5000 // 5s private const val ES_SOCKET_TIMEOUT = 5000 // 5s } @Bean(\"highLevelEsRestClient\") fun highLevelEsRestClient(): RestHighLevelClient { val builder = createBaseRestClientBuilder() return RestHighLevelClientBuilder(builder.build()) // NOTE es 8 이상일 경우 서버 호환을 위해 true로 설정 .setApiCompatibilityMode(true) .build() } @Bean fun bulkProcessor( @Qualifier(\"highLevelEsRestClient\") highLevelEsRestClient: RestHighLevelClient, ): BulkProcessor { val bulkAsync = { request: BulkRequest, listener: ActionListener&lt;BulkResponse&gt; -&gt; highLevelEsRestClient.bulkAsync(request, RequestOptions.DEFAULT, listener) Unit } return BulkProcessor .builder(bulkAsync, MyEsBulkListener(), \"myBulkProcessor\") .setBulkActions(50000) .setBulkSize(ByteSizeValue.ofMb(50L)) .setFlushInterval(TimeValue.timeValueMillis(5000L)) .setConcurrentRequests(1) .setBackoffPolicy(BackoffPolicy.exponentialBackoff()) .build() } private fun createBaseRestClientBuilder(): RestClientBuilder { return RestClient .builder( HttpHost(\"localhost\", 9200, \"http\"), ) .setRequestConfigCallback { it.setConnectTimeout(ES_CONNECTION_TIMEOUT) it.setSocketTimeout(ES_SOCKET_TIMEOUT) } }}고수준 Rest Client 사용@Serviceclass HighLevelEsService( private val highLevelEsRestClient: RestHighLevelClient, private val bulkProcessor: BulkProcessor,) { companion object : KLogging() fun getSample(): String { val req = GetRequest() .index(\"my-index-01\") .id(\"document-id-01\") .routing(\"abc123\") val res = highLevelEsRestClient.get(req, RequestOptions.DEFAULT) logger.debug { \"[ES_TEST] res : $res\" } return res.sourceAsString } fun searchSample(): List&lt;MutableMap&lt;String, Any&gt;&gt; { val queryBuilder = QueryBuilders.boolQuery() .must(TermQueryBuilder(\"filedOne\", \"hello\")) .should(MatchQueryBuilder(\"filedTwo\", \"hello world\").operator(Operator.AND)) .should(RangeQueryBuilder(\"filedThree\").gte(100).lt(200)) .minimumShouldMatch(1) val searchSourceBuilder = SearchSourceBuilder() .from(0) .size(10) .query(queryBuilder) val searchRequest = SearchRequest() .indices(\"my-index-01\", \"my-index-02\") .routing(\"abc123\") .source(searchSourceBuilder) val res = highLevelEsRestClient.search(searchRequest, RequestOptions.DEFAULT) logger.debug { \"[ES_TEST] res : $res\" } val searchHits = res.hits val totalHits = searchHits.totalHits logger.debug { \"[ES_TEST] totalHits : $totalHits\" } return searchHits .hits .map { it.sourceAsMap } } fun bulk() { val bulkRequest = BulkRequest() bulkRequest.add( UpdateRequest() .index(\"my-index-01\") .id(\"document-id-01\") .routing(\"abc123\") .doc(mapOf(\"hello\" to \"elasticsearch\")), ) val bulkResponse = highLevelEsRestClient.bulk(bulkRequest, RequestOptions.DEFAULT) if (bulkResponse.hasFailures()) { logger.error(\"[ES_TEST] ${bulkResponse.buildFailureMessage()}\") } bulkResponse.items logger.debug { \"[ES_TEST] bulkResponse : ${ToStringBuilder.reflectionToString(bulkResponse)} \" } } fun bulkProcessor(id: String) { val source = mapOf&lt;String, Any&gt;( \"hello\" to \"world\", \"world\" to 123, \"name\" to \"name-$id\", ) val indexRequest = IndexRequest(\"my-index-01\") .id(id) .routing(\"abc123\") .source(source, XContentType.JSON) bulkProcessor.add(indexRequest) }} ES Rest API를 호출할때 body로 전달했던 내용들을 Builder Pattern을 제공해준다. 저수준 RestClient 보다 훨씬 직관적이고, 추상화된 API를 제공하고 있다.bulk API 호출하기 위에 구현한 HighLevelEsService의 내용을 참고하면 BulkRequest 라는 클래스를 통해 bulk 처리를 하는 방법과 BulkProcessor을 통해 처리하는 2가지 방법을 제공한다.BulkProcessor 관련 BulkProcessor는 필요한 bulk 처리에 대해서 add하게 되면 BulkProssor가 지정된 기준에 맞춰 bulk 요청을 만들어서 보낸다. 이런 과정을 BulkProcessor가 flush한다고 표현한다. BulkProcessor thread-safety 관련 개발/테스트하는 과정에서 BulkProcessor을 Spring bean으로 등록하고 사용할수 있는지에 대해서 확인(thread-safety) 핵심적인 내용은 아래 코드를 참고할수 있는데, bulk 대상을 add할때 lock을 활용하고 있는것으로 보임. 그래서 thread-safety를 보장할수 있을것으로 보임 다만, bulkProcessor 특성상 bulk 처리를 내부에서 알아서 처리하므로, 실시간으로 사용자의 응답으로 서빙하는 케이스에서는 사용을 지양하는것이 맞지 않을까 생각됨. /** * A bulk processor is a thread safe bulk processing class, allowing to easily set when to \"flush\" a new bulk request * (either based on number of actions, based on the size, or time), and to easily control the number of concurrent bulk * requests allowed to be executed in parallel. * &lt;p&gt; * In order to create a new bulk processor, use the {@link Builder}. */public class BulkProcessor implements Closeable {// ...(중략) private void internalAdd(DocWriteRequest&lt;?&gt; request) { // bulkRequest and instance swapping is not threadsafe, so execute the mutations under a lock. // once the bulk request is ready to be shipped swap the instance reference unlock and send the local reference to the handler. Tuple&lt;BulkRequest, Long&gt; bulkRequestToExecute = null; lock.lock(); try { ensureOpen(); bulkRequest.add(request); bulkRequestToExecute = newBulkRequestIfNeeded(); } finally { lock.unlock(); } // execute sending the local reference outside the lock to allow handler to control the concurrency via it's configuration. if (bulkRequestToExecute != null) { execute(bulkRequestToExecute.v1(), bulkRequestToExecute.v2()); } }}BulkProcessor.Listenerclass MyEsBulkListener : BulkProcessor.Listener { companion object : KLogging() override fun beforeBulk(executionId: Long, request: BulkRequest) { logger.debug { \"[ES_TEST] before bulk\" } } override fun afterBulk(executionId: Long, request: BulkRequest, response: BulkResponse) { if (!response.hasFailures()) { logger.debug { \"[ES_TEST] bulk success\" } } else { logger.error { \"[ES_TEST] bulk failures\" } val failedItems = response.items .filter { it.isFailed } logger.error { \"[ES_TEST] failed items : ${ToStringBuilder.reflectionToString(failedItems)}\" } } } override fun afterBulk(executionId: Long, request: BulkRequest?, failure: Throwable?) { logger.debug { \"[ES_TEST] after bulk\" } }} BulkProcessor에 Listner를 등록하여 처리 과정 중에 전처리/후처리 등을 직접 구현하여 적용할 수 있다.ES 8버전에서 고수준 RestClient 사용하기 ES에서 고수준 RestClient는 지원 중단을 선언하였고, 더이상 maven repo를 통해 신규 버전이 배포되지 않고 있다. 다만, 직접 es code를 checkout 받아서 수동으로 build하면 ES8 버전의 jar을 사용할수도 있다.# checkoutgit clone https://github.com/elastic/elasticsearch.gitgit checkout tags/v8.11.1# build./gradleew clean :client:rest-high-level:build -x test 위 빌드 결과물인 build/distributions/elasticsearch-rest-high-level-client-8.11.1-SNAPSHOT.jar 을 프로젝트에 포함시키면 된다.dependencies { val esVersion = \"8.11.1\" // 수동 jar 설치 후 적용하는 방식 implementation(fileTree(mapOf(\"dir\" to \"manual-build\", \"includes\" to listOf(\"*.jar\")))) implementation(\"org.elasticsearch:elasticsearch:$esVersion\")} 다만 , 위 방식으로 테스트해보니 elasticsearch 서버 라이브러리를 통쨰로 넣어야 동작을 하는데, IDE에서 많이 힘들어해서 이 방식은 좋은 방법이 아닌것 같다. 따라서, 그냥 JavaClient을 사용하는것이 현명해보인다.4.5.3 자바 클라이언트 ES 7.15버전부터 출시되었고, 기존의 고수준 restClient을 대체하게 된 클라이언트이다.(2021.9월 최초 출시) 8 이상을 사용한다면 고수준 restClient와 혼용하더라도 점진적으로 새 자바 클라이언트를 도입하는 것이 좋다. 아직은 초기라서 고수준 restClient에 비해 불편함이 있을수 있는데 이 부분을 감안해서 선택하는것이 좋다.자바 클라이언트의 초기화dependencies { val esVersion = \"8.11.1\" val esHighLevelClientVersion = \"7.17.16\" // es 저수준 rest client implementation(\"org.elasticsearch.client:elasticsearch-rest-client:$esVersion\") // es 고수준 rest client implementation(\"org.elasticsearch.client:elasticsearch-rest-high-level-client:$esHighLevelClientVersion\") // java client implementation(\"co.elastic.clients:elasticsearch-java:$esVersion\")} 위와 같이 java client 디펜던시 설정을 추가해준다.@Configurationclass EsConfig( private val objectMapper: ObjectMapper,) { companion object { private const val ES_CONNECTION_TIMEOUT = 5000 // 5s private const val ES_SOCKET_TIMEOUT = 5000 // 5s private const val CLIENT_BUFFER_SIZE = 500 * 1024 * 1024 // 500MB } @Bean(\"lowLevelEsRestClient\") fun lowLevelEsRestClient(): RestClient { return createBaseRestClientBuilder() .build() } @Bean(\"esJavaClient\") fun esJavaClient( @Qualifier(\"lowLevelEsRestClient\") lowLevelEsRestClient: RestClient, ): ElasticsearchClient { val restClientOptions = RestClientOptions( RequestOptions.DEFAULT .toBuilder() .setHttpAsyncResponseConsumerFactory( HttpAsyncResponseConsumerFactory.HeapBufferedResponseConsumerFactory(CLIENT_BUFFER_SIZE), ) .build(), ) val transport = RestClientTransport(lowLevelEsRestClient, JacksonJsonpMapper(objectMapper), restClientOptions) return ElasticsearchClient(transport) } @Bean fun bulkIngester( @Qualifier(\"esJavaClient\") esJavaClient: ElasticsearchClient, ): BulkIngester&lt;String&gt; { val listener = StringBulkIngestListener&lt;String&gt;() return BulkIngester.of { it.client(esJavaClient) .maxOperations(200) .maxConcurrentRequests(1) .maxSize(5 * 1024 * 1024) // 5MB .flushInterval(5L, TimeUnit.SECONDS) .listener(listener) } } @Bean(\"highLevelEsRestClient\") fun highLevelEsRestClient( @Qualifier(\"lowLevelEsRestClient\") lowLevelEsRestClient: RestClient, ): RestHighLevelClient { return RestHighLevelClientBuilder(lowLevelEsRestClient) // NOTE es 8 이상일 경우 서버 호환을 위해 true로 설정 .setApiCompatibilityMode(true) .build() } private fun createBaseRestClientBuilder(): RestClientBuilder { return RestClient .builder( HttpHost(\"localhost\", 9200, \"http\"), ) .setRequestConfigCallback { it.setConnectTimeout(ES_CONNECTION_TIMEOUT) it.setSocketTimeout(ES_SOCKET_TIMEOUT) } }} spring application configuration은 위와 같이 설정할 수 있다. 위 예시는 책 예제를 기반으로 작성한것으로 고수준 RestClient와 JavaClient 모두를 사용하는 방식으로 작성되어있는데 상황에 따라 적절히 선택하면 된다.@SpringBootApplication( scanBasePackages = [ \"com.starter.es\", \"com.starter.core.common\", \"com.starter.core.jasypt\", \"com.starter.core.security\", ], exclude = [ DataSourceAutoConfiguration::class, DataSourceTransactionManagerAutoConfiguration::class, HibernateJpaAutoConfiguration::class, ElasticsearchClientAutoConfiguration::class, ],)class EsApplicationfun main(args: Array&lt;String&gt;) { runApplication&lt;EsApplication&gt;(*args)} application 설정시 ElasticsearchClientAutoConfiguration::class을 exclude를 해주어야 한다. spring auto configuration을 사용한다면 위 설정 말고도 적절히 라이브러리 디펜던시 추가가 필요하고, bean 설정도 변경이 필요할것이다. 개인적으로는 auto configuration보다는 직접 bean 정의를 하는것이 명확해서 좋은 것 같음. 자바 클라이언트 사용하기@Serviceclass EsJavaClientService( private val elasticsearchClient: ElasticsearchClient, private val bulkIngester: BulkIngester&lt;String&gt;,) { companion object : KLogging() { private const val INDEX_NAME = \"my-index\" } fun indexExample(): String { val indexRequest = IndexRequest.Builder&lt;MyIndexClass&gt;() .index(INDEX_NAME) .id(\"my-id-1\") .routing(\"my-routing-1\") .document(MyIndexClass(\"hello\", 1L, createNow())) .build() val response = elasticsearchClient.index(indexRequest) val result = response.result() logger.debug { \"[ES_TEST] response : $response , resultName : ${result.name}\" } return response.toString() } fun getSample(id: String): MyIndexClass? { val getRequest = GetRequest.Builder() .index(INDEX_NAME) .id(id) .routing(\"my-routing-1\") .build() val response = elasticsearchClient.get(getRequest, MyIndexClass::class.java) val result = response.source() logger.debug { \"[ES_TEST] response : $response , result : $result\" } return result } fun bulk() { val createOperation = CreateOperation.Builder&lt;MyIndexClass&gt;() .index(INDEX_NAME) .id(\"my-id-2\") .routing(\"my-routing-2\") .document(MyIndexClass(\"world\", 2L, createNow())) .build() val indexOperation = IndexOperation.Builder&lt;MyIndexClass&gt;() .index(INDEX_NAME) .id(\"my-id-3\") .routing(\"my-routing-3\") .document(MyIndexClass(\"world\", 4L, createNow())) .build() val updateAction = UpdateAction.Builder&lt;MyIndexClass, MyPartialIndexClass&gt;() .doc(MyPartialIndexClass(\"world updated\")) .build() val updateOperation = UpdateOperation.Builder&lt;MyIndexClass, MyPartialIndexClass&gt;() .index(INDEX_NAME) .id(\"my-id-1\") .routing(\"my-routing-1\") .action(updateAction) .build() val bulkOpOne = BulkOperation.Builder().create(createOperation).build() val bulkOpTwo = BulkOperation.Builder().index(indexOperation).build() val bulkOpThree = BulkOperation.Builder().update(updateOperation).build() val operations = listOf&lt;BulkOperation&gt;(bulkOpOne, bulkOpTwo, bulkOpThree) val bulkResponse = elasticsearchClient.bulk { it.operations(operations) } for (item in bulkResponse.items()) { logger.debug { \"[ES_TEST] results : ${item.result()}, error : ${item.error()}\" } } } fun search(fieldOneValue: String): List&lt;MyIndexClass&gt; { val searchRequest = SearchRequest.Builder() .index(INDEX_NAME) .from(0) .size(10) .query { q -&gt; q.term { t -&gt; t.field(\"fieldOne\") .value { v -&gt; v.stringValue(fieldOneValue) } } } .build() val response = elasticsearchClient.search(searchRequest, MyIndexClass::class.java) val hits = response.hits().hits() for (item in hits) { logger.debug { \"[ES_TEST] source : ${item.source()}\" } } return hits.mapNotNull { it.source() } } fun bulkWithIngester() { val startDatetime = LocalDateTime.now() val epochSecond = startDatetime.toEpochSecond(ZoneOffset.UTC) logger.debug { \"[ES_TEST] startEpochSecond : $epochSecond\" } for (number in 0L..1000L) { val bulkOperation = BulkOperation.of { builder -&gt; builder.index { indexOpBUilder -&gt; indexOpBUilder .index(INDEX_NAME) .id(\"my-id-$number\") .routing(\"my-routing-$number\") .document(MyIndexClass(\"world\", concatNumber(epochSecond, number), createNow())) } } bulkIngester.add(bulkOperation, \"my-context-$number\") } logger.debug { \"[ES_TEST] [${LocalDateTime.now()}] sleep 10 seconds ...\" } Thread.sleep(10000L) for (number in 1001L..1500L) { val bulkOperation = BulkOperation.of { builder -&gt; builder.index { indexOpBUilder -&gt; indexOpBUilder .index(INDEX_NAME) .id(\"my-id-$number\") .routing(\"my-routing-$number\") .document(MyIndexClass(\"world\", concatNumber(epochSecond, number), createNow())) } } bulkIngester.add(bulkOperation, \"my-context-$number\") } logger.debug { \"[ES_TEST] [${LocalDateTime.now()}] sleep 10 seconds ...\" } Thread.sleep(10000L) logger.debug { \"[ES_TEST] It's completed.\" } // bean이므로 굳이 닫지않음. // bulkIngester.close() } private fun createNow() = ZonedDateTime.now(ZoneOffset.UTC) private fun concatNumber(baseNumber: Long, number: Long): Long { return String.format(\"%d%04d\", baseNumber, number).toLong() }} 위 예시에는 단건문서 색인, 조회, bulk API 호출, search, bulkIngester 사용하는 예제까지 모두 포함되어있다.JavaClient의 주요 특징 라이브러리 코드를 살펴보면 java 8부터 지원하는 Function을 잘 지원하고 있다. 책에서는 람다의 깊이를 _0, _1, _2 ...와 같은 방식으로 표현하라고 되어있으나 이는 가독성에 좋은 방법이 아니라고 생각된다. 이때 Function도 적당히 사용해야 가독성이 좋으니 가독성을 유지할수 있는 적당한 수준으로 작성하자. Function 사용 대신 실제 Builder 클래스를 build()하는 방식으로도 구현 가능하다. Operation을 처리할떄 Generic Type을 지원하는 것이 주된 특징이다. 기존 고수준 restClient에서는 이런 타입 핸들링이 불가능했던것으로 보이고, 이걸 역직렬화 처리를 Map 또는 JsonString으로부터 해줬어야 했으나 이런 부분이 개선된 부분인것 같다. BulkIngester 사용 관련 ES 8.7 버전부터 JavaClient에서 Bulk 처리를 위한 BulkIngester가 추가되었다. maxOperations, maxConcurrentRequests, maxSize, flushInterval 등의 builder 메소드를 사용해서 bulk API 처리 정책을 정할 수 있다. 고수준 RestClient의 BulkProcessor처럼 Listener을 지정하여 전/후처리에 대한 동작을 정의할 수 있다. BulkProcessor와 다르게 Generic을 지원한다. 그래서 인덱스 클래스 타입을 지정하여 특정 인덱스에 대한 타입 안정성을 가져갈수 있겠으나, 이런 경우 범용성은 떨어질수 있을것 같다. 필요에 따라서 판단해야겠지만 범용적인 하나의 BulkIngester을 사용해야 한다면 Generic Type을 String으로 지정하여 사용할 수 있을것 같다. BulkIngester thread-safety BulkIngester도 개발/테스트 과정에서 확인해보니 thread-safety를 보장하여 spring bean으로 등록하고 사용하는것이 문제 없는것으로 확인했다. 코드를 확인해보니 add하기 전에 상태를 체크해서 동기화 처리를 하고 있음.public class BulkIngester&lt;Context&gt; implements AutoCloseable { // ...(중략) // Synchronization objects private final ReentrantLock lock = new ReentrantLock(); private final FnCondition addCondition = new FnCondition(lock, this::canAddOperation); private final FnCondition sendRequestCondition = new FnCondition(lock, this::canSendRequest); private final FnCondition closeCondition = new FnCondition(lock, this::closedAndFlushed); // ...(중략) public void add(BulkOperation operation, Context context) { if (isClosed) { throw new IllegalStateException(\"Ingester has been closed\"); } IngesterOperation ingestOp = IngesterOperation.of(operation, client._jsonpMapper()); addCondition.whenReady(() -&gt; { // ...(중략) }}class FnCondition { // ...(중략) public &lt;T&gt; T whenReadyIf(BooleanSupplier canRun, Supplier&lt;T&gt; fn) { lock.lock(); try { if (canRun != null &amp;&amp; !canRun.getAsBoolean()) { return null; } invocations++; boolean firstLoop = true; while (!ready.getAsBoolean()) { if (firstLoop) { contentions++; firstLoop = false; } condition.awaitUninterruptibly(); } if (canRun != null &amp;&amp; !canRun.getAsBoolean()) { return null; } return fn.get(); } finally { lock.unlock(); } }}Reference Github Sample Code 샘플코드는 위 Repo에서 확인할수 있다. BulkProcessor thread-safety 관련 BulkIngester thread-safety 관련" }, { "title": "[엘라스틱서치 바이블] 4. 데이터 다루기 -2(집계)", "url": "/posts/es-bible-4-2/", "categories": "DevLog, Elasticsearch", "tags": "Elasticsearch", "date": "2023-12-24 21:19:00 +0900", "snippet": "4. 데이터 다루기 -24.4 집계 ES는 검색을 수행한 뒤에 그 결과를 집계(aggregation)하는 다양한 방법을 제공한다.4.4.1 집계 기본 집계는 검색의 연장선 집계의 대상을 추려낼 검색 조건을 검색 API에 담은 뒤 집계 조건을 추가해서 호출한다.sum### sumGET /kibana_sample_data_ecommerce/_sear...", "content": "4. 데이터 다루기 -24.4 집계 ES는 검색을 수행한 뒤에 그 결과를 집계(aggregation)하는 다양한 방법을 제공한다.4.4.1 집계 기본 집계는 검색의 연장선 집계의 대상을 추려낼 검색 조건을 검색 API에 담은 뒤 집계 조건을 추가해서 호출한다.sum### sumGET /kibana_sample_data_ecommerce/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 0, \"query\": { \"term\": { \"currency\" : { \"value\": \"EUR\" } } }, \"aggs\": { \"my-sum-aggregation-name\": { \"sum\" : { \"field\":\"taxless_total_price\" } } }}{ \"took\": 26, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 4675,\t \"relation\": \"eq\"\t},\t\"max_score\": null,\t\"hits\": [] }, \"aggregations\": {\t\"my-sum-aggregation-name\": {\t \"value\": 350884.12890625\t} }} search API 요청 body에 “aggs”만 추가하고, size을 0으로 지정한 것을 확인할 수 있다.집계 요청시 주의사항 size을 0으로 지정하면 검색에 상위 매칭된 문서가 무엇인지 받아볼 수 없다. 매칭된 문서와 관계 없이 조건에 매치되는 모든 문서에 대한 집계 작업을 할 수 있다. 집계 요청에서는 매칭 문서를 볼 니즈가 없으므로 대부분 집계 요청에서는 size를 0으로 지정하는 것이 이득이다. 요청 한번에 여러 집계를 요청할 수 있다. 집계를 구분할 수 있는 이름을 붙여준다.(my-sum-aggregation-name) 집계 작업은 검색 쿼리에 매칭된 모든 문서에 대해 수행하므로 과도한 양을 대상으로 집계를 수행할 경우 전체 클러스터 성능에 영향을 줄수 있다. 키바나 대시보드같은 시각화도구에서도 발생할 수 있음. 통제되지 않은 사용자가 집계를 할 수 없도록 해야 한다. 4.4.2 메트릭 집계 문서에 대한 산술적인 연산을 수행한다.avg, max, min, sum 집계 평균, 최대값, 최소값, 합계 등을 집계할 수 있다.### avgGET /kibana_sample_data_ecommerce/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 0, \"query\": { \"term\": { \"currency\" : { \"value\": \"EUR\" } } }, \"aggs\": { \"my-avg-aggregation-name\": { \"avg\" : { \"field\":\"taxless_total_price\" } } }}stats 집계 지정한 필드의 평균, 최댓값, 최솟값, 합, 개수를 모두 계산해서 반환한다.### statsGET /kibana_sample_data_ecommerce/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 0, \"query\": { \"term\": { \"currency\" : { \"value\": \"EUR\" } } }, \"aggs\": { \"my-stats-aggregation-name\": { \"stats\" : { \"field\":\"taxless_total_price\" } } }}{ \"took\": 10, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 4675,\t \"relation\": \"eq\"\t},\t\"max_score\": null,\t\"hits\": [] }, \"aggregations\": {\t\"my-stats-aggregation-name\": {\t \"count\": 4675,\t \"min\": 6.98828125,\t \"max\": 2250.0,\t \"avg\": 75.05542864304813,\t \"sum\": 350884.12890625\t} }} 위와 같이 여러 숫자 값을 한꺼번에 반환하는 메트릭 집계를 다중 값 숫자 메트릭 집계(multi-value numeric metric aggregation)이라 한다.cardinality 집계 cardinality 집계는 지정한 필드가 가진 고유한 값의 개수를 계산해 반환한다. 이 값은 HyperLogLog++ 알고리즘을 사용해 추정한 근사값이다. ### cardinalityGET /kibana_sample_data_ecommerce/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 0, \"query\": { \"term\": { \"currency\" : { \"value\": \"EUR\" } } }, \"aggs\": { \"my-cardinality-aggregation-name\": { \"cardinality\" : { \"field\":\"customer_id\", \"precision_threshold\": 3000 } } }} precision_threshold 옵션은 정확도를 조절하기 위해 사용된다. 정확도와 메모리 사용량에 비례한다. 정확도를 올리기 위해 무작정 많이 높힐 필요가 없고, 최종 cardinality보다 높다면 정확도는 충분하다. 기본값은 3000이고, 최댓값은 40000이다. HyperLogLog++ 알고리즘 HyperLogLog Naver D2 레퍼런스 HyperLogLog의 핵심 아이디어는 정수의 상위 비트에 대한 확률적인 접근에서부터 시작한다. 어떤 정수가 있을 때 상위 첫 번째 비트가 0인 정수 또는 상위 첫 번째 비트가 1인 정수는 각각 전체 표현 가능한 정수 중 50%를 차지한다. 마찬가지로 상위 비트가 00, 01, 10, 11로 시작하는 정수 각각에 대한 최대 cardinality는 전체 표현 가능한 정수의 25%가 된다. HyperLogLog에서는 이렇게 상위 몇 비트(b)를 사용할 것인지에 따라 레지스터의 개수(m=2b)가 결정된다. 그리고 레지스터별 최대 cardinality는 ‘전체 표현 가능 정수 개수’ ×개이다.(레지스터 개수를 늘이면 좀 더 정해에 가까운 추정 값을 계산할 수 있음) hashSet 대비 오차는 있으나 매우 효율적으로 근사값을 구할수 있음.4.4.3 버킷 집계 버킷 집계는 문서를 특정 기준으로 쪼개어 여러 부분 집합으로 나눈다. 이 부분 집합을 버킷이라고 하고, 각 버킷에 포함된 문서를 대상으로 별도의 하위 집계(sub-aggregation)을 수행하여 집계하는 방식이다.range 집계 지정된 필드값을 기준으,로 문서를 원하는 버킷 구간으로 쪼개서 집계한다. 버킷 구간을 나눌 기준이 될 필드와 기준값을 지정해 요청한다.### rangeGET /kibana_sample_data_flights/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"distnace-kilometers-range\": { \"range\" : { \"field\":\"DistanceKilometers\", \"ranges\": [ { \"to\": 5000 }, { \"from\": 5000, \"to\": 10000 }, { \"to\": 10000 } ] }, \"aggs\": { \"average-ticket-price\": { \"avg\": { \"field\": \"AvgTicketPrice\" } } } } }}{ \"took\": 86, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 10000,\t \"relation\": \"gte\"\t},\t\"max_score\": null,\t\"hits\": [] }, \"aggregations\": {\t\"distnace-kilometers-range\": {\t \"buckets\": [\t\t{\t\t \"key\": \"*-5000.0\",\t\t \"to\": 5000.0,\t\t \"doc_count\": 4039,\t\t \"average-ticket-price\": {\t\t\t\"value\": 513.3039915741715\t\t }\t\t},\t\t{\t\t \"key\": \"*-10000.0\",\t\t \"to\": 10000.0,\t\t \"doc_count\": 10060,\t\t \"average-ticket-price\": {\t\t\t\"value\": 611.5759058298221\t\t }\t\t},\t\t{\t\t \"key\": \"5000.0-10000.0\",\t\t \"from\": 5000.0,\t\t \"to\": 10000.0,\t\t \"doc_count\": 6021,\t\t \"average-ticket-price\": {\t\t\t\"value\": 677.4985535093724\t\t }\t\t}\t ]\t} }} 요청 body을 보면 range 밑에 “aggs”가 하나 더 있는데, 이 부분이 하위 집계이다. 위 요청 내용을 보면 DistanceKilometers 값을 기준으로 3개의 버킷을 쪼개고 아 내부에서 AvgTicketPrice avg 값을 집계한것을 알 수 있다. 하위 집계에 또 버킷 집계를 넣으면 다시 그 하위 집계를 지정하는 것도 가능하다. 다만, 하위 집계의 깊이가 너무 깊어지면 성능에 심각한 문제가 생길수 있으니 지양해야 한다. data_range 집계 range 집계와 유사하나 date 타입 필드를 대상으로 사용하는 점과 from, to에 간단한 날짜 시간 계산식을 사용할수 있다는 차이가 있다.### date_rangeGET /kibana_sample_data_ecommerce/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"distnace-kilometers-range\": { \"date_range\" : { \"field\":\"order_date\", \"ranges\": [ { \"to\": \"now-10d/d\" }, { \"from\": \"now-10d/d\", \"to\": \"now\" }, { \"to\": \"now\" } ] } } }} 위 요청에서 주목할 부분은 now를 사용한 점인데, now가 포함된 집계 요청은 캐시되지 않는다.(호출 시점마다 now 값이 다르기 떄문) 새로운 데이터가 들어와서 인덱스의 상태가 달라져도 샤드 요청 캐시는 무효화된다.histogram 집계 지정한 필드의 값을 기준으로 버킷을 나눈다는 점에서 range 집계와 유사하다. 다른 점은 버킷 구분의 경계 기준값을 직접 지정하는 것이 아니라 버킷의 간격을 지정해서 집계를 나눈다는 점이다.GET /kibana_sample_data_flights/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"my-histogram\": { \"histogram\" : { \"field\":\"DistanceKilometers\", \"interval\": 1000 } } }}{ \"took\": 22, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 10000,\t \"relation\": \"gte\"\t},\t\"max_score\": null,\t\"hits\": [] }, \"aggregations\": {\t\"my-histogram\": {\t \"buckets\": [\t\t{\t\t \"key\": 0.0,\t\t \"doc_count\": 1799\t\t},\t\t{\t\t \"key\": 1000.0,\t\t \"doc_count\": 1148\t\t},\t\t{\t\t \"key\": 2000.0,\t\t \"doc_count\": 529\t\t},\t\t{\t\t \"key\": 3000.0,\t\t \"doc_count\": 241\t\t},\t\t{\t\t \"key\": 4000.0,\t\t \"doc_count\": 322\t\t},\t\t{\t\t \"key\": 5000.0,\t\t \"doc_count\": 411\t\t},\t\t{\t\t \"key\": 6000.0,\t\t \"doc_count\": 1062\t\t},\t\t{\t\t \"key\": 7000.0,\t\t \"doc_count\": 1718\t\t},\t\t{\t\t \"key\": 8000.0,\t\t \"doc_count\": 1343\t\t},\t\t{\t\t \"key\": 9000.0,\t\t \"doc_count\": 1487\t\t},\t\t{\t\t \"key\": 10000.0,\t\t \"doc_count\": 647\t\t},\t\t{\t\t \"key\": 11000.0,\t\t \"doc_count\": 653\t\t},\t\t{\t\t \"key\": 12000.0,\t\t \"doc_count\": 323\t\t},\t\t{\t\t \"key\": 13000.0,\t\t \"doc_count\": 302\t\t},\t\t{\t\t \"key\": 14000.0,\t\t \"doc_count\": 278\t\t},\t\t{\t\t \"key\": 15000.0,\t\t \"doc_count\": 282\t\t},\t\t{\t\t \"key\": 16000.0,\t\t \"doc_count\": 327\t\t},\t\t{\t\t \"key\": 17000.0,\t\t \"doc_count\": 41\t\t},\t\t{\t\t \"key\": 18000.0,\t\t \"doc_count\": 69\t\t},\t\t{\t\t \"key\": 19000.0,\t\t \"doc_count\": 32\t\t}\t ]\t} }} 위처럼 interval을 지정하면 해당 필드의 최솟값과 최댓값을 확인한 후 그 사이를 지정한 interval에 지정한 간격으로 쪼개서 버킷을 나눈다. 위치를 조정하고 싶은 경우 offset을 지정하여 사용할 수 있다.### histogram + offsetGET /kibana_sample_data_flights/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"my-histogram\": { \"histogram\" : { \"field\":\"DistanceKilometers\", \"interval\": 1000, \"offset\": 50 } } }} offset을 지정하지 않는 경우 [0, 1000], [1000-2000], ...로 구간이 생성되고, offset 50으로 지정한 경우 [50 1050], [1050, 2050], ...으로 구간이 생성되었다. 이 밖에도 min_doc_coun를 지정해서 버킷 내 문서 개수가 일정 이하인 버킷은 결과에서 제외할 수 있다.data_histogram 집계### date_histogramGET /kibana_sample_data_ecommerce/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"my-date-histogram\": { \"date_histogram\" : { \"field\":\"order_date\", \"calendar_interval\": \"month\" } } }}{ \"took\": 34, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 4675,\t \"relation\": \"eq\"\t},\t\"max_score\": null,\t\"hits\": [] }, \"aggregations\": {\t\"my-date-histogram\": {\t \"buckets\": [\t\t{\t\t \"key_as_string\": \"2023-12-01T00:00:00.000Z\",\t\t \"key\": 1701388800000,\t\t \"doc_count\": 3751\t\t},\t\t{\t\t \"key_as_string\": \"2024-01-01T00:00:00.000Z\",\t\t \"key\": 1704067200000,\t\t \"doc_count\": 924\t\t}\t ]\t} }} 위처럼 날짜 기준으로 집계를 할수 있다.calendar_interval 12h 같은 단위값은 지정할 수 없음. minute 또는 1m : 분단위 hour 또는 1h : 시간 단위 day 또는 1d : 일 단위 month 또는 1M : 월 단위 quarter 또는 1q : 분기 단위 year 또는 1y : 연 단위fixed_interval calendar_interval에서는 12h 같은 값을 지정할 수 없으나 fixed_interval에서는 지정이 가능하다. ex. \"fixed_interval\": \"3h\"로 지정하면 3시간 간격으로 버킷 구간을 쪼갤 수 있다.terms 집계 필드에 대해 가장 빈도수가 높은 term 순서대로 버킷을 생성한다. 최대 몇개의 버킷을 생성할것인지 size로 지정한다.### terms 집계GET /kibana_sample_data_logs/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"my-terms-aggs\": { \"terms\" : { \"field\":\"host.keyword\", \"size\": \"10\" } } }}{ \"took\": 27, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 10000,\t \"relation\": \"gte\"\t},\t\"max_score\": null,\t\"hits\": [] }, \"aggregations\": {\t\"my-terms-aggs\": {\t \"doc_count_error_upper_bound\": 0,\t \"sum_other_doc_count\": 0,\t \"buckets\": [\t\t{\t\t \"key\": \"artifacts.elastic.co\",\t\t \"doc_count\": 6488\t\t},\t\t{\t\t \"key\": \"www.elastic.co\",\t\t \"doc_count\": 4779\t\t},\t\t{\t\t \"key\": \"cdn.elastic-elastic-elastic.org\",\t\t \"doc_count\": 2255\t\t},\t\t{\t\t \"key\": \"elastic-elastic-elastic.org\",\t\t \"doc_count\": 552\t\t}\t ]\t} }} 각 샤드에서 size 개수만큼 term을 뽑아 빈도수를 센다. 각 샤드에서 수행된 계산을 한곳으로 모아 합산한 후 size개수만큼 버킷을 뽑는다. size 개수와 각 문서의 분포에 따라 결과가 정확하지 않을 수 있음.( term의 개수가 2개인데 size를 10개로 한다고한들 10개를 만들수 없다.) doc_count_error_upper_bound doc_count의 오차 상한선을 나타낸다. 이 값이 크다면 size를 높이는 것을 고려할 수 있다. 정확도가 올라가는 만큼 성능은 하락할 수 있다.sum_other_doc_count 최종적으로 버킷에 포함되지 않은 문서 수 상위 term에 들지 못한 문서 개수의 총합이다.composite 집계 모든 term에 대해서 페이지네이션으로 전부 순회하면서 집계하려는 경우 사용(term에서는 정확하지 않을수 있기에) sources로 지정된 하위 집계의 버킷 전부를 페이지네이션을 이용해서 효율적으로 순회하는 집계다. 또한 soruces에 하위 집계를 여러 개 지정한 뒤 조합된 버킷을 생성할 수 있다.### composite 집계GET /kibana_sample_data_logs/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"composite-aggs\": { \"composite\" : { \"size\": 100, \"sources\": [ { \"terms-aggs\": { \"terms\": { \"field\": \"host.keyword\" } } }, { \"date-histogram-aggs\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"day\" } } } ] } } }}{ \"took\": 79, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 10000,\t \"relation\": \"gte\"\t},\t\"max_score\": null,\t\"hits\": [] }, \"aggregations\": {\t\"composite-aggs\": {\t \"after_key\": {\t\t\"terms-aggs\": \"cdn.elastic-elastic-elastic.org\",\t\t\"date-histogram-aggs\": 1706054400000\t },\t \"buckets\": [\t\t{\t\t \"key\": {\t\t\t\"terms-aggs\": \"artifacts.elastic.co\",\t\t\t\"date-histogram-aggs\": 1702771200000\t\t },\t\t \"doc_count\": 124\t\t},\t\t{\t\t \"key\": {\t\t\t\"terms-aggs\": \"artifacts.elastic.co\",\t\t\t\"date-histogram-aggs\": 1702857600000\t\t },\t\t \"doc_count\": 88\t\t}, ....\t ]\t} }} composite 아래의 size는 페이지네이션 한 번에 몇 개의 버킷을 반환할 것인가를 지정한다. sources에는 버킷을 조합하여 순회할 하위 집계를 지정한다. 하위 집계는 terms, histogram, date_histogratm 집계 등 일부 집계만 지정할 수 있다. ES Composite Aggregation Doc 응답의 버킷 key가 여러 집계의 결과로 조합된 것을 확인할 수 있다. terms-aggs, date-histogram-aggs의 조합을 기준으로 집계됨. composite.after 집계 결과의 다음 페이지를 보고싶다면 composite 하위에 after을 추가하여 조회할수 있다.### composite 집계 afterGET /kibana_sample_data_logs/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"composite-aggs\": { \"composite\" : { \"size\": 100, \"sources\": [ { \"terms-aggs\": { \"terms\": { \"field\": \"host.keyword\" } } }, { \"date-histogram-aggs\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"day\" } } } ], \"after\": { \"terms-aggs\": \"cdn.elastic-elastic-elastic.org\", \"date-histogram-aggs\": 1675209600000 } } } }}4.4.4 파이프라인 집계 파이프라인 집계는 문서나 필드의 내용이 아니라 다른 집계 결과를 집계 대상으로 한다. 주로 buckets_path라는 인자를 통해 다른 집계의 결과를 가져오고, 이 buckets_path는 상대 경로로 지정한다.buckets_path 지정 구문 &gt; : 하위 집계로 이동하는 구분자 . : 하위 메트릭으로 이동하는 구분자 집계 이름 메트릭 이름cumulative_sum 집계 다른 집계 값을 누적하여 합산한다.### cumulative_sum 집계GET /kibana_sample_data_ecommerce/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"daily-timestamp-bucket\": { \"date_histogram\" : { \"field\": \"order_date\", \"calendar_interval\": \"day\" }, \"aggs\": { \"daily-total-quantity-average\": { \"avg\": { \"field\": \"total_quantity\" } }, \"pipeline-sum\": { \"cumulative_sum\": { \"buckets_path\": \"daily-total-quantity-average\" } } } } }}{ \"took\": 30, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 4675,\t \"relation\": \"eq\"\t},\t\"max_score\": null,\t\"hits\": [] }, \"aggregations\": {\t\"daily-timestamp-bucket\": {\t \"buckets\": [\t\t{\t\t \"key_as_string\": \"2023-12-07T00:00:00.000Z\",\t\t \"key\": 1701907200000,\t\t \"doc_count\": 146,\t\t \"daily-total-quantity-average\": {\t\t\t\"value\": 2.1780821917808217\t\t },\t\t \"pipeline-sum\": {\t\t\t\"value\": 2.1780821917808217\t\t }\t\t}, ....\t\t{\t\t \"key_as_string\": \"2024-01-05T00:00:00.000Z\",\t\t \"key\": 1704412800000,\t\t \"doc_count\": 152,\t\t \"daily-total-quantity-average\": {\t\t\t\"value\": 2.289473684210526\t\t },\t\t \"pipeline-sum\": {\t\t\t\"value\": 64.71700410498919\t\t }\t\t},\t\t{\t\t \"key_as_string\": \"2024-01-06T00:00:00.000Z\",\t\t \"key\": 1704499200000,\t\t \"doc_count\": 142,\t\t \"daily-total-quantity-average\": {\t\t\t\"value\": 2.183098591549296\t\t },\t\t \"pipeline-sum\": {\t\t\t\"value\": 66.90010269653848\t\t }\t\t}\t ]\t} }} 일 단위로 total_quantity의 평균을 구하기 위해 date_histogram으로 버킷을 나눈 뒤 하위 집계로 avg 집계를 지정했다. 그리고 date_histogram의 하위 집계로 cumulative_sum 집계를 추가 지정했다.max_bucket 집계 다른 집계의 결과를 받아서 그 결과가 가장 큰 버킷의 key와 결과값을 구하는 집계이다.### max_bucket 집계GET /kibana_sample_data_ecommerce/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"daily-timestamp-bucket\": { \"date_histogram\" : { \"field\": \"order_date\", \"calendar_interval\": \"day\" }, \"aggs\": { \"daily-total-quantity-average\": { \"avg\": { \"field\": \"total_quantity\" } } } }, \"max-total-quantity\": { \"max_bucket\": { \"buckets_path\": \"daily-timestamp-bucket&gt;daily-total-quantity-average\" } } }}{ \"took\": 6, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 4675,\t \"relation\": \"eq\"\t},\t\"max_score\": null,\t\"hits\": [] }, \"aggregations\": {\t\"daily-timestamp-bucket\": {\t \"buckets\": [\t\t{\t\t \"key_as_string\": \"2023-12-07T00:00:00.000Z\",\t\t \"key\": 1701907200000,\t\t \"doc_count\": 146,\t\t \"daily-total-quantity-average\": {\t\t\t\"value\": 2.1780821917808217\t\t }\t\t}, ....\t\t{\t\t \"key_as_string\": \"2024-01-05T00:00:00.000Z\",\t\t \"key\": 1704412800000,\t\t \"doc_count\": 152,\t\t \"daily-total-quantity-average\": {\t\t\t\"value\": 2.289473684210526\t\t }\t\t},\t\t{\t\t \"key_as_string\": \"2024-01-06T00:00:00.000Z\",\t\t \"key\": 1704499200000,\t\t \"doc_count\": 142,\t\t \"daily-total-quantity-average\": {\t\t\t\"value\": 2.183098591549296\t\t }\t\t}\t ]\t},\t\"max-total-quantity\": {\t \"value\": 2.289473684210526,\t \"keys\": [\t\t\"2024-01-05T00:00:00.000Z\"\t ]\t} }} buckets_path가 절대경로가 아니라 상대경로 이므로 daily-timestamp-bucket에서 하위 집계인 daily-total-quantity-average로 이동하는 구분자를 사용했다. \"buckets_path\": \"daily-timestamp-bucket&gt;daily-total-quantity-average\" Reference HyperLogLog Naver D2 레퍼런스 ES Composite Aggregation Doc" }, { "title": "[엘라스틱서치 바이블] 4. 데이터 다루기 -1", "url": "/posts/es-bible-4-1/", "categories": "DevLog, Elasticsearch", "tags": "Elasticsearch", "date": "2023-12-16 19:49:00 +0900", "snippet": "4. 데이터 다루기 -14.1 단건 문서 API 문서(document) API는 인덱스에 문서를 색인, 조회, 업데이트, 삭제하는 API이다. 상세한 내용을 알아보도록 하자.4.1.1 색인 API 색인 API는 문서 단건을 색인한다.PUT [인덱스 이름]/_doc/[_id값]POST [인덱스 이름]/_docPUT [인덱스 이름]/_create/[_...", "content": "4. 데이터 다루기 -14.1 단건 문서 API 문서(document) API는 인덱스에 문서를 색인, 조회, 업데이트, 삭제하는 API이다. 상세한 내용을 알아보도록 하자.4.1.1 색인 API 색인 API는 문서 단건을 색인한다.PUT [인덱스 이름]/_doc/[_id값]POST [인덱스 이름]/_docPUT [인덱스 이름]/_create/[_id값]POST [인덱스 이름]/_create/[_id값]각 API의 차이 기본 API 는 PUT [인덱스 이름]/_doc/[_id값] 이다. _id값에 일치하는 문서가 이미 있다면 새 문서로 덮어 씌움 POST 메서드는 _id 값을 지정하지 않고 색인을 요청할 경우에 사용한다.(es에서 random한 _id 생성) _create 메서드의 경우에는 항상 새 문서를 생성하는 경우에만 허용하고, 기존 문서를 덮어쓰면서 색인하는것을 금지한다.### 4.1.1 색인 APIPUT /my_index2/_create/2Host: localhost:9200Content-Type: application/json{ \"hello\": \"world2\"}{ \"error\": {\t\"root_cause\": [\t {\t\t\"type\": \"version_conflict_engine_exception\",\t\t\"reason\": \"[2]: version conflict, document already exists (current version [1])\",\t\t\"index_uuid\": \"zV7gpn5XQ4aHoQ9Bw36ygA\",\t\t\"shard\": \"0\",\t\t\"index\": \"my_index2\"\t }\t],\t\"type\": \"version_conflict_engine_exception\",\t\"reason\": \"[2]: version conflict, document already exists (current version [1])\",\t\"index_uuid\": \"zV7gpn5XQ4aHoQ9Bw36ygA\",\t\"shard\": \"0\",\t\"index\": \"my_index2\" }, \"status\": 409} 한번 생성된 _id로 다시 한번 create 하면 error 응답을 주는것을 확인할 수 있다.라우팅 라우팅을 지정하지 않는 경우 _id 값의 해시값을 기반으로 샤드가 배정된다.### routingPUT /routing_test/_doc/2?routing=myid2Host: localhost:9200Content-Type: application/json{ \"login_id\": \"myid\", \"comment\": \"hello elasticsearch\", \"created_at\" : \"2022-12-01T00:08:12.378Z\"}refresh 색인시 refresh 매개변수를 지정하면 문서를 색인한 직후에 해당 샤드를 refresh해서 즉시 검색 가능하게 만들 것인지 여부를 지정할 수 있다. refresh 공식 문서 true : 색인 직후 문서가 색인된 샤드를 refresh 하고 응답 wait_for : 색인 이후 문서가 refresh될 때까지 기다린 후 응답을 반환. true로 지정했을때와 다르게 refresh를 직접 유발하지는 않음. index.refresh_interval 에 지정된 시간까지 대기하다가 refresh가 완료되면 응답함 false : 기본값으로 refresh와 관련된 동작을 수행하지 않음. refresh 관련 주의사항 실제 서비스를 만들다보면 색인 직후 검색 API를 사용해야 하고, 그 검새 결과에 최신 변경 내용이 포함되어야하는 경우가 있고 그경우 refresh 옵션을 고려할 수 있다. 다만, true 또는 wait_for 옵션을 사용할 경우 반드시 성능에 대한 고려가 필요하다. 호출량이 많은 서비스가 매번 refresh를 지정해 호출하면 전체 클러스터 성능이 크게 저하될 수 있다. 이러한 사용방식은 너무 많은 작은 세그먼트를 생성하여 장기적으로 검색 성능이 떨어지고, 추후 세그먼트 병합 비용도 발생함. wait_for을 하는 경우에도 너무 많은 대기 요청이 있는 경우 강제로 refresh가 수행될 수 있음. index.max_refresh_listeners 설정 기본값 : 1000(이 이상 wait_for하는 경우 강제 refresh) 추가 인덱스 설계 코멘트 기본적으로 문서 색인 요청의 결과가 검색 역색인에 즉시 동기 반영되어야 하는 케이스는 많지 않도록 서비스 설계해야 함. 대량 색인의 경우 단건 색인 API 대신 bulk API를 사용해야 한다.4.1.2 조회 API 문서 단건을 조회하는 API, es refresh 되지 않아도 변경 내용을 확인할 수 있다. 고유 식별자를 지정하여 조회하는것이므로 역색인이 필요없고, translog에서도 데이터를 읽어올수 있음.GET [인덱스 이름]/_doc/[_id값]GET [인덱스 이름]/_source/[_id값]_doc과 _soruce의 차이 _doc을 이용하면 인덱스, _id를 포함한 기본적인 메타 데이터도 함꼐 조회할 수 있음. _soruce를 이용하면 메타데이터 없이 문서 정보만 조회할 수 있음.### _docGET /my_index2/_doc/2Host: localhost:9200Content-Type: application/json{ \"_index\": \"my_index2\", \"_id\": \"2\", \"_version\": 1, \"_seq_no\": 1, \"_primary_term\": 2, \"found\": true, \"_source\": {\t\"hello\": \"world2\" }}### _sourceGET /my_index2/_source/2Host: localhost:9200Content-Type: application/json{ \"hello\": \"world2\"}필드 필터링 _source_includes와 _source_excludes 옵션을 사용하면 결과에 원하는 필드만 필터링해 포함시킬 수 있다.### createPUT /my_index2/_create/3Host: localhost:9200Content-Type: application/json{ \"title\": \"hello world\", \"view\": 1234, \"public\": true, \"point\": 4.5, \"created_at\" : \"2019-01-17T14:05:01.234Z\"}### 필드 필터링(_source_includes)GET /my_index2/_doc/3?_source_includes=p*,views,Host: localhost:9200Content-Type: application/json{ \"_index\": \"my_index2\", \"_id\": \"3\", \"_version\": 1, \"_seq_no\": 2, \"_primary_term\": 2, \"found\": true, \"_source\": {\t\"public\": true,\t\"point\": 4.5 }}### 필드 필터링(_source_includes &amp; _source_excludes)GET /my_index2/_doc/3?_source_includes=p*,views&amp;_source_excludes=publicHost: localhost:9200Content-Type: application/json{ \"_index\": \"my_index2\", \"_id\": \"3\", \"_version\": 2, \"_seq_no\": 3, \"_primary_term\": 2, \"found\": true, \"_source\": {\t\"views\": 1234,\t\"point\": 4.5 }} includes, excludes를 꼭 같이 사용할 필요는 없고, excludes에 명시적으로 지정한 경우는 무조건 처리됨. includes만 사용하고자 하는 경우 _source 매개변수로 사용할수 있다. _source는 true/false를 지정해 문서의 소스를 가져올지 말지에 대해서도 지정할수 있는데 필드명이 false 인 경우 충돌이 있을수 있음. includes, excludes에 명시한 내용이 적절히 조회되는 것을 알 수 있다.라우팅 조회 API도 색인과 마찬가지로 라우팅을 반드시 제대로 지정해야 한다.GET /routing_test/_doc/2?routing=myid2Host: localhost:9200Content-Type: application/json추가 검색 옵션들 https://www.elastic.co/guide/en/elasticsearch/reference/8.11/search-your-data.html4.1.3 업데이트 API 지정한 문서 하나를 업데이트한다. 기본적으로 부분 업데이트(partial updarte)로 동작한다.(문서 전체를 교체하려면 색인 API를 사용하라)POST [인덱스 이름]/_update/[_id값] API 자체는 부분 업데이트이지만, 루씬 세그먼트가 불변이라서 실제 ES의 업데이트 작업은 기존 문서의 내용을 조회한 뒤 부분 업데이트될 내용을 합쳐 새 문서를 만들어 색인하는 형태로 진행된다. 업데이트 API에는 doc을 이용하는 방법과 script를 이용하는 방법이 있다.doc에 내용을 직접 기술하여 업데이트POST [인덱스 이름]/_update/[_id값]{ \"doc\": { [업데이트할 내용] }}### createPUT /update_test/_doc/1Host: localhost:9200Content-Type: application/json{ \"title\": \"hello world\", \"views\": 35, \"created_at\" : \"2019-01-17T14:05:01.234Z\"}### updatePOST /update_test/_update/1Host: localhost:9200Content-Type: application/json{ \"doc\": { \"views\": 36, \"updated_at\" : \"2019-01-23T17:00:01.567Z\" }}### getGET /update_test/_doc/1Host: localhost:9200Content-Type: application/json{ \"_index\": \"update_test\", \"_id\": \"1\", \"_version\": 2, \"result\": \"updated\", \"_shards\": {\t\"total\": 2,\t\"successful\": 2,\t\"failed\": 0 }, \"_seq_no\": 1, \"_primary_term\": 1} update 응답에 result : updated로 나오는것을 알 수 있다. 이후에 다시 조회하면 부분 업데이트가 적용된 결과를 확인할 수 있다.{ \"_index\": \"update_test\", \"_id\": \"1\", \"_version\": 2, \"_seq_no\": 1, \"_primary_term\": 1, \"found\": true, \"_source\": {\t\"title\": \"hello world\",\t\"views\": 36,\t\"created_at\": \"2019-01-17T14:05:01.234Z\",\t\"updated_at\": \"2019-01-23T17:00:01.567Z\" }}detect_noop 업데이트 API를 호출하면 ES는 그 작업을 수행하기 전에 업데이트 내용이 기존문서 내용을 실질적으로 변경하는지 여부를 확인한다. 실제 데이터 변경사항이 없으면 요청(noop 요청, no operation)이라면 쓰기 작업을 수행하지 않는다. noop 검사를 통해 불필요한 디스크 I/O를 줄일 수 있고, 이 기본값은 true이다. detect_noop을 명시적으로 false로 지정하여 비활성화할수 있는데, 이 경우에는 업데이트가 일어난다.{ \"_index\": \"update_test\", \"_id\": \"1\", \"_version\": 2, \"result\": \"noop\", \"_shards\": {\t\"total\": 0,\t\"successful\": 0,\t\"failed\": 0 }, \"_seq_no\": 1, \"_primary_term\": 1}detect_noop 옵션을 비활성화하는 상황 일반적인 상황에서는 활성화하는 것이 좋다. 데이터 특성상 noop 업데이트가 발생할 가능성이 아예 0%인 경우에는 detect_noop을 비활성화하는 것이 성능을 약간 향상시켜줄 수 있다.detect_noop + custom plugin 작성시 유의사항 custom plugin 등 ES를 커스터마이징한다면, 이 옵션을 주의 깊게 살펴봐야 한다. 플러그인에서 IndexingOperationListener 같은 리스너를 달 경우 색인 전후에 원하는 작업을 끼워넣을 수 있다. 업데이트 요청이 noop로 처리된다면 ES입장에서는 실제 색인이 수행되지 않으므로 IndexingOperationListener의 preIndex, postIndex 핸들러쪽이 수행되지 않게 된다.doc_as_upsert 업데이트 API는 기본적으로 기존문서의 내용을 먼저 읽어들인 뒤 업데이트를 수행한다.(기존 문서가 없으면 요청 실패) doc_as_upsert 옵션을 통해 기존문서가 없는 경우 새로 문서를 추가하는 방법을 제공한다.### doc_as_upsertPOST /update_test/_update/2Host: localhost:9200Content-Type: application/json{ \"doc\": { \"views\": 36, \"updated_at\" : \"2019-01-23T17:00:01.567Z\" }, \"doc_as_upsert\": true}script를 이용한 업데이트 별도의 스크립트 언어를 넣어서 문서를 업데이트 하는 방법이 있따.### script를 이용하여 업데이트POST /update_test/_update/1Host: localhost:9200Content-Type: application/json{ \"script\": { \"source\": \"ctx._source.views += params.amount\", \"lang\": \"painless\", \"params\": { \"amount\": 1 } }, \"scripted_upsert\": false} script : 스크립트를 이용해서 업데이트를 수행할 때는 그 내용을 위와 같이 script 필드 안에 기술한다. source : 소스는 스키릅트 본문 기술 lang : 스크립트 언어 종류 지정(기본값 : painless) params : 스크립트 본문에서 사용할 매개변수 값들을 넣어둘수 있다. scripted_upsert : 스크립트를 사용한 업데이트가 upsert로 동작하도록 할지를 지정(기본값 false)제공하는 스크립트 langauge라우팅과 refresh 업데이트 API에도 색인 API와 마찬가지로 routing과 refresh 옵션을 지정할 수 있다. 색인시 routing을 지정했다면 업데이트시에도 동일한 값으로 라우팅을 지정해 업데이트해야 의도한 대로 동작한다. refresh 션 또한 조회 API와 동일하게 동작한다.4.1.4 삭제 API 지정한 문서 하나를 삭제한다. 한번 삭제한 문서는 되돌릴 수 없기 때문에 삭제 작업을 항상 신중해야 한다. 삭제 API에도 색인이나 업데이트와 마찬가지로 routing과 refresh 옵션을 지정할 수 있다.DELETE [인덱스 이름]/_doc/[_id값]삭제시 주의사항 DELETE [인덱스 이름]을 하는 경우 인덱스 전체가 삭제될 수 있다.4.2 복수 문서 API 서비스 환경에서 단건 문서 API보다는 복수 문서 API를 활용해야 한다.4.2.1 bulk API bulk API는 여러 색인, 업데이트, 삭제 작업을 한 번의 요청에 담아서 보내는 API이다. bulk API는 ES의 다른 API와는 다르게 요청 본문을 JSON이 아니라 NSJSON 형태로 만들어서 보낸다. 요청 Content-Type 헤더에 application/x-ndjson 을 사용해야 한다. 여러 줄의 JSON을 줄바꿈 문자(\\n)로 구분하여 요청 ### _bulk### intellij version에 따라 body parse가 안되는 이슈가 있는듯POST /_bulkHost: localhost:9200Content-Type: application/x-ndjson{\"index\":{\"_index\":\"bulk_test\",\"_id\":\"1\"}}\\n{\"field1\":\"value1\"}\\n{\"delete\":{\"_index\":\"bulk_test\",\"_id\":\"2\"}}\\n{\"create\":{\"_index\":\"bulk_test\",\"_id\":\"3\"}}\\n{\"field1\":\"value3\"}\\n{\"update\":{\"_index\":\"bulk_test\",\"_id\":\"1\"}}\\n{\"doc\":{\"field2\":\"value2\"}}\\n{\"index\":{\"_index\":\"bulk_test\",\"_id\":\"4\",\"routing\":\"a\"}}\\n{\"field1\":\"value4\"}\\n\\n 위 요청은 5개의 세부 요청으로 이루어져 있고, 아래의 요청이 포함되어 있다. index, create : 색인 요청 create는 문서 생성하는것만 허용하고, 기존 문서를 덮어쓰지 않는다. index는 기존에 동일한 _id로 문서가 존재하지는 여부와 상관 없이 항상 색인을 수행한다. update : 업데이트 요청 다음 라인에서 doc 또는 script 사용 가능. delete : 삭제 요청 { \"errors\": false, \"took\": 2180, \"items\": [ { \"index\": { \"_index\": \"bulk_test\", \"_id\": \"1\", \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 0, \"_primary_term\": 1, \"status\": 201 } }, { \"delete\": { \"_index\": \"bulk_test\", \"_id\": \"2\", \"_version\": 1, \"result\": \"not_found\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 1, \"_primary_term\": 1, \"status\": 404 } }, { \"create\": { \"_index\": \"bulk_test\", \"_id\": \"3\", \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 2, \"_primary_term\": 1, \"status\": 201 } }, { \"update\": { \"_index\": \"bulk_test\", \"_id\": \"1\", \"_version\": 2, \"result\": \"updated\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 3, \"_primary_term\": 1, \"status\": 200 } }, { \"index\": { \"_index\": \"bulk_test\", \"_id\": \"4\", \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 4, \"_primary_term\": 1, \"status\": 201 } } ]} bulk API의 응답은 각 세부 요청을 수행하고 난 결과를 모아 하나의 응답으로 돌아온다. 전체 응답 내 각 세부 응답은 요청의 순서와 동일하다. 각 요청별 세부 응답의 상태코드를 통해 처리결과를 각각 전달받을 수 있다.특정 index의 bulk 처리POST [인덱스이름]/_bulk 위처럼 _bulk 앞에 인덱스 이름을 넣으면 기본 대상이 해당 인덱스로 지정된다.bulk API의 작업 순서 bulk API에 기술된 작업은 반드시 그 순서대로 수행된다는 보장이 없다. 조정 역할을 하는 노드가 요청을 수신하면 각 요청 내용을 보고 적절한 주 샤드로 요청을 넘겨준다. 이 때 주 샤드로 넘어간 각 요청은 각자 독자적으로 수행하므로 요청간 순서를 100% 보장하지 않는다. 완전히 동일한 인덱스, _id, 라우팅 조합을 가진 요청은 반드시 동일한 주 샤드로 요청이 가므로 이때는 순서를 보장할수 있다. bulk API의 성능 네트워크를 통해 요청을 여러 번 반복해서 호출해야 한다면 이를 묶어 한꺼번에 전송하는것이 일반적으로 성능상 이득이다. ES도 마찬가지로 단건 문서 API를 여러번 하는것보다 bulk API를 사용하는 편이 성능이 월등히 빠르다. bulk API 요청 1번에 몇개의 요청을 모아서 보내는 것이 성능상 적절한지는 정해져있지 않고, 각 요청의 크기나 데이터 특성 등을 보고 적절히 조절해야 한다. HTTP 요청을 chunked로 보내는 것은 성능을 떨어지게 만들기 떄문에 피해야 한다. ( HTTP Chunk 참고 )4.2.2 multi get API multi get API는 _id를 여럿 지정하여 해당 문서를 한 번에 조회하는 API이다.GET _mgetGET [인덱스 이름]/_mget _bulk API와 마찬가지로 _mget 앞에 인덱스 이름을 명시했다면 이 인덱스를 기본 인덱스로 지정한다.### _mgetPOST /_mgetHost: localhost:9200Content-Type: application/json{ \"docs\": [ { \"_index\": \"bulk_test\", \"_id\": 1 }, { \"_index\": \"bulk_test\", \"_id\": 4, \"routing\": \"a\" }, { \"_index\": \"my_index2\", \"_id\": 1, \"_source\": { \"include\": [\"p*\"], \"exclude\": [\"point\"]\t } } ]} 요청했던 순서대로 문서의 내용을 모아 단일 응답으로 돌아온다.{ \"docs\": [\t{\t \"_index\": \"bulk_test\",\t \"_id\": \"1\",\t \"_version\": 2,\t \"_seq_no\": 3,\t \"_primary_term\": 1,\t \"found\": true,\t \"_source\": {\t\t\"field1\": \"value1\",\t\t\"field2\": \"value2\"\t }\t},\t{\t \"_index\": \"bulk_test\",\t \"_id\": \"4\",\t \"_version\": 1,\t \"_seq_no\": 4,\t \"_primary_term\": 1,\t \"_routing\": \"a\",\t \"found\": true,\t \"_source\": {\t\t\"field1\": \"value4\"\t }\t},\t{\t \"_index\": \"my_index2\",\t \"_id\": \"1\",\t \"_version\": 1,\t \"_seq_no\": 0,\t \"_primary_term\": 1,\t \"found\": true,\t \"_source\": {}\t} ]}4.2.3 update by query update by query 는 앞서 명한 bulk API, multi get API와는 성격이 좀 다른 복수 문서 API이다. 검색 쿼리를 통해 주어진 조건에 만족하는 문서를 찾은 뒤 그 문서를 대상으로 업데이트나 삭제 작업을 실시하는 API이다. 보통은 서비스에서 일상적으로 사용하기보다는 관리적인 목적으로 호출하는 것이 일반적이다.POST [인덱스 이름]/_update_by_query{ \"script\": { \"source\": \" // ... \", }, \"query\": { // ... }}update by query 업데이트 지원 관련 doc을 이용한 업데이트를 지원하지 않고, script를 통한 업데이트만 지원한다. painless 스크립트를 사용하는 경우 문맥 정보 중에서 ctx._now를 사용할 수 없다.(현재 타임스탬프값을 밀리세컨드로 반환한 값)update by query 동작 방식 ES는 query 절의 검색 조건에 맞는 문서를 찾아 일종의 스냅샷을 찍는다. 이후 각 문서마다 지정된 스크립트에 맞게 업데이트 실시 순차적으로 실행하는 과정중에 스냅샷을 찍어뒀던 문서에서 변화가 생긴 문서가 발견되면 이를 업데이트 하지 않는다. 버전 충돌 문제시 전체 작업을 그만두거나 다음 작업으로 넘어가는 방식을 사용자가 선택할 수 있다. ( conflicts 매개변수 지정, 기본값 abort) 도중에 충돌로 인해 중간에 작업이 중단된 경우 그 이전까지 업데이트된 내용이 롤백되거나 하지는 않는다(이를 염두해두고 작업을 수행해야 한다.) ### _update_by_queryPOST /bulk_test/_update_by_queryHost: localhost:9200Content-Type: application/json{ \"script\": { \"source\": \"ctx._source.field1 = ctx._source.field1 + '_' + ctx._id\", \"lang\": \"painless\" }, \"query\": { \"exists\": { \"field\": \"field1\" } }} bulk_test 인덱스 내에 field1이라는 이름의 필드가 존재하는 문서를 대상으로 업데이트를 수행했다.{ \"took\": 242, \"timed_out\": false, \"total\": 3, \"updated\": 3, \"deleted\": 0, \"batches\": 1, \"version_conflicts\": 0, \"noops\": 0, \"retries\": {\t\"bulk\": 0,\t\"search\": 0 }, \"throttled_millis\": 0, \"requests_per_second\": -1.0, \"throttled_until_millis\": 0, \"failures\": []} 응답을 보면 작업된 문서 개수, 버전 충돌, 재시도 횟수 등을 확인할 수 있다.스로틀링 위 예시 응답 내용 중 throttled_millis, requests_per_second, throttled_until_millis 들을 볼 수 있는데 이는 스로틀링과 관련된 필드이다. update by query API는 문제가 생긴 데이터를 일괄적으로 처리하거나 변경된 비즈니스 요건에 맞게 데이터를 일괄 수정하는 작업 등에 많이사용되는데, 대량 작업을 수행하면 운영 중인 기존 서비스에도 영향을 줄수 있으므로 이런 상황을 피하기 위해 스로틀링 기능을 제공한다. 스로틀링 적용을 통해 작업의 속도를 조정하고 클러스터 부하와 서비스 영향을 최소화할 수 있다.### throttlingPOST /bulk_test/_update_by_query?scroll_size=1000&amp;scroll=1m&amp;requests_per_seconds=500Host: localhost:9200Content-Type: application/json{ // ...}스로틀링 설정시 고려 사항 scroll_size는 문서 검색/업데이트를 수행하는 단위 개수이다. 1000개로 지정한다면 1000개의 문서를 가져온뒤 1000개 문서에 대한 업데이트를 수행하고, 그 이후에 다시 1000개를 가져오는 방식으로 동작한다. 서비스와 작업 환경에 맞는 적절한 scroll_size를 지정하는것이 필요하다. scroll 은 검색 조건에 만족하는 모든 문서를 대상으로 검색이 처음 수행됐을 당시 상태를 검색 문맥(search context)에 저정하는데, 이 search context를 얼마나 보존할지에 대한 설정이다. 1m으로 설정할 경우 1분 동안 검색 문맥이 유지된다. 모든 작업이 종료될 때까지 필요한 시간을 지정하는것은 아니고, 한 배치 작업에 필요한 시간을 지정하면 된다.(scroll_size만큼의 작업 처리) scroll_size만큼의 작업을 수행할 수 없을만큼 짧게 지정해서는 안된다. 너무 큰 값을 지정하면 힙이나 디스크 공간, file descriptor 등 많은 자원 소비가 필요하니 적절한 값을 지정해야 한다. requests_per_secoond 은 이름 그대로 평균적으로 초당 몇 개까지의 작업을 수행할것인지에 대한 지정이다. scroll_size 단위로 업데이트 작업을 수행한 뒤 requests_per_secoond 값에 맞도록 일정 시간을 대기하는 방식으로 진행되낟. scroll_size 1000, requests_per_second를 500으로 설정했다면 ES는 2초마다 스크롤 한번 분량만큼을 업데이트한다. requests_per_secoond를 -1로 설정하면 스로틀링을 적용하지 않는다. 비동기적 요청과 tasks API update 처리에 따라서 작업시간이 수십시간이 될수도 있는데, 이를 HTTP blocking 방식으로 응답을 무한정 대기할수는 없다. 이런 문제를 위해 update by query 요청시 wait_for_completion 매개변수를 false로 지정하여 비동기적 처리를 할 수 있다. 이떄 응답으로 task id를 반환한다. task API는 비동기 처리시 반환받은 task_id를 가지고 현재 진행중인 task의 상태를 확인할 수 있다.POST [인덱스 이름]/_update_by_query?wait_for_completion=false{ // ...}{ \"task\" : \"f8Aa9sj-RbaOebr3acDcxQ:120896\"}tasks APIGET .tasks/_doc/[task id]GET _tasks/[task id]2개 API의 차이점 .tasks API는 ES 내부 인덱스인 .tasks의 문서 정보를 단건으로 조회하는 방식이다. 비동기로 등록되지 않은 작업도 확인할 수 있다. _tasks API는 tasks 관리 APAI인데 ES 8.11 기준으로도 beta 기능이라 변경될 가능성이 있다.task API 호출### task apiGET /_tasks/f8Aa9sj-RbaOebr3acDcxQ:120896Host: localhost:9200Content-Type: application/json{ \"completed\": true, \"task\": {\t\"node\": \"f8Aa9sj-RbaOebr3acDcxQ\",\t\"id\": 120896,\t\"type\": \"transport\",\t\"action\": \"indices:data/write/update/byquery\",\t\"status\": {\t \"total\": 3,\t \"updated\": 3,\t \"created\": 0,\t \"deleted\": 0,\t \"batches\": 1,\t \"version_conflicts\": 0,\t \"noops\": 0,\t \"retries\": {\t\t\"bulk\": 0,\t\t\"search\": 0\t },\t \"throttled_millis\": 0,\t \"requests_per_second\": -1.0,\t \"throttled_until_millis\": 0\t},\t\"description\": \"update-by-query [bulk_test] updated with Script{type=inline, lang='painless', idOrCode='ctx._source.field1 = ctx._source.field1 + '_' + ctx._id', options={}, params={}}\",\t\"start_time_in_millis\": 1702726259365,\t\"running_time_in_nanos\": 110659458,\t\"cancellable\": true,\t\"cancelled\": false,\t\"headers\": {} }, \"response\": {\t\"took\": 104,\t\"timed_out\": false,\t\"total\": 3,\t\"updated\": 3,\t\"created\": 0,\t\"deleted\": 0,\t\"batches\": 1,\t\"version_conflicts\": 0,\t\"noops\": 0,\t\"retries\": {\t \"bulk\": 0,\t \"search\": 0\t},\t\"throttled\": \"0s\",\t\"throttled_millis\": 0,\t\"requests_per_second\": -1.0,\t\"throttled_until\": \"0s\",\t\"throttled_until_millis\": 0,\t\"failures\": [] }}task 작업 취소 작업 진행 중 문제가 발생했다면 다음과 같이 작업을 취소할 수 있다. 어떤 노드에서 작업이 취소되었는지, 어떤 task 작업이 취소됐는지 등을 확인할 수 있다. wait_for_completion=true로 지정해 호출한 작업이더라도 task id를 알아내서 작업취소할 수 있다.POST _tasks/[task id]/_cancel스로틀링 동적 변경 대량 작업을 수행하다 외부적인 이슈로 클러스터 전체에 문제가 발생하는 경우 스로틀링 동적 변경을 통해 대응할 수 있다. ES update_by_query 특성상 이미 처리 완료된 데이터에 대한 트랜잭션 롤백이 없으므로 중간에 task를 중단시키는것이 능사가 아닐수 있다. 72시간동안 수행되어야할 작업 중 일부만 남겨두고 있었다면 중간에 task를 취소하는것도 리스크일수 있음. POST _update_by_query/[task id]/_rethrottle?request_per_second=[변경할 값] 위와 같이 _rethrottle을 이용하면 작업의 스로틀링을 동적으로 변경할 수 있어 문제 상황을 유연하게 대응할 수 있다.task 결과 삭제 wait_for_completion=false을 통해 .tasks 인덱스에 등록된 작업이 성공하거나 취소됐는지의 여부와 상관없이 등록된 작업 결과는 ES에 계속 남는다. 작업의 상황을 충분히 확인했다면 인덱스의 문서를 삭제하면 좋다DELETE .tasks/_doc/[task id]슬라이싱 관리적 목적의 대량 업데이트를 수행하는 경우, 스로틀링을 적용해 부하를 줄이는 선택도 있겠지만, 반대로 업데이트 성능을 최대로 끌어내 빠른 시간 안에 끝내고자 하는 선택도 있다.(ex. 서비스 요청 차단 후 정기점검) slices 매개변수를 지정하면 검색과 업데이트를 지정한 개수로 쪼개 병렬적으로 수행한다. 기본값은 1이고, auto로 지정하는 경우 ES가 적절한 개수를 지정해서 작업을 병렬 수행한다. POST [인덱스 이름]/_update_by_query?slice=auto{ // ...}슬라이싱 관련 주의사항 slices=auto인 경우 보통은 지정한 인덱스의 주 샤드 수가 슬라이스의 수로 지정된다. 주 샤드 수보다 높은 slices 수를 지정하는 경우 오히려 성능이 급감할수 있음을 유의해야 한다. 샤드 내 slices 분배가 필요해지는데 슬라이스 수가 주 샤드 이내인 경우에는 필요 없었던 과정임. 슬라이싱은 기본적으로 샤드를 기준으로 작업을 쪼개는 것이기 때문에 각 요청 슬라이스가 동일한 작업량을 분배받는것은 아니다 requests_per_second 옵션은 각 슬라이스에 쪼개져서 적용됨을 알아야 한다. requests_per_second=1000 + slices=5라면, 각 슬라이스는 200씩을 분배 받게 된다. 4.2.4 delete by query delete by query는 update by query처럼 먼저 지정한 검색 쿼리로 삭제할 대상을 지정한 뒤에 삭제를 수행하는 작업이다. 보통은 주기적인 배치성 작업으로 오래된 데이터 등 더이상 사용하지 않는 데이터를 삭제하는 경우에 사용된다.POST [인덱스 이름]/_delete_by_query{ \"query\": { // ... }}delete by query 동작 방식 update by query와 마찬가지로 검색 조건에 맞는 문서를 찾아 스냅샷을 찍는다. 삭제 작업이 진행되는 동안 문서의 내용이 변경됐다면 버전 충돌이 일어날떄의 처리 등 update와 동일하다. 이 외에 tasks API를 통한 관리, 스로틀링 적용, 슬라이싱 적용 등 모두 update와 동일하다.4.3 검색 API ES의 기본이자 핵심은 검색엔진이다. 자주 사용되는 검색 쿼리, 쿼리 문맥과 필터 문맥, 검색 결과 정렬, 페이지네이션을 학습해보자.4.3.1 검색 대상 지정 ES는 다양한 종류의 검색 쿼리를 제공한다. GET/POST 중 무엇을 사용하더라도 동작은 동일하다.GET [인덱스 이름]/_searchPOST [인덱스 이름]/_searchGET _searchPOST _search검색 관련 인덱스 이름을 지정하지 않으면 전체 인덱스에 대해 검색한다. 인덱스 범위를 최대한 좁혀서 검색해야 성능 부담이 적으므로 일반적으로는 인덱스 이름을 명시적으로 지정한다. 인덱스 이름을 지정할 떄는 와일드카드 문자(*)와 콤마로 구분하여 검색 대상을 여럿 지정하는것도 가능하다.### _searchGET /my_index*,analyzer_test*,mapping_test/_searchHost: localhost:9200Content-Type: application/json{ \"took\": 76, \"timed_out\": false, \"_shards\": {\t\"total\": 3,\t\"successful\": 3,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 3,\t \"relation\": \"eq\"\t},\t\"max_score\": 1.0,\t\"hits\": [\t {\t\t\"_index\": \"my_index2\",\t\t\"_id\": \"3\",\t\t\"_score\": 1.0,\t\t\"_source\": {\t\t \"title\": \"hello world\",\t\t \"views\": 1234,\t\t \"public\": true,\t\t \"point\": 4.5,\t\t \"created_at\": \"2019-01-17T14:05:01.234Z\"\t\t}\t },\t {\t\t\"_index\": \"my_index2\",\t\t\"_id\": \"1\",\t\t\"_score\": 1.0,\t\t\"_source\": {\t\t \"hello\": \"world2\"\t\t}\t },\t {\t\t\"_index\": \"my_index2\",\t\t\"_id\": \"2\",\t\t\"_score\": 1.0,\t\t\"_source\": {\t\t \"hello\": \"world2\"\t\t}\t }\t] }} 위 예시처럼 검색 대상만 지정하고 쿼리 종류와 질의어를 지정하지 않으면 지정한 대상 내 모든 문서가 hit된다.4.3.2 쿼리 DSL 검색과 쿼리 문자열 검색 ES에서는 크게 요청 본문에 쿼리 DSL을 기술하여 검색하는 방법과 요청 주소줄에 q라는 매개변수를 넣고 그곳에 루씬 쿼리 문자열을 지정하는 2가지 방법을 제공한다.쿼리 DSL 검색 아래와 같이 요청 본문에 query 필드를 넣어 그 안에 원하는 쿼리와 질의어를 기술한다.### 쿼리 DSLGET /my_index2/_searchHost: localhost:9200Content-Type: application/json{ \"query\": { \"match\": { \"title\" : \"hello\" } }}쿼리 문자열 검색 q 매개변수에 루씬 쿼리 문자열을 넣어서 하는 방식은 다음과 같다. 이 요청 방식 특성상 복잡한 쿼리르 지정하기 어렵고 긴 쿼리르 전달하는 것도 부담스럽기 때문에 보통은 간단한 요청을 이용하는 경우 사용된다.### 쿼리 문자열GET /my_index2/_search?q=title:helloHost: localhost:9200Content-Type: application/json루씬 쿼리 문자열 문법 몇가지 ES queryStirngSyntax 공식문서루씬 쿼리 문자열 블로그루씬 쿼리 문자열 관련 참고사항 와일드카드 검색은 서비스환경에서는 사실상 사용하지 않는것이 좋다. *ello 나 ?ello처럼 와일드카드 문자가 앞에 오는 쿼리는 부담이 더욱 심하다.(인덱스가 들고있는 모든 term을 가지고 검색을 해야함) 단 한번의 쿼리로 ES 클러스터 전체를 다운시킬 수도 있다. indices.query.query_string.allowLeadingWildcard 설정을 false로 지정하면 와일드카 문자가 앞에 오는 쿼리르 막을 수 있다. search.allow_expensive_queries 설정을 false로 지정하면 와일드카드 검색을 포함한 몇몇 무거원 쿼리를 사용한 검색 자체를 아예 막을 수 있다.4.3.3 match_all 쿼리 match_all 쿼리는 모든 문서를 매치하는 쿼리이다.(query 부분을 비워두면 기본값으로 지정됨)GET [인덱스 이름]/_search{ \"query\": { \"math_all\": {} }}4.3.4 match 쿼리 match는 지정한 필드의 내용이 질의어와 매치되는 문서를 찾는 쿼리이다. 필드가 text  타입이라면 질의어도 모두 애널라이저로 분석된다.GET [인덱스 이름]/_search{ \"query\": { \"match\": { \"fieldName\": { \"query\": \"test query sentence\", \"operator\": \"and\" } } }} 위 예제를 기준으로 설명하면 다음과 같다. 인덱스의 fieldName 필드가 text타입고, standard 애널라이저를 사용한다면, 검색 쿼리도 test, query, sentence 총 3개의 토큰으로 분석된다. match 쿼리는 기본동작이 OR로 동작하는데 위 예시에서는 \"operator\": \"and\"을 추가하여 위 3개의 텀에 모두 매칭되는 경우에만 검색이 이루어진다. 4.3.5 term 쿼리 term 쿼리는 지정한 필드의 값이 질의어와 정확히 일치하는 문서를 찾는 쿼리이다. 필드 타입에 지정된것과 동일하게 처리됨. 대상 필드에 노멀라이저가 지정되어 있다면 질의어도 노멀라이저 처리를 거친다. GET [인덱스 이름]/_search{ \"query\": { \"term\": { \"fieldName\": { \"value\": \"hello\" } } }}term 쿼리 주의사항 keyword 타입과 보통 잘 어울리는 쿼리이다. text 타입의 필드를 대상으로 하는 경우 질의어는 노멀라이저 처리를 거치지만, 필드의 값은 애널라이저로 분석한 뒤 생성된 역색인을 이용하게 된다.4.3.6 terms 쿼리 term 쿼리와 매우 유사한데, 질의어를 여러개 지정할 수 있고, 하나 이상의 질의어가 일치하면 검색 결과에 포함하는 쿼리이다.GET [인덱스 이름]/_search{ \"query\": { \"terms\": { \"fieldName\": [\"hello\", \"world\"] } }}4.3.7 range 쿼리 range 쿼리는 지정한 필드의 값이 특정 범위 내에 있는 문서를 찾는 쿼리이다.### ex1GET [인덱스 이름]/_search{ \"query\": { \"range\": { \"fieldName\": { \"gte\": 100, \"lt\": 200 } } }}### ex2 - dateGET [인덱스 이름]/_search{ \"query\": { \"range\": { \"fieldName\": { \"gte\": \"2019-01-15T00:00:00.000Z||+36h/d\", \"lt\": \"now-3h/d\" } } }} gt(greater than), lt(less than), gte, lte를 지정하여 range 검색을 할 수 있다.range query 주의사항 ES 문자열 필드 대상으로 한 range 쿼리를 부학가 큰 쿼리로 분류한다. 사용시 데이터 양상을 파악하고 부담이 없는 상황에서만 사용해야 한다. search.allow_expensive_queries 설정을 false로 지정하면 막을 수 있다.range query에 date를 사용하는 경우 표현식 now: 현재시각을 나타낼 수 있다.-|| : 날짜 시간 문자열의 마지막에 붙이고, 이 뒤에 붙는 문자열은 시간 계산식으로 파싱된다. +, - : 지정된 시간만큼 더하거나 빼는 연산을 수행한다. / : 버림을 수행한다. (ex. /d는 날짜 단위 이하의 시간을 버림한다.)4.3.8 prefix 쿼리 prefix 쿼리는 필드의 값이 지정한 질의어로 시작하는 문서를 찾는 쿼리이다.GET [인덱스 이름]/_search{ \"query\": { \"prefix\": { \"fieldName\": { \"value\": \"hello\" } } }}prefix 사용시 주의사항 preifx도 무거운 쿼리로 분류되나, 와일드카드 검색처럼 아예 사용하지 말아야 할 정도는 아니다. prefix를 서비스 호출 용도로 사용하려 한다면 인덱스 필드 맵핑 설정에 index_prefixes 설정을 넣는 방법이 있다.### index_prefixesPUT /prefix_mapping_testHost: localhost:9200Content-Type: application/json{ \"mappings\": { \"properties\": { \"prefixField\": { \"type\": \"text\", \"index_prefixes\": { \"min_chars\": 3, \"max_chars\" : 5 } } } }} 위처럼 index_prefixes를 지정하면 ES 문서를 색인할 때 min_chars와 max_chars 사이의 prefix를 미리 별도 색인한다. 색인 크기와 색인 속도에서 손해를 보는 대신 prefix 쿼리의 성능을 높일 수 있다. min_chars의 기본값은 2이고, max_chars의 기본값은 5이다. search.allow_expensive_queries 설정을 false로 지정하면, index_prefixes가 적용되지 않은 prefix 쿼리는 사용할 수 없다. 4.3.9 exists 쿼리 exists 쿼리는 지정한 필드를 포함한 문서를 검색한다.GET [인덱스 이름]/_search{ \"query\": { \"exists\": { \"field\": \"fieldName\" } }}4.3.10 bool 쿼리 bool 쿼리는 여러 쿼리를 조합하여 검색하는 쿼리이다. must, must_not, filter, shoulde의 4가지 종류의 조건절에 다른 쿼리를 조합하여 사용한다.GET [인덱스 이름]/_search{ \"query\": { \"bool\": { \"must\" : [ { \"term\" : {\"field1\": {\"value\": \"hello\"} } }, { \"term\" : {\"field2\": {\"value\": \"world\"} } } ], \"must_not\" : [ { \"term\" : {\"field4\": {\"value\": \"elasticsearch-test\"} } } ], \"filter\" : [ { \"term\" : {\"field3\": {\"value\": true} } } ], \"should\" : [ { \"match\" : {\"field4\": {\"query\": \"elasticsearch\"} } }, { \"match\" : {\"field5\": {\"query\": \"lucene\"} } } ], \"minimum_should_match\": 1 } }} must 조건절과 filter 조건절에 들어간 하위 쿼리는 모두 AND 조건으로 만족해야 최종 검색 결과에 포함된다. must_not 조건절에 들어간 쿼리를 만족하는 문선느 최종 검색 결과에서 제외된다. should 조건절에 들어간 쿼리는 minimum_should_match에 지정한 개수 이상의 하위 쿼리를 만족하는 문서가 최종 검색 결과에 포함된다. minimum_should_match의 기본값은 1이고, 이 경우 should 조건절에 들어간 쿼리는 OR조건으로 검색하는 것으로 이해할 수 있다.쿼리 문맥과 필터 문맥 must와 filter는 모두 AND 조건으로 검색을 수행하지만, 점수를 계산하느냐 여부가 다르다. filter 조건에 들어간 쿼리는 단순히 문서 매치 여부만을 판단하고, 랭킹에 사용할 점수를 매기지 않는다. must_not도 점수를 매기지 않는다. 점수를 매기지 않고 단순히 조건을 만족하는지 여부를 참 또는 거짓으로 따지는 검색과정을 필터 문맥(filter context)라고 한다. 조건을 얼마나 더 만족하는지 유사도 점수를 매기는 검색 과정을 쿼리 문맥(query context) 라고 한다. 유사도가 필요하지 않은 경우 필터 문맥으로 검색해야 성능상 유리하다. 쿼리 문맥 상대적으로 성능이 느리고, 쿼리 캐시 활용 불가 bool.must, bool.should, match, term 필터 문맥 상대적으로 성능이 빠르고, 쿼리 캐시 활용 가능 bool.filter, bool.must_not, exists, range, constant_score 쿼리 수행 순서 bool 쿼리를 사용하여 여러 쿼리를 조합하는 경우 어떤 실행순러ㅗ 수행된다는 규칙 이없다. ES 내부적으로 쿼리를 루씬의 여러 쿼리로 쪼갠 뒤 조합하여 재작성하는데, 이 각 쿼리별 비용이 얼마나 소요될지는 내부적으로 추정하고, 이 추정된 비용을 기준으로 무엇을 먼저 실행할지 내부적으로 결정된다. 또 하부 쿼리를 병렬 수행하기도 하고, 복잡한 과정을 거친다. 만약 내부적인 쿼리 실행 순서를 원하는대로 해야한다면 커스텀 플러그인에서 커스텀 쿼리를 만들어야 한다.4.3.11 constant_score 쿼리 constant_score 쿼리는 하위 filter 부분에 지정한 쿼리를 필터 문맥에서 검색하는 쿼리이다. 유사도는 일괄적으로 1로 지정된다.### 4.3.11 constant_score 쿼리GET [인덱스 이름]/_search{ \"query\": { \"constant_score\": { \"filter\" : { \"term\": { \"fieldName\": \"hello\" } } } }}4.3.12 그 외 주요 매개변수 쿼리 종류와 관계 없이 검색 API에 공통적으로 적용할 수 있는 주요 매개변수 몇가지를 알아보자라우팅 라우팅 지정 여부가 가져오는 성능차이가 크다. 라우팅을 지정할 수 있는 경우라면 최대한 라우팅 이득을 볼수 있도록 설계하자(ex. 특정 사용자 기준으로 데이터를 관리하는 경우)### 라우팅GET [인덱스이름]/_search?routing=i[라우팅]{ \"query\": { // ... }}explain 검색을 수행하는 동안 쿼리의 각 하위 부분에서 점수가 어떻게 계산됐는지 설명을 확인하기 위해 explain을 사용할 수 있다.GET [인덱스이름]/_search?explain=true{ \"query\": { // ... }}GET /my_index3/_search?explain=trueHost: localhost:9200Content-Type: application/json{ \"query\": { \"bool\": { \"must\" : [ { \"term\" : {\"field1\": {\"value\": \"hello\"} } }, { \"term\" : {\"field2\": {\"value\": \"world\"} } } ], \"must_not\" : [ { \"term\" : {\"field4\": {\"value\": \"elasticsearch-test\"} } } ], \"filter\" : [ { \"term\" : {\"field3\": {\"value\": true} } } ], \"should\" : [ { \"match\" : {\"field4\": {\"query\": \"elasticsearch\"} } }, { \"match\" : {\"field5\": {\"query\": \"lucene\"} } } ], \"minimum_should_match\": 1 } }}{ \"took\": 23, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 1,\t \"relation\": \"eq\"\t},\t\"max_score\": 1.1507283,\t\"hits\": [\t {\t\t\"_shard\": \"[my_index3][0]\",\t\t\"_node\": \"b_fDpGOqSWOciib82ngCTA\",\t\t\"_index\": \"my_index3\",\t\t\"_id\": \"1\",\t\t\"_score\": 1.1507283,\t\t\"_source\": {\t\t \"field1\": \"hello\",\t\t \"field2\": \"world\",\t\t \"field3\": true,\t\t \"field4\": \"elasticsearch\",\t\t \"field5\": \"lucene\"\t\t},\t\t\"_explanation\": {\t\t \"value\": 1.1507283,\t\t \"description\": \"sum of:\",\t\t \"details\": [\t\t\t{\t\t\t \"value\": 0.2876821,\t\t\t \"description\": \"weight(field1:hello in 0) [PerFieldSimilarity], result of:\",\t\t\t \"details\": [\t\t\t\t{\t\t\t\t \"value\": 0.2876821,\t\t\t\t \"description\": \"score(freq=1.0), computed as boost * idf * tf from:\",\t\t\t\t \"details\": [\t\t\t\t\t{\t\t\t\t\t \"value\": 2.2,\t\t\t\t\t \"description\": \"boost\",\t\t\t\t\t \"details\": []\t\t\t\t\t},\t\t\t\t\t{\t\t\t\t\t \"value\": 0.2876821,\t\t\t\t\t \"description\": \"idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\",\t\t\t\t\t \"details\": [\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1,\t\t\t\t\t\t \"description\": \"n, number of documents containing term\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1,\t\t\t\t\t\t \"description\": \"N, total number of documents with field\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t}\t\t\t\t\t ]\t\t\t\t\t},\t\t\t\t\t{\t\t\t\t\t \"value\": 0.45454544,\t\t\t\t\t \"description\": \"tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\",\t\t\t\t\t \"details\": [\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.0,\t\t\t\t\t\t \"description\": \"freq, occurrences of term within document\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.2,\t\t\t\t\t\t \"description\": \"k1, term saturation parameter\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 0.75,\t\t\t\t\t\t \"description\": \"b, length normalization parameter\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.0,\t\t\t\t\t\t \"description\": \"dl, length of field\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.0,\t\t\t\t\t\t \"description\": \"avgdl, average length of field\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t}\t\t\t\t\t ]\t\t\t\t\t}\t\t\t\t ]\t\t\t\t}\t\t\t ]\t\t\t},\t\t\t{\t\t\t \"value\": 0.2876821,\t\t\t \"description\": \"weight(field2:world in 0) [PerFieldSimilarity], result of:\",\t\t\t \"details\": [\t\t\t\t{\t\t\t\t \"value\": 0.2876821,\t\t\t\t \"description\": \"score(freq=1.0), computed as boost * idf * tf from:\",\t\t\t\t \"details\": [\t\t\t\t\t{\t\t\t\t\t \"value\": 2.2,\t\t\t\t\t \"description\": \"boost\",\t\t\t\t\t \"details\": []\t\t\t\t\t},\t\t\t\t\t{\t\t\t\t\t \"value\": 0.2876821,\t\t\t\t\t \"description\": \"idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\",\t\t\t\t\t \"details\": [\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1,\t\t\t\t\t\t \"description\": \"n, number of documents containing term\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1,\t\t\t\t\t\t \"description\": \"N, total number of documents with field\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t}\t\t\t\t\t ]\t\t\t\t\t},\t\t\t\t\t{\t\t\t\t\t \"value\": 0.45454544,\t\t\t\t\t \"description\": \"tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\",\t\t\t\t\t \"details\": [\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.0,\t\t\t\t\t\t \"description\": \"freq, occurrences of term within document\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.2,\t\t\t\t\t\t \"description\": \"k1, term saturation parameter\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 0.75,\t\t\t\t\t\t \"description\": \"b, length normalization parameter\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.0,\t\t\t\t\t\t \"description\": \"dl, length of field\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.0,\t\t\t\t\t\t \"description\": \"avgdl, average length of field\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t}\t\t\t\t\t ]\t\t\t\t\t}\t\t\t\t ]\t\t\t\t}\t\t\t ]\t\t\t},\t\t\t{\t\t\t \"value\": 0.2876821,\t\t\t \"description\": \"weight(field4:elasticsearch in 0) [PerFieldSimilarity], result of:\",\t\t\t \"details\": [\t\t\t\t{\t\t\t\t \"value\": 0.2876821,\t\t\t\t \"description\": \"score(freq=1.0), computed as boost * idf * tf from:\",\t\t\t\t \"details\": [\t\t\t\t\t{\t\t\t\t\t \"value\": 2.2,\t\t\t\t\t \"description\": \"boost\",\t\t\t\t\t \"details\": []\t\t\t\t\t},\t\t\t\t\t{\t\t\t\t\t \"value\": 0.2876821,\t\t\t\t\t \"description\": \"idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\",\t\t\t\t\t \"details\": [\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1,\t\t\t\t\t\t \"description\": \"n, number of documents containing term\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1,\t\t\t\t\t\t \"description\": \"N, total number of documents with field\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t}\t\t\t\t\t ]\t\t\t\t\t},\t\t\t\t\t{\t\t\t\t\t \"value\": 0.45454544,\t\t\t\t\t \"description\": \"tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\",\t\t\t\t\t \"details\": [\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.0,\t\t\t\t\t\t \"description\": \"freq, occurrences of term within document\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.2,\t\t\t\t\t\t \"description\": \"k1, term saturation parameter\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 0.75,\t\t\t\t\t\t \"description\": \"b, length normalization parameter\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.0,\t\t\t\t\t\t \"description\": \"dl, length of field\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.0,\t\t\t\t\t\t \"description\": \"avgdl, average length of field\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t}\t\t\t\t\t ]\t\t\t\t\t}\t\t\t\t ]\t\t\t\t}\t\t\t ]\t\t\t},\t\t\t{\t\t\t \"value\": 0.2876821,\t\t\t \"description\": \"weight(field5:lucene in 0) [PerFieldSimilarity], result of:\",\t\t\t \"details\": [\t\t\t\t{\t\t\t\t \"value\": 0.2876821,\t\t\t\t \"description\": \"score(freq=1.0), computed as boost * idf * tf from:\",\t\t\t\t \"details\": [\t\t\t\t\t{\t\t\t\t\t \"value\": 2.2,\t\t\t\t\t \"description\": \"boost\",\t\t\t\t\t \"details\": []\t\t\t\t\t},\t\t\t\t\t{\t\t\t\t\t \"value\": 0.2876821,\t\t\t\t\t \"description\": \"idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\",\t\t\t\t\t \"details\": [\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1,\t\t\t\t\t\t \"description\": \"n, number of documents containing term\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1,\t\t\t\t\t\t \"description\": \"N, total number of documents with field\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t}\t\t\t\t\t ]\t\t\t\t\t},\t\t\t\t\t{\t\t\t\t\t \"value\": 0.45454544,\t\t\t\t\t \"description\": \"tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\",\t\t\t\t\t \"details\": [\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.0,\t\t\t\t\t\t \"description\": \"freq, occurrences of term within document\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.2,\t\t\t\t\t\t \"description\": \"k1, term saturation parameter\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 0.75,\t\t\t\t\t\t \"description\": \"b, length normalization parameter\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.0,\t\t\t\t\t\t \"description\": \"dl, length of field\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t},\t\t\t\t\t\t{\t\t\t\t\t\t \"value\": 1.0,\t\t\t\t\t\t \"description\": \"avgdl, average length of field\",\t\t\t\t\t\t \"details\": []\t\t\t\t\t\t}\t\t\t\t\t ]\t\t\t\t\t}\t\t\t\t ]\t\t\t\t}\t\t\t ]\t\t\t},\t\t\t{\t\t\t \"value\": 0.0,\t\t\t \"description\": \"match on required clause, product of:\",\t\t\t \"details\": [\t\t\t\t{\t\t\t\t \"value\": 0.0,\t\t\t\t \"description\": \"# clause\",\t\t\t\t \"details\": []\t\t\t\t},\t\t\t\t{\t\t\t\t \"value\": 1.0,\t\t\t\t \"description\": \"field3:T\",\t\t\t\t \"details\": []\t\t\t\t}\t\t\t ]\t\t\t}\t\t ]\t\t}\t }\t] }} _explanation 부분에서 점수 계산 과정의 상세한 설명을 확인 할 수 있다. _explanation을 만들기 위해 내부적으로 쿼리를 덜 최적화해 수행하기도 하므로, 성능이 비교적 하락할 수 있다. 서비스 환경에서는 사용하지 않고 디버깅 용도로만 사용하는 것이 좋다. 각 조건들을 검색할떄 나왔던 score를 합산하여 최종 score가 결정됨.search_type search_type을 지정하면 유사도 점수를 계산할 때 각 샤드 레벨에서 계산을 끝낼지 여부를 선택할 수 있다.GET [인덱스 이름]/_search?search_type=dfs_query_then_fetch{ \"query\": { // ... }} query_then_fetch(기본값) 각 샤드 레벨에서 유사도 점수 계싼을 끝낸다. 점수 계산이 약간 부정확할 수 있지만, 검색 성능의 차이가 크기 때문에 특별한 경우가 아니라면 이대로 사용하는것을 권장 dfs_query_then_fetch 모든 샤드로부터 정보를 모아 유사도 점수를 글로벌하게 계산한다. 점수의 정확도는 올라가지만 검색 성능은 떨어진다. 4.3.13. 검색 결과 정렬 요청 본문에 sort를 지정하면 검색 결과를 정렬할 수 있다. 정렬 대상 필드를 여러개로 지정하는 경우 요청에 지정한 순서대로 정렬을 수행한다.GET [인덱스 이름]/_search{ \"query\": { // ... }, \"sort\": [ {\"field1\": {\"order\": \"desc\"} }, {\"field2\": {\"order\": \"asc\"} }, \"field3\" ]}정렬시 유의사항 ES에서는 필드타입에 따라서 정렬에 사용할 수 있는 타입과 불가능한 타입이 있다. 숫자, date, boolean, keyword 타입은 정렬 대상이 될수 있다. text 타입은 정렬 불가능하다. fielddata를 true로 지정하면 text 타입도 정렬에 사용할수 있으나, 성능문제가 있으므로 사용을 지양해야 한다. 정렬에서 _score, _doc을 지정해서 정렬을 수행할 수도 있다. sort 기본값은 _score 내림차순으로 조회된다. 정렬 수행을 위해서는 필드의 값이 메모리에 올라가야하는데 운영환경 서비스 사용시 정렬 대상 필드를 integer, short, float 등 타입으로 설계하는 것도 좋은 방법이다.4.3.14. 페이지네이션 검색 결과를 페이지네이션하는 방법을 알아보자.from과 size size는 검색 API의 결과로 몇 개의 문서를 반환할것인지를 지정한다.(기본값 : 10) from은 몇 번쨰 문서부터 결과를 반환할지에 대한 오프셋을 지정한다.(기본값 : 0)GET [인덱스 이름]/_search{ \"from\": 0, \"size\": 10, \"query\": { // ... }}from과 size 방식의 한계점 실제로 페이지네이션을 구현할때 from, size는 제한적으로 사용해야 한다. 내부적으로 MySQL의 limit-offset 방식의 쿼리처럼 동작하여 페이지 후반부로 갈수록 성능 이슈가 있을수 있음. 예를 들어 from:15, size:5로 지정한 검색을 하는 경우 상위 20개 문서를 가져온 뒤에 하위 5개만 잘라서 반환하는 방식으로 동작한다. 이전 페이지를 검색할 떄의 상태와 페이지를 넘기고 다음 검색을 수행할 때 인덱스 상태가 동일하지 않을수 있다는 문제가 있다. 페이징 검색 요청 사이에 새로운 문서가 색인되거나 삭제될 경우 의도한 페이지의 신규 데이터를 조회하는것이 아니라 데이터가 겹칠수 있다. 엄격한 페이지네이션을 제공해야 할 경우 from, size는 사용하지 않아야 한다. ES에서 from + size 합이 10,000을 넘어서는 검색은 수행을 제한하고 있다. index.max_result_window 값을 조정해 변경할 수 있지만 비권장 scroll scroll은 검색 조건에 매칭되는 전체 문서를 모두 순회해야 할 때 적합한 방법이다. scroll은 순회하는 동안에는 최초 검색시 문맥(search_context)가 유지되어, 중복이나 누락이 발생하지 않는다.GET [인덱스 이름]/_search?scroll=1m{ \"size\": 1000, \"query\": { \"bool\": { \"must\" : [ { \"term\" : {\"field1\": {\"value\": \"hello\"} } }, { \"term\" : {\"field2\": {\"value\": \"world\"} } } ] } }}{ \"_scroll_id\": \"FGluY2x1ZGVfY29udGV4dF91dWlkDXF1ZXJ5QW5kRmV0Y2gBFnA3S3pUN1JnU1dpYzdVU2tPSEltVGcAAAAAAAACDhZiX2ZEcEdPcVNXT2NpaWI4Mm5nQ1RB\", \"took\": 5, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 1,\t \"relation\": \"eq\"\t},\t\"max_score\": 0.5753642,\t\"hits\": [\t {\t\t\"_index\": \"my_index3\",\t\t\"_id\": \"1\",\t\t\"_score\": 0.5753642,\t\t\"_source\": {\t\t \"field1\": \"hello\",\t\t \"field2\": \"world\",\t\t \"field3\": true,\t\t \"field4\": \"elasticsearch\",\t\t \"field5\": \"lucene\"\t\t}\t }\t] }} 첫번쨰 scroll 검색을 할떄 scroll_id가 반환되는데, 이 값을 기준으로 scroll 검색을 수행하면 되고, 빈 hits 응답이 올때까지 scroll 검색을 반복하면 된다.GET /_search/scrollHost: localhost:9200Content-Type: application/json{ \"scroll_id\": \"FGluY2x1ZGVfY29udGV4dF91dWlkDXF1ZXJ5QW5kRmV0Y2gBFnA3S3pUN1JnU1dpYzdVU2tPSEltVGcAAAAAAAACSBZiX2ZEcEdPcVNXT2NpaWI4Mm5nQ1RB\", \"scroll\": \"1m\"}수동 검색 문맥 삭제 빠른 자원 반납을 위해 아래와 같이 명시적으로 검색 문맥을 삭제할수 있다.DELETE /_search/scrollHost: localhost:9200Content-Type: application/json{ \"scroll_id\": \"FGluY2x1ZGVfY29udGV4dF91dWlkDXF1ZXJ5QW5kRmV0Y2gBFnA3S3pUN1JnU1dpYzdVU2tPSEltVGcAAAAAAAACSBZiX2ZEcEdPcVNXT2NpaWI4Mm5nQ1RB\"}scroll 검색시 성능 향상 scroll 검색은 검색 결과의 정렬 여부가 상관없는 작업에 주로 사용하는 경우가 많은데, 이 경우 _doc으로 정렬을 지정하면 유사도 점수를 계산하지 않으며, 정렬을 위한 별도의 자원도 사용하지 않아서 성능을 끌어올릴 수 있다. sort의 기본값이 _score desc임을 기억하자 GET [인덱스 이름]/_search?scroll=1m{ \"size\": 1000, \"query\": { // ... }, \"sort\": [\"_doc\"]}scroll의 사용 목적 scroll API는 사실 서비스에서 지속적으로 호출하는 용도로는 적합하지 않다. 주로 대량의 데이터를 다른 스토리지로 이전하거나 덤프하는 용도로 사용된다.search_after 서비스에서 사용자에게 검색 결과를 페이지네이션으로 제공하는 경우 search_after을 사용하는 것이 가장 적합하다. search_after는 기본적으로 search의 sort을 활용하여 처리한다. 이 떄 정렬조건이 동점이 나오지 않도록 정렬 필드를 지정해야 한다.GET /kibana_sample_data_ecommerce/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 20, \"query\": { \"term\": { \"currency\" : { \"value\": \"EUR\" } } }, \"sort\": [ { \"order_date\": \"desc\" }, { \"order_id\": \"asc\" } ]}\"sort\": [ 1704573590000, \"591924\"] 첫 번쨰 검색이 끝나면 검색 결과 sort 값을 가져와 search_after 부분에 넣어 그 다음 검색을 요청할 수 있다.### search_after nextGET /kibana_sample_data_ecommerce/_searchHost: localhost:9200Content-Type: application/json{ \"size\": 20, \"search_after\" : [ 1704573590000, \"591924\" ], \"query\": { \"term\": { \"currency\" : { \"value\": \"EUR\" } } }, \"sort\": [ { \"order_date\": \"desc\" }, { \"order_id\": \"asc\" } ]}search_after의 동점 제거용 필드 지정시 주의사항 _id 값을 동점 제거용 기준 필드로는 사용을 지양해야 한다. _id 필드는 doc_values가 꺼져 있기 때문에 이를 기준으로 정렬하게 되면 많은 메모리를 사용하게 된다. 동점 제거용 필드를 제대로 지정했다 하더라도, 인덱스 상태가 변하는 도중이라면 페이지네이션 과정에서 누락되는 문서가 발생할수 있다. 인덱스의 상태를 특정 시점으로 고려하려면 point in time API를 함께 조합해서 사용해야 한다.point in time API point in time API는 검색 대상의 상태를 고정할 때 사용한다. keep_live 매개변수에 상태를 유지할 시간을 지정한다.POST /kibana_sample_data_ecommerce/_pit?keep_alive=1mHost: localhost:9200Content-Type: application/json{ \"id\": \"gcSHBAEca2liYW5hX3NhbXBsZV9kYXRhX2Vjb21tZXJjZRZsMFQydEtZeVRuZWdHQmhxRjh0STV3ABZmOEFhOXNqLVJiYU9lYnIzYWNEY3hRAAAAAAAAAAueFkdhdW1CUXBtUmFLdURtam1SdFhiVVEAARZsMFQydEtZeVRuZWdHQmhxRjh0STV3AAA=\"} 이렇게 얻은 pit id를 search_after와 같은곳에서 활용할 수 있다. pit id가 있는 상태에서는 대상 인덱스를 지정하지 않고 search한다. pit를 지정하는 것 자체가 검색 대상을 지정하는것이기 떄문이다. pit을 지정하면 동점 제거용 필드도 별도로 지정할 필요가 없다. search_after를 사용하는것은 이 search api 응답에 전달받은 sort 값을 기준으로 동일하게 사용하면 된다.GET /_searchHost: localhost:9200Content-Type: application/json{ \"size\": 20, \"query\": { \"term\": { \"currency\" : { \"value\": \"EUR\" } } }, \"sort\": [ { \"order_date\": \"desc\" }, { \"order_id\": \"asc\" } ], \"pit\": { \"id\": \"gcSHBAEca2liYW5hX3NhbXBsZV9kYXRhX2Vjb21tZXJjZRZsMFQydEtZeVRuZWdHQmhxRjh0STV3ABZmOEFhOXNqLVJiYU9lYnIzYWNEY3hRAAAAAAAAAAw1FkdhdW1CUXBtUmFLdURtam1SdFhiVVEAARZsMFQydEtZeVRuZWdHQmhxRjh0STV3AAA=\", \"keep_alive\": \"1m\" }}pit 명시적인 삭제 pit을 다 사용한 뒤에는 명시적으로 삭제 할 수 있다.DELETE /_pitHost: localhost:9200Content-Type: application/json{ \"id\": \"gcSHBAEca2liYW5hX3NhbXBsZV9kYXRhX2Vjb21tZXJjZRZsMFQydEtZeVRuZWdHQmhxRjh0STV3ABZmOEFhOXNqLVJiYU9lYnIzYWNEY3hRAAAAAAAAAAw1FkdhdW1CUXBtUmFLdURtam1SdFhiVVEAARZsMFQydEtZeVRuZWdHQmhxRjh0STV3AAA=\"}Reference ES 공식 Reference HTTP Chunk 참고 루씬 쿼리 문자열 블로그" }, { "title": "[엘라스틱서치 바이블] 3. 인덱스 설계 -2", "url": "/posts/es-bible-3-2/", "categories": "DevLog, Elasticsearch", "tags": "Elasticsearch", "date": "2023-12-08 20:16:00 +0900", "snippet": "3. 인덱스 설계 -23.3 애널라이저와 토크나이저 이전장을 통해 text 필드의 데이터는 애널라이저를 통해 분석돼 여러 텀으로 쪼개져 색인된다는것을 배웠다. 구체적으로 애널라이저가 동작하는 과정과 ES가 자체적으로 제공하는 빌트인 애널라이저를 사용해보고 특정 상황에 맞는 커스텀 애널라이저를 적용하는 방법을 살펴보자. 애널라이저는 0개 이상의 캐...", "content": "3. 인덱스 설계 -23.3 애널라이저와 토크나이저 이전장을 통해 text 필드의 데이터는 애널라이저를 통해 분석돼 여러 텀으로 쪼개져 색인된다는것을 배웠다. 구체적으로 애널라이저가 동작하는 과정과 ES가 자체적으로 제공하는 빌트인 애널라이저를 사용해보고 특정 상황에 맞는 커스텀 애널라이저를 적용하는 방법을 살펴보자. 애널라이저는 0개 이상의 캐릭터 필더, 1개의 토크나이저, 0개 이상의 토큰 필터로 구성된다. 문서가 들어오면 위 애널라이저를 통해 텀을 만들고 이를 색인하는 과정을 거치게 된다.3.3.1 analyze API ES는 애널라이저와 각 구성 요소의 동작을 쉽게 테스트해볼수 있도록 API를 제공하고 있다.GET _analyzePOST _analyze### analyze APIPOST /_analyzeHost: localhost:9200Content-Type: application/json{ \"analyzer\": \"standard\", \"text\": \"Hello, HELLO, World!\"}{ \"tokens\": [\t{\t \"token\": \"hello\",\t \"start_offset\": 0,\t \"end_offset\": 5,\t \"type\": \"&lt;ALPHANUM&gt;\",\t \"position\": 0\t},\t{\t \"token\": \"hello\",\t \"start_offset\": 7,\t \"end_offset\": 12,\t \"type\": \"&lt;ALPHANUM&gt;\",\t \"position\": 1\t},\t{\t \"token\": \"world\",\t \"start_offset\": 14,\t \"end_offset\": 19,\t \"type\": \"&lt;ALPHANUM&gt;\",\t \"position\": 2\t} ]} standard 애널라이저의 분석 결과를 위와 같이 확인해볼 수 있다.3.3.2 캐릭터 필터 캐릭터 필터는 텍스트를 캐릭터의 스트림으로 받아서 특정한 문자를 추가, 변경, 삭제한다. 애널라이저에는 0개 이상의 캐릭터 필터를 지정할 수 있다. 각 필터는 순서대로 수행된다.ES 내장 빌트인 필터 HTML_strip 캐릭터 필터 : &lt;b&gt; 와 같은 HTML 요소 안쪽의 데이터를 꺼낸다. &amp;apos; 같은 HTML 엔티티도 디코딩한다. mapping 캐릭터 필터 : 치환할 대상이 되는 문자와 치환 문자를 맵 형태로 선언한다. pattern replace 캐릭터 필터 : 정규 표현식을 이용해서 문자를 치환한다.### 캐릭터 필터POST /_analyzeHost: localhost:9200Content-Type: application/json{ \"char_filter\": [\"html_strip\"], \"text\": \"&lt;p&gt;I&amp;apos;m so &lt;b&gt;happy&lt;/b&gt;!&lt;/p&gt;\"}{ \"tokens\": [\t{\t \"token\": \"\\nI'm so happy!\\n\",\t \"start_offset\": 0,\t \"end_offset\": 32,\t \"type\": \"word\",\t \"position\": 0\t} ]}3.3.3 토크나이저 토크나이저는 캐릭터 스트림을 받아서 여러 토큰으로 쪼개어 토큰 스크림을 만든다. 애널라이저에는 한 개의 토크나이저만 지정할 수 있다.standard 토크나이저 가장 기본적인 토크나이저로, Unicode Text segmentation 알고리즘을 사용하여 텍스트를 단어 단위로 나눈다. 대부분의 문장 부호(punctuation symbol)가 사라진다. 필드 맵핑에 특정 애널라이저를 지정하지 않으면 기본값으로 적용된다.keyword 토크나이저 들어온 텍스트를 쪼개지 않고 그대로 내보낸다.### keyword 토크나이저POST /_analyzeHost: localhost:9200Content-Type: application/json{ \"tokenizer\": \"keyword\", \"text\": \"Hello, HELLO, World!\"}{ \"tokens\": [\t{\t \"token\": \"Hello, HELLO, World!\",\t \"start_offset\": 0,\t \"end_offset\": 20,\t \"type\": \"word\",\t \"position\": 0\t} ]}rgram 토그나이저 텍스트를 min_gram 값 이상, max_gram 값 이하의 단위로 쪼갠다. ex. min_gram=2, max_gram=3, input=”hello” “he”, “hel”, “el”, “ell”, “ll”, “llo”로 총 5개의 토큰으로 쪼개진다. POST /_analyzeHost: localhost:9200Content-Type: application/json{ \"tokenizer\": { \"type\": \"ngram\", \"min_gram\": 3, \"max_gram\": 4 }, \"text\": \"Hello, World!\"} 21개의 토큰으로 쪼개지는 결과를 확인할 수 있다. 위 결과에서는 “o, W” 나 “ld!” 와 같은 공백 문자나 문장 부호가 포함되어 사실상 활용 의미가 없는 토큰도 포함되는데, 이를 방지하기 위한 token_chars 라는 속성을 제공한다.### ngram 토크나이저2POST /_analyzeHost: localhost:9200Content-Type: application/json{ \"tokenizer\": { \"type\": \"ngram\", \"min_gram\": 3, \"max_gram\": 4, \"token_chars\": [\"letter\"] }, \"text\": \"Hello, World!\"}{ \"tokens\": [\t{\t \"token\": \"Hel\",\t \"start_offset\": 0,\t \"end_offset\": 3,\t \"type\": \"word\",\t \"position\": 0\t},\t{\t \"token\": \"Hell\",\t \"start_offset\": 0,\t \"end_offset\": 4,\t \"type\": \"word\",\t \"position\": 1\t},\t{\t \"token\": \"ell\",\t \"start_offset\": 1,\t \"end_offset\": 4,\t \"type\": \"word\",\t \"position\": 2\t},\t{\t \"token\": \"ello\",\t \"start_offset\": 1,\t \"end_offset\": 5,\t \"type\": \"word\",\t \"position\": 3\t},\t{\t \"token\": \"llo\",\t \"start_offset\": 2,\t \"end_offset\": 5,\t \"type\": \"word\",\t \"position\": 4\t},\t{\t \"token\": \"Wor\",\t \"start_offset\": 7,\t \"end_offset\": 10,\t \"type\": \"word\",\t \"position\": 5\t},\t{\t \"token\": \"Worl\",\t \"start_offset\": 7,\t \"end_offset\": 11,\t \"type\": \"word\",\t \"position\": 6\t},\t{\t \"token\": \"orl\",\t \"start_offset\": 8,\t \"end_offset\": 11,\t \"type\": \"word\",\t \"position\": 7\t},\t{\t \"token\": \"orld\",\t \"start_offset\": 8,\t \"end_offset\": 12,\t \"type\": \"word\",\t \"position\": 8\t},\t{\t \"token\": \"rld\",\t \"start_offset\": 9,\t \"end_offset\": 12,\t \"type\": \"word\",\t \"position\": 9\t} ]} ngram 토크나이저는 ES에서 RDB LIKE '%검색어%' 와 유사한 검색을 구현하고 싶을때, 자동완성 관련 서비스를 구현하고 싶을때 주로 활용한다. min / max에 지정한 값만큼의 길이로 토큰을 분리하여 처리하는 토크나이저이다. min / max의 값이 5이상으로 높여서 차이가 2이상으로 벌어진다면 분석시도가 실패하기도 한다.ngram의 token_chars 속성 종류 설명 letter 언어의 글자로 분류되는 문자 digit 숫자로 분류되는 문자 whitespace 띄어쓰기나 줄바굼 문자 등 공백으로 인식되는 문자 punctuation !나 “ 등 문자 부호 symbol $와 같은 기호 custom custom_token_chars 설정을 통해 따로 지정한 커스텀 문자 edge_ngram 토크나이저 edge_ngram 토크나이저는 ngram 토크나이저와 유사한 동작을 수행한다. ngram 토크나이저와 다른 점은 생성된 모든 토큰의 시작 글자를 단어의 시작 글자로 고정시켜서 생성한다는 점이 다르다.### edge_ngram 토크나이저POST /_analyzeHost: localhost:9200Content-Type: application/json{ \"tokenizer\": { \"type\": \"edge_ngram\", \"min_gram\": 3, \"max_gram\": 4, \"token_chars\": [\"letter\"] }, \"text\": \"Hello, World!\"}{ \"tokens\": [\t{\t \"token\": \"Hel\",\t \"start_offset\": 0,\t \"end_offset\": 3,\t \"type\": \"word\",\t \"position\": 0\t},\t{\t \"token\": \"Hell\",\t \"start_offset\": 0,\t \"end_offset\": 4,\t \"type\": \"word\",\t \"position\": 1\t},\t{\t \"token\": \"Wor\",\t \"start_offset\": 7,\t \"end_offset\": 10,\t \"type\": \"word\",\t \"position\": 2\t},\t{\t \"token\": \"Worl\",\t \"start_offset\": 7,\t \"end_offset\": 11,\t \"type\": \"word\",\t \"position\": 3\t} ]}그 외 토크나이저 letter 토크나이저 : 공백, 특수문자 등 언어의 글자로 분류되는 문자가 아닌 문자를 만났을떄 쪼갠다. whitespace 토크나이저 : 공백문자를 만났을때 쪼갠다. pattern 토크나이저 : 정규표현식을 단어의 구분자로 사용하여 쪼갠다.3.3.4 토큰 필터 토큰 필터는 토큰 스트림을 받아서 토큰을 추가/변경/삭제한다. 하나의 애널라이저에 토큰 필터를 0개 이상 지정할수 있고, 여러개를지정한 경우 순차적으로 적용된다.대표적인 토큰필터 lowercase / uppercase 토큰필터 : 토큰의 내용을 소문자/대문자로 만들어 준다. stop 토큰 필터 : 불용어를 지정하여 제거할 수 있다. (ex. the, a, an, in 등) synonym 토큰 필터 : 유의어 사전 파일을 지정하여 지정된 유의어를 치환한다. pattern_replace 토큰필터 : 정규식을 사용하여 토큰의 내용을 치환한ㄷ. stemmer 토큰 필터 : 지원되는 몇몇 언어의 어간 추출을 수행한다.(한국어 지원X) trim 토큰 필터 : 토큰의 전후에 위치한 공백 문자를 제거한다. truncate 토큰 필터 : 지정한 길이로 토큰을 자른다.### 3.3.4 token filter 테스트POST /_analyzeHost: localhost:9200Content-Type: application/json{ \"filter\": [ \"lowercase\" ], \"text\": \"Hello, World!\"}3.3.5 내장 애널라이저 애널라이저는 캐릭터 필터 + 토크나이저 + 토큰 필터를 조합하여 구성됨. ES에서는 내장 캐릭터 필터, 토크나이저, 토큰 필터를 조합하여 미리 만들어 놓은 다양한 내장 애널라이저가 있다.내장 애널라이저 종류 standard 애널라이저 : standard 토크나이저와 lowercase 토큰 필터로 구성(별도로 지정하지 않은 경우 기본으로 적용됨) simple 애널라이저 : letter가 아닌 문자 단위로 토큰을 쪼갠 뒤 lowercase 토큰 필터를 적용 whithspace 애널라이저 : whithspace 토크나이저로 구성되고, 공백문자 단위로 토큰을 쪼갠다. stop 애널라이저 : standard와 같지만, 뒤에 stop 토큰 필터를 적용해서 불용어를 제거한다. keyword 애널라이저 : 특별히 분석을 실시하지 않고, 하나의 큰 토큰을 그대로 반환한다. pattern 애널라이저 : pattern 토크나이저와 lowercase 토큰 필터로 구성된다. language 애널라이저 : 여러 언어의 분석을 지원한다.(한국어 지원X)fingerpint 애널라이저 중복 검출에 사용할 수 있는 특별한 핑거프린트용 토큰을 생성 standard 토크나이저 적용 뒤 lowercase 토큰 필터, ASCII folding 토큰 필터, stop 토큰 필터, fingerprint 토큰 필터를 차례대로 적용한다.(stop 토큰 필터는 기본적으로 비활성화) fingerprint 토큰 필터는 토큰을 정렬한 뒤 중복을 제거하고 단일 토큰으로 합쳐버린다.### 3.3.5 내장 애널라이저POST /_analyzeHost: localhost:9200Content-Type: application/json{ \"analyzer\": \"fingerprint\", \"text\": \"Yes yes, Global said this sentence is consistent and.\"}3.3.6 애널라이저를 매핑에 적용 실제 각 필드의 매핑에 애널라이저를 적용하는 방법을 알아보자### 3.3.6 애널라이저를 매핑에 적용PUT /analyzer_testHost: localhost:9200Content-Type: application/json{ \"settings\": { \"analysis\": { \"analyzer\": { \"default\": { \"type\": \"keyword\" } } } }, \"mappings\": { \"properties\": { \"defaultText\":{ \"type\": \"text\", \"analyzer\": \"keyword\" }, \"standardText\": { \"type\": \"text\", \"analyzer\": \"standard\" } } }}3.3.7 커스텀 애널라이저 내장 애널라이저로 목표하는 바를 달성할 수 없다면 커스텀 애널라이저 사용을 고려할 수 있다. 캐릭터 필터, 토크나이저, 토큰 필터를 원하는 대로 조합해 지정한다.### 3.3.7 커스텀 애널라이저PUT /analyzer_test2Host: localhost:9200Content-Type: application/json{ \"settings\": { \"analysis\": { \"char_filter\": { \"my_char_filter\": { \"type\": \"mapping\", \"mappings\": [ \"i. =&gt; 1.\", \"ii. =&gt; 2.\", \"iii. =&gt; 3.\", \"iv. =&gt; 4.\" ] } }, \"analyzer\": { \"my_analyzer\": { \"char_filter\": [ \"my_char_filter\" ], \"tokenizer\": \"whitespace\", \"filter\": [ \"lowercase\" ] } } } }, \"mappings\": { \"properties\": { \"myText\":{ \"type\": \"text\", \"analyzer\": \"my_analyzer\" } } }}my_analyzer 테스트### 3.3.7 커스텀 애널라이저 테스트GET /analyzer_test2/_analyzeHost: localhost:9200Content-Type: application/json{ \"analyzer\": \"my_analyzer\", \"text\": \"i.Hello ii.World iii.Bye, iv.World!\"}{ \"tokens\": [\t{\t \"token\": \"1.hello\",\t \"start_offset\": 0,\t \"end_offset\": 7,\t \"type\": \"word\",\t \"position\": 0\t},\t{\t \"token\": \"2.world\",\t \"start_offset\": 8,\t \"end_offset\": 16,\t \"type\": \"word\",\t \"position\": 1\t},\t{\t \"token\": \"3.bye,\",\t \"start_offset\": 17,\t \"end_offset\": 25,\t \"type\": \"word\",\t \"position\": 2\t},\t{\t \"token\": \"4.world!\",\t \"start_offset\": 26,\t \"end_offset\": 35,\t \"type\": \"word\",\t \"position\": 3\t} ]} 커스텀 캐릭터 필터가 잘 적용된것을 알 수 있다.3.3.8 플러그인 설치를 통한 애널라이저 추가와 한국어 형태소 분석 한국어 형태소 분석을 지원하는 기본 애널라이저는 없다. 하지만 es가 공식 제공하는 nori 플러그인을 설치하면 한국어를 분석할 수 있다. (일본어 : kuromoji, 중국어 : smartcn)ES의 플러그인을 설치하는 방법# bin/elasticsearch-plugin install [플러그인 이름]bin/elasticsearch-plugin install analysis-noribin/elasticsearch-plugin install analysis-kuromojibin/elasticsearch-plugin install analysis-smartcndocker-compose에서 플러그인을 설치하는 방법version: '3.7'services: es01: image: docker.elastic.co/elasticsearch/elasticsearch:8.11.1 container_name: es01 environment: - node.name=es01 - cluster.name=es-docker-cluster - discovery.seed_hosts=es02,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - xpack.security.enabled=false - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 command: &gt; bash -c ' bin/elasticsearch-plugin install analysis-nori exec /usr/local/bin/docker-entrypoint.sh ' volumes: - ./data01:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - internal-network command에 플러그인 설치 명령어 추가해주면 완료 근데 뭔가 잘 안되는듯하다. 별도로 nori plugin을 설치되어있는걸로 별도 이미지 빌드해서 docker에 띄우는게 좋을것 같음 ### 3.3.8 플러그인 설치를 통한 애널라이즈 추가 및 한국어 형태소 분석GET /_analyzeHost: localhost:9200Content-Type: application/json{ \"analyzer\": \"nori\", \"text\": \"우리는 컴퓨터를 다룬다.\"}{ \"tokens\": [\t{\t \"token\": \"우리\",\t \"start_offset\": 0,\t \"end_offset\": 2,\t \"type\": \"word\",\t \"position\": 0\t},\t{\t \"token\": \"컴퓨터\",\t \"start_offset\": 4,\t \"end_offset\": 7,\t \"type\": \"word\",\t \"position\": 2\t},\t{\t \"token\": \"다루\",\t \"start_offset\": 9,\t \"end_offset\": 12,\t \"type\": \"word\",\t \"position\": 4\t} ]} 애널라이저가 조사를 제거했고, “다룬다”의 어간인 “다루”를 제대로 분리했다.(한국어 형태소를 잘 분석한 것)3.3.9 노멀라이저 노멀라이저는 애널라이저와 비슷한 역할을 하나 적용 대상이 text 타입이 아닌 keyword 타입 필드라는 차이가 있다. 또, 애널라이저와는 다르게 단일 토큰을 생성한다. 최종적으로 단일토큰만 생성하므로 ASCII folding, lowercase, uppercawse 등 글자 단위로 작업을 수행하는 필터만 적용 가능하다.### 3.3.9 노멀라이저PUT /normalizer_testHost: localhost:9200Content-Type: application/json{ \"settings\": { \"analysis\": { \"normalizer\": { \"my_normalizer\": { \"type\": \"custom\", \"char_filter\": [], \"filter\": [ \"asciifolding\", \"uppercase\" ] } } } }, \"mappings\": { \"properties\": { \"myNormalizerKeyword\": { \"type\": \"keyword\", \"normalizer\": \"my_normalizer\" }, \"lowercaseKeyword\": { \"type\": \"keyword\", \"normalizer\": \"lowercase\" }, \"defaultKeyword\": { \"type\": \"keyword\" } } }} 위 예제에서는 my_normalizer라는 커승텀 노멀라이저를 만들어서 테스트해본다. keyword 타입에는 특별히 설정하지 않으면 아무런 노멀라이저도 적용되지 않는다.(text타입의 standard 애널라이저 적용과는 다른부분)### 3.3.9 testGET /normalizer_test/_analyzeHost: localhost:9200Content-Type: application/json{ \"field\": \"myNormalizerKeyword\", \"text\": \"Happy World!!\"}### 3.3.9 test2GET /normalizer_test/_analyzeHost: localhost:9200Content-Type: application/json{ \"field\": \"lowercaseKeyword\", \"text\": \"Happy World!!\"}### 3.3.9 test3GET /normalizer_test/_analyzeHost: localhost:9200Content-Type: application/json{ \"field\": \"defaultKeyword\", \"text\": \"Happy World!!\"}3.4 템플릿 인덱스를 생성할 때마다 인덱스 설정과 매핑, 매핑에 지정할 애널라이저 등을 매번 지정해야한다면 수고가 많이 들어간다. 서비스와 데이터 설계에 따라 다를수 있지만, ES를 실무에 적용하다보면 수시로 많은 양의 유사한 구조를 가진 인덱스를 생성해야 할 때가 많다.-이런 경우를 위해 템플릿을 사전에 정의해두고, 인덱스 생성시 사전 정의한 템플릿 설정대로 인덱스를 생성할수 있는 기능을 제공한다.3.4.1 인덱스 템플릿### 3.4.1 인덱스 템플릿### 템플릿 정의PUT /_index_template/my_templateHost: localhost:9200Content-Type: application/json{ \"index_patterns\": [ \"pattern_test_index-*\", \"another_pattern-*\" ], \"priority\": 1, \"template\": { \"settings\": { \"number_of_shards\": 2, \"number_of_replicas\": 2 }, \"mappings\": { \"properties\": { \"myTextField\": { \"type\": \"text\" }\t } } }}### 템플릿 패턴에 일치하는 인덱스 생성PUT /pattern_test_index-1Host: localhost:9200Content-Type: application/json### 템플릿 패턴에 일치하는 인덱스 조회GET /pattern_test_index-1Host: localhost:9200Content-Type: application/json{ \"pattern_test_index-1\": {\t\"aliases\": {},\t\"mappings\": {\t \"properties\": {\t\t\"myTextField\": {\t\t \"type\": \"text\"\t\t}\t }\t},\t\"settings\": {\t \"index\": {\t\t\"routing\": {\t\t \"allocation\": {\t\t\t\"include\": {\t\t\t \"_tier_preference\": \"data_content\"\t\t\t}\t\t }\t\t},\t\t\"number_of_shards\": \"2\",\t\t\"provided_name\": \"pattern_test_index-1\",\t\t\"creation_date\": \"1702113639646\",\t\t\"number_of_replicas\": \"2\",\t\t\"uuid\": \"HFi6GQ9tRAG__CrH5Qf5vQ\",\t\t\"version\": {\t\t \"created\": \"8500003\"\t\t}\t }\t} }}3.4.2 컴포넌트 템플릿 인덱스 템플릿을 많이 만들어 사용하다 보면 템플릿 간 중복되는 부분이 생긴다. 중복되는 부분을 재사용할 수 있는 작은 블록으로 쪼갠 것이 컴포넌트 템플릿이다.### 3.4.2 컴포넌트 템플릿### 컴포넌트 템플릿 정의1PUT /_component_template/timestamp_mappingsHost: localhost:9200Content-Type: application/json{ \"template\": { \"mappings\": { \"properties\": { \"timestamp\": { \"type\": \"date\" } } } }}### 컴포넌트 템플릿 정의2PUT /_component_template/my_shard_settingsHost: localhost:9200Content-Type: application/json{ \"template\": { \"settings\": { \"number_of_shards\": 2, \"number_of_replicas\": 2 } }}컴포넌트 템플릿 사용 인덱스 템플릿을 생성할 때 재사용할 컴포넌트 템플릿 블록을 composed_of 항목에 넣으면 된다.### 컴포넌트 템플릿을 사용한 인덱스 템플릿 생성PUT /_index_template/my_template2Host: localhost:9200Content-Type: application/json{ \"index_patterns\": [ \"timestamp-index-*\" ], \"composed_of\": [\"timestamp_mappings\", \"my_shard_settings\"]}### 인덱스 생성PUT /timestamp-index-001Host: localhost:9200Content-Type: application/json### 인덱스 조회GET /timestamp-index-001Host: localhost:9200Content-Type: application/json{ \"timestamp-index-001\": {\t\"aliases\": {},\t\"mappings\": {\t \"properties\": {\t\t\"timestamp\": {\t\t \"type\": \"date\"\t\t}\t }\t},\t\"settings\": {\t \"index\": {\t\t\"routing\": {\t\t \"allocation\": {\t\t\t\"include\": {\t\t\t \"_tier_preference\": \"data_content\"\t\t\t}\t\t }\t\t},\t\t\"number_of_shards\": \"2\",\t\t\"provided_name\": \"timestamp-index-001\",\t\t\"creation_date\": \"1702114010362\",\t\t\"number_of_replicas\": \"2\",\t\t\"uuid\": \"jrNZeqhlT7KOH1A7GHed3w\",\t\t\"version\": {\t\t \"created\": \"8500003\"\t\t}\t }\t} }} 템플릿을 이용하여 만든 인덱스가 의도대로 잘 만들어진것을 알 수 있다.3.4.3 레거시 템플릿 인덱스 템플릿, 컴포넌트 템플릿 API는 ES 7.8.0부터 추가된 기능이다. 이전 버전에서 사용하던 템플릿 API는 레거시 템플릿이 되었다. 이전 버전의 템플릿 기능은 _index_tempalte 대신에 _template을 사용해서 사용 가능하고, 일반적인 사용 방법은 동일하지만, 레거시 템플릿에서는 컴포넌트 템플릿을 조합할 수 없다는 차이가 있다. index_pattern 적용에서 레거시 템플릿과 충돌이 발생한다면 신규 템플릿을 먼저 매칭하고 이게 없을 경우에만 레거시 템플릿을 확인한다는 점에 유의해야 한다.3.4.4 동적 템플릿 동적 템플릿은 인덱스에 새로 들어온 필드의 매핑을 사전에 정의한대로 동적 생성하는 기능이다. 인덱스 생성할 때나 인덱스 템플릿을 생성할 때 함께 지정한다.### 3.4.4 동적 템플릿PUT /_index_template/dynamic_mapping_templateHost: localhost:9200Content-Type: application/json{ \"index_patterns\": [\"dynamic_mapping*\"], \"priority\": 1, \"template\": { \"settings\": { \"number_of_shards\": 2, \"number_of_replicas\": 2 }, \"mappings\": { \"dynamic_templates\": [ { \"my_text\": { \"match_mapping_type\": \"string\", \"match\": \"*_text\", \"mapping\": { \"type\": \"text\" } } }, { \"my_keyword\": { \"match_mapping_type\": \"string\", \"match\": \"*_keyword\", \"mapping\": { \"type\": \"keyword\" } } } ] } }} 위와 같이 동적 템플릿을 적용하면 필드의 이름을 기준으로 적절히 mapping type을 지정할수 있다.동적 템플릿 적용시 조건 match_mapping_type : 데이터 타입을 JSON 파서를 이용해서 확인한다. 더 큰 범위의 데이터 타입 이름을 사용한다. boolean, double, long, string, object, dat 등을 사용할 수 있다. match / unmatch : 필드의 이름이 지정된 패턴과 일치/불일치하는지 확인한다. path_match / patch_unmatch : match / unmatch와 동일하게 동작하지만 필드 이름으로 마침표를 사용한 전체 경로를 이용한다. ex : my_obejct.name.text* 3.4.5 빌트인 인덱스 템플릿 ES 7.9.0 이상 버전은 미리 정의된 빌트인 인덱스 템플릿을 제공한다. 로그나 메트릭을 편리하게 수집하기 위한 X-Pack 전용 추가 기능 Elastic Agent에서 사용하기 위해 내장된 템플릿이다. metrics-*-*, logs-*-* 인덱스 패턴에 priority 값 100을 가진 템플릿이 사전 정의되어 있다. 이를 적용하지 않고 커스텀 템플릿을 적용하고 싶다면 priority 값이 더 높은것으로 지정하면 된다. 3.5 라우팅 라우팅은 ES가 인덱스를 구성하는 샤드 중 몇 번 샤드를 대상으로 작업을 수행할지 지정하기 위해 사용되는 값이다. 라우팅 값은 문서를 색인할 때 문서마다 하나씩 지정할 수 있다. 작업 대상 샤드 번호는 지정된 라우팅 값을 해시한 후 주 샤드의 개수로 나머지 연산을 수행한 ㄱ밧이 된다. 라우팅 값을 지정하지 않고 문서를 색인하는 경우 라우팅 기본값은 _id 값이 된다. 색인시 라우팅 값을 지정했다면 조회/업데이트/삭제/검색 등의 작업에서도 똑같이 라우팅을 지정해야 한다.### 3.5 라우팅### 인덱스 생성PUT /routing_testHost: localhost:9200Content-Type: application/json{ \"settings\": { \"number_of_shards\": 5, \"number_of_replicas\": 1 }}### 라우팅 색인PUT /routing_test/_doc/1?routing=myidHost: localhost:9200Content-Type: application/json{ \"login_id\": \"myid\", \"comment\": \"hello world\", \"created_at\": \"2020-09-08T22:14:09.123Z\"}### 라우팅 색인2PUT /routing_test/_doc/1?routing=myid2Host: localhost:9200Content-Type: application/json{ \"login_id\": \"myid2\", \"comment\": \"hello world2\", \"created_at\": \"2020-09-09T22:14:09.123Z\"}### 조회(전체 샤드 대상으로 조회가 이루어짐)GET /routing_test/_searchHost: localhost:9200Content-Type: application/json### 조회GET /routing_test/_search?routing=myidHost: localhost:9200Content-Type: application/json### 조회2GET /routing_test/_search?routing=myid2Host: localhost:9200Content-Type: application/json{ \"took\": 6, \"timed_out\": false, \"_shards\": {\t\"total\": 5,\t\"successful\": 5,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 2,\t \"relation\": \"eq\"\t},\t\"max_score\": 1.0,\t\"hits\": [\t {\t\t\"_index\": \"routing_test\",\t\t\"_id\": \"1\",\t\t\"_score\": 1.0,\t\t\"_routing\": \"myid2\",\t\t\"_source\": {\t\t \"login_id\": \"myid2\",\t\t \"comment\": \"hello world2\",\t\t \"created_at\": \"2020-09-09T22:14:09.123Z\"\t\t}\t },\t {\t\t\"_index\": \"routing_test\",\t\t\"_id\": \"1\",\t\t\"_score\": 1.0,\t\t\"_routing\": \"myid\",\t\t\"_source\": {\t\t \"login_id\": \"myid\",\t\t \"comment\": \"hello world\",\t\t \"created_at\": \"2020-09-08T22:14:09.123Z\"\t\t}\t }\t] }} 응답 결과의 shards 부분을 살펴보면 5개의 샤드 전체를 대상으로 검색이 수행된 점을 알 수 있다. hit된 문서의 _routing 메타 필드 안에 우리가 색인 시 지정한 라우팅 값이 myid가 들어가 있음도 확인 할 수 있다. routing 값을 이용해서 조회하는 경우 1개의 샤드만을 대상으로 검색이 수행되었다는 것도 알 수 있다.운영환경시 유의사항 운영환경에서는 문서를 색인하거나 검색할 때 가능한 한 라우팅 값을 지정해 주는 것이 좋다. 지금은 복잡한 쿼리를 지정하지도 않았고, 데이터의 양도 적기 때문에 성능상 큰 차이를 느낄 수가 없지만, 데이터가 많다면 성능차이는 매우 크다.3.5.1 인덱스 내에서의 _id 고유성 보장 전체 샤드를 대상으로 검색하지 않는 상황에서 문서 단건을 조회할 때 라우팅 값을 명시하지 않는 경우에는 원하는 문서를 조회하지 못할 수도 있다. 문서 조회 API는 샤드 하나를 지정하여 조회를 수행한다. 따라서 라우팅 값이 올바르게 명시되지 않는다면 ES는 이미 색인된 문서가 존재하는데도 다른 샤드에서 문서를 조회한 뒤 요청한 문서가 없다는 응답을 반환할 수 있다. 추가로 인덱스 내에서 _id 값의 고유성 검증은 샤드 단위로만 보장이 된다. 색인 /조회/업데이트/삭제 작업이 모두 라우팅 수행 이후의 단일 샤드 내에서 이뤄지기 떄문이다. 라우팅 값이 다르게 되면 한 인덱스 내에서 같은 _id를 가진 문서가 여러개 생길 수 있다는 점에 유의하자.3.5.2 인덱스 매핑에서 라우팅을 필수로 지정하기 실무에서는 ES를 운영하는 주체, 인덱스를 설계하고 생성하는 주체, 데이터를 색인하는 주체, 색인된 데이터를 조회 및 검색하여 서비스에 사용하는 주체가 각각 다를 수 있다. 따라서 담당자들간에 라우팅 지정에 대한 정책을 세우고 인식을 조율할 필요가 있다. 다양한 조직에서 여러 사람들과 협업하면 일관된 정책을 유지하는것은 쉽지 않을수 있다. 이런일을 방지하기 위해 인덱스 매핑에서 _routing 메타 필드를 지정하여 라우팅 값 명시를 필수로 설정할 수 있다.### 3.5.2 인덱스 매핑에서 라우팅을 필수로 지정하기### 인덱스 생성PUT /routing_test2Host: localhost:9200Content-Type: application/json{ \"mappings\": { \"_routing\": { \"required\": true } }}### 인덱스 색인 테스트PUT /routing_test2/_doc/1Host: localhost:9200Content-Type: application/json{ \"comment\": \"index without routing\"} routing_missing_exception이 발생하는것을 확인할 수 있다.Reference ." }, { "title": "[엘라스틱서치 바이블] 3. 인덱스 설계 - 1", "url": "/posts/es-bible-3-1/", "categories": "DevLog, Elasticsearch", "tags": "Elasticsearch", "date": "2023-11-18 18:50:00 +0900", "snippet": "3. 인덱스 설계 - 1 ES의 인덱스는 아주 세부적인 부분까지 설정으로 제어할 수 있다. 이 설정에 따라 동작과 특성이 매우 달라지므로 인덱스 설계에 신경써야 한다. 맵핑, 필드 타입, 애널라이저 등 핵심적인 설정을 학습해 ES 인덱스에 대한 이해도를 높여보자.3.1 인덱스 설정 인덱스 생성시 동작에 관한 설정을 지정할 수 있다.인덱스 설정 ...", "content": "3. 인덱스 설계 - 1 ES의 인덱스는 아주 세부적인 부분까지 설정으로 제어할 수 있다. 이 설정에 따라 동작과 특성이 매우 달라지므로 인덱스 설계에 신경써야 한다. 맵핑, 필드 타입, 애널라이저 등 핵심적인 설정을 학습해 ES 인덱스에 대한 이해도를 높여보자.3.1 인덱스 설정 인덱스 생성시 동작에 관한 설정을 지정할 수 있다.인덱스 설정 조회[GET] [인덱스 이름]/_settingsGET /my_index/_settingsHost: localhost:9200Content-Type: application/json{ \"my_index\": {\t\"settings\": {\t \"index\": {\t\t\"routing\": {\t\t \"allocation\": {\t\t\t\"include\": {\t\t\t \"_tier_preference\": \"data_content\"\t\t\t}\t\t }\t\t},\t\t\"number_of_shards\": \"1\",\t\t\"provided_name\": \"my_index\",\t\t\"creation_date\": \"1700298266901\",\t\t\"number_of_replicas\": \"1\",\t\t\"uuid\": \"O1_nsfp1Rc-f8RPSSUiWUg\",\t\t\"version\": {\t\t \"created\": \"8050299\"\t\t}\t }\t} }}인덱스 설정 변경[PUT] [인덱스 이름]/_settings{ [변경할 내용]}PUT /my_index/_settingsHost: localhost:9200Content-Type: application/json{ \"index.number_of_replicas\": 0}{ \"acknowledged\": true}3.1.1 number_of_shards number_of_shards는 이 인덱스가 데이터를 몇 개의 샤드로 쪼갤 것인지를 지정하는 값이다. 이 값은 매우 신중하게 설계해야 한다. 한번 지정하면 reindex 같은 동작을 통해 인덱스를 통째로 재색인하는 등 특별한 작업을 수행하지 않는 한 바꿀수 없기 떄문이다. 샤드 개수를 어떻게 지정하느냐는 ES 클러스터 전체의 성능에도 큰 영향을 미친다. ES7부터 기본값은 1(이전에는 5였음)주의 사항 샤드 하나마다 루씬 인덱스가 하나씩 더 생성된다는 사실과 주 샤드 하나당 복제본 샤드도 늘어난다는 사실을 염두에 둬야 한다. 샤드 숫자가 너무 많아지면 클러스터 성능이 떨어지고, 색인 성능이 감소한다. 인덱스당 샤드 숫자를 작게 지정하면 샤드 하나의 크기가 커진다. 샤드의 크기가 지나치게 커지면 장애 상황 등에서 샤드 복구에 너무 많은 시간이 소요되고, 클러스터 안정성이 떨어진다. 실제 운영을 하게 된다면 대량 데이터를 담기 위해 이 값을 적절한 값으로 꼭 조정해줘야 한다.3.1.2 number_of_replicas 주 샤드 하나당 복제본 샤드를 몇개 둘 것인지를 지정하는 설정 ES 클러스터에 몇 개의 노드를 붙일 것이며, 어느 정도 고가용성을 제공할 것인지 등을 고려해서 지정하면 된다. 0으로 설정할 경우 복제본 샤드를 생성하지 않고 주 샤드만 두는 설정이다. 복제본 샤드를 생성하지 않는 설정은 주로 대용량 초기 데이터를 마이그레이션 하는 등의 시나리오에서 쓰기 성능을 일시적으로 끌어올리기 위해 사용한다.3.1.3 refresh_interval ES가 해당 인덱스를 대상으로 refresh를 얼마나 자주 수행할 것인지를 지정한다. 색인된 문서는 refresh되어야 하는 검색 대상이 되기 때문에 중요한 설정이다. -1로 값을 지정하면 주기적 refresh를 수행하지 않는다. 설정 조회시 명시적으로 설정돼 있지 않은 경우 1초 defaut로 수행한다.(기본값 설정으로 되돌린다면 null로 업데이트하면된다)PUT /my_index/_settingsHost: localhost:9200Content-Type: application/json{ \"index.refresh_interval\": \"1s\"}3.1.4 인덱스 설정을 지정하여 인덱스 생성 별도의 설정 없이 인덱스를 생성하는 경우 기본 설정으로 생성되는데, 설정을 포함하여 인덱스를 생성할 수 있다.PUT [인덱스 이름]{ \"settings\": { [인덱스 설정] } }}PUT /my_index2Host: localhost:9200Content-Type: application/json{ \"settings\": { \"index.number_of_shards\": 2, \"index.number_of_replicas\": 2 }}{ \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"my_index2\"} acknowledged 는 해당 인덱스가 클러스터에 제대로 생성되었는지 여부를 나타낸다. shards_acknowledged 값은 타임아웃이 떨어지기 전에 지정한 개수만큼 샤드가 활성화되었는지를 나타낸다. wait_for_active_shareds 인자로 해당 개수를 지정할 수 있다. 기본적으로는 1개의 샤드, 즉 주샤드가 시간 안에 활용화되면 true를 반환한다. 3.1.5 인덱스 삭제DELETE [인덱스 이름]DELETE /my_index2Host: localhost:92003.1.6 인덱스 설정 재대로 적용되었는지 조회GET /my_index2Host: localhost:9200{ \"my_index2\": {\t\"aliases\": {},\t\"mappings\": {},\t\"settings\": {\t \"index\": {\t\t\"routing\": {\t\t \"allocation\": {\t\t\t\"include\": {\t\t\t \"_tier_preference\": \"data_content\"\t\t\t}\t\t }\t\t},\t\t\"number_of_shards\": \"2\",\t\t\"provided_name\": \"my_index2\",\t\t\"creation_date\": \"1700304795832\",\t\t\"number_of_replicas\": \"2\",\t\t\"uuid\": \"l2KPoucXQbOwxk1W0oto4A\",\t\t\"version\": {\t\t \"created\": \"8050299\"\t\t}\t }\t} }}3.2 매핑과 필드 타입 매핑은 문서가 인덱스에 어떻게 색인되고 저장되는지 정의하는 부분이다. JSON 문서의 각 필드를 어떤방식으로 분석하고 색인할지, 어떤 타입으로 저장할지 등을 세부적으로 지정할 수 있다.### 데이터 생성PUT /my_index2/_doc/1Host: localhost:9200Content-Type: application/json{ \"title\": \"Hello world\", \"views\": 1234, \"public\": true, \"point\": 4.5, \"created\" : \"2023-11-18T20:00:00.000Z\"}### 인덱스 조회GET /my_index2Host: localhost:9200Content-Type: application/json{ \"my_index2\": {\t\"aliases\": {},\t\"mappings\": {\t \"properties\": {\t\t\"created\": {\t\t \"type\": \"date\"\t\t},\t\t\"point\": {\t\t \"type\": \"float\"\t\t},\t\t\"public\": {\t\t \"type\": \"boolean\"\t\t},\t\t\"title\": {\t\t \"type\": \"text\",\t\t \"fields\": {\t\t\t\"keyword\": {\t\t\t \"type\": \"keyword\",\t\t\t \"ignore_above\": 256\t\t\t}\t\t }\t\t},\t\t\"views\": {\t\t \"type\": \"long\"\t\t}\t }\t},\t\"settings\": {\t} }} 이전에 없던 mappings 항목 밑에 각 필드의 타입과 관련된 정보가 새로 생긴 것을 확인할 수 있다.3.2.1 동적 매핑 vs. 명시적 매핑 ES가 자동으로 생성하는 매핑을 동적 매핑(Dynamic Mapping)이라고 부른다. 반대로 사용자가 직접 맵핑을 지정해주는 방법을 명시적 맵핑(Explicit Mapping)이라고 부른다.PUT /mapping_testHost: localhost:9200Content-Type: application/json{ \"mappings\": { \"properties\": { \"createdDate\":{ \"type\": \"date\", \"format\": \"strict_date_time || epoch_millis\" }, \"keywordString\": { \"type\": \"keyword\" }, \"textString\": { \"type\": \"text\" } } }, \"settings\": { \"index.number_of_shards\": 1, \"index.number_of_replicas\": 1 }} 각 필드에 데이터를 어떠한 형태로 저장할 것인지 타입이라는 설정으로 지정했다. 중요한 것은 필드 타입을 포함한 매핑 설정 내 대부분의 내용은 한 번 지정되면 사실상 변경이 불가능하므로 서비스 설계와 데이터 설계를 할때는 매우 신중하게 해야 한다. 위와 같은 이유로 서비스 운영 환경에서 대용량의 데이터를 처리해야 할 때는 기본적으로 명시된 매핑을 지정해서 인덱스를 운영해야 한다. 이 매핑을 어떻게 하느냐에 따라 서비스 운영 양상이 많이 달라지고, 성능의 차이도 크다. 맵핑 신규 필드 추가 신규 필드 추가가 예정돼 있다면 동적 매핑에 기대지 말고 명시적으로 매핑을 지정하는것이 좋다. 다음과 같이 신규 필드 매핑 정보를 추가할 수 있다.### 3.2.1 맵핑 신규 필드 추가PUT /mapping_test/_mappingHost: localhost:9200Content-Type: application/json{ \"properties\": { \"longValue\": { \"type\": \"long\" } }}맵핑 조회### 3.2.1 맵핑 조회GET /mapping_test/_mappingHost: localhost:9200{ \"mapping_test\": {\t\"mappings\": {\t \"properties\": {\t\t\"createdDate\": {\t\t \"type\": \"date\",\t\t \"format\": \"strict_date_time || epoch_millis\"\t\t},\t\t\"keywordString\": {\t\t \"type\": \"keyword\"\t\t},\t\t\"longValue\": {\t\t \"type\": \"long\"\t\t},\t\t\"textString\": {\t\t \"type\": \"text\"\t\t}\t }\t} }}3.2.2 필드 타입 매핑의 기본은 필드 타입이다. 매핑의 필드 타입은 한번 지정되면 변경이 불가능하므로 매우 신중하게 지정해야 한다.심플 타입 text, keyword, date, long, double, boolean, ip 등 주로 직관적으로 알기 쉬운 간단한 자료형이다.숫자 타입 ES에서 지원하는 숫자 타입은 다음과 같다. long, integer, short, byte, double, float, half_float, scaled_float 작은 비트를 사용하는 자료형을 고르면 색인과 검색시 이득이 있다. 다만 저장할 때는 실제 값에 맞춰 최적화되기 때문에 디스크 사용량에는 이득이 없다. 부동소수점 수를 담는 필드라면 일반적으로 scaled_float 타입을 고려할 수 있다. 고정 환산 계수로 스케일링하고, long으로 저장되는데, 이를 사용하는 경우 정확도에서 손해를 보는 만큼 디스크를 절약할수 있다. date 타입\"createdDate\":{ \"type\": \"date\", \"format\": \"strict_date_time || epoch_millis\"} 필드 맵핑에 정의한 내용을 다시 살펴보면 2가지 format을 지원하므로 2가지를 모두 저장할 수 있다.### 3.2.2 dateTypePUT /mapping_test/_doc/1Host: localhost:9200Content-Type: application/json{ \"createdDate\": \"2020-09-03T02:41:32.001Z\"}### 3.2.2 dateType2PUT /mapping_test/_doc/2Host: localhost:9200Content-Type: application/json{ \"createdDate\": 1599068514123} 위처럼 2개의 문서를 모두 추가 가능하다.GET /mapping_test/_searchHost: localhost:9200Content-Type: application/json{ \"query\": { \"range\": { \"createdDate\": { \"gte\": \"2020-09-02T17:00:00.000Z\", \"lte\": \"2020-09-03T03:00:00.000Z\" } } }} range을 이용해서 조회하면 다음과 같은 결과를 얻을 수 있다.{ \"took\": 354, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 2,\t \"relation\": \"eq\"\t},\t\"max_score\": 1.0,\t\"hits\": [\t {\t\t\"_index\": \"mapping_test\",\t\t\"_id\": \"1\",\t\t\"_score\": 1.0,\t\t\"_source\": {\t\t \"createdDate\": \"2020-09-03T02:41:32.001Z\"\t\t}\t },\t {\t\t\"_index\": \"mapping_test\",\t\t\"_id\": \"2\",\t\t\"_score\": 1.0,\t\t\"_source\": {\t\t \"createdDate\": 1599068514123\t\t}\t }\t] }} 같은 필드에 문자열의 형식으로 문서를 색인 요청하든지 epoch milliseconds 형식으로 요청하든지 상관없이 모두 성공적으로 ES에 색인되는것을 확인할 수 있다. format은 여러 형식으로 지정할 수 있으며, 문서가 어떤 형식으로 들어오더라도 ES 내부적으로는 UTC 시간대로 변환하는 과정을 거쳐 epoch millisec 형식의 long 숫자로 색인된다. 위 예제에 정의된 2가지 타입 외에 epoch_second, date_time, date_optional_time, strict_date_optional_time 등이 더 있다.배열 long 타입 필드에는 309라는 단일 숫자 데이터를 넣을수도 있고, [221, 309, 1599208568] 과 같은 배열 데이터도 넣을수 있다.### 인덱스 생성PUT /array_testHost: localhost:9200Content-Type: application/json{ \"mappings\": { \"properties\": { \"longField\": { \"type\": \"long\" }, \"keywordField\": { \"type\": \"keyword\" } } }}### 배열 인덱스 doc 추가PUT /array_test/_doc/1Host: localhost:9200Content-Type: application/json{ \"longField\": 309, \"keywordField\": [\"hello\", \"world\"]}### 배열 인덱스 doc 추가2PUT /array_test/_doc/2Host: localhost:9200Content-Type: application/json{ \"longField\": [221, 309, 1599208568], \"keywordField\": \"hello\"} 문서들을 색인해보면 모두 성공적으로 색인이 되는것을 알수 있다.실패하는 케이스( 타입을 다르게 하는 경우)### 3.2.2 배열 인덱스 doc 추가 실패PUT /array_test/_doc/2Host: localhost:9200Content-Type: application/json{ \"longField\": [221, \"hello\"]}term 을 통해 문서 내 지정한 필드의 값 질의### 3.2.2 배열 인덱스 조회GET /array_test/_searchHost: localhost:9200Content-Type: application/json{ \"query\": { \"term\": { \"longField\": 309 } }}{ \"took\": 3, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 2,\t \"relation\": \"eq\"\t},\t\"max_score\": 1.0,\t\"hits\": [\t {\t\t\"_index\": \"array_test\",\t\t\"_id\": \"1\",\t\t\"_score\": 1.0,\t\t\"_source\": {\t\t \"longField\": 309,\t\t \"keywordField\": [\t\t\t\"hello\",\t\t\t\"world\"\t\t ]\t\t}\t },\t {\t\t\"_index\": \"array_test\",\t\t\"_id\": \"2\",\t\t\"_score\": 1.0,\t\t\"_source\": {\t\t \"longField\": [\t\t\t221,\t\t\t309,\t\t\t1599208568\t\t ],\t\t \"keywordField\": \"hello\"\t\t}\t }\t] }} 조회 결과를 보면 단일데이터, 배열데이터에 관계 없이 모두 검색 결과에 포함된것을 알수 있는데, 이는 ES 색인 과정에서 데이터가 각 값마다 하나의 독립적인 역색인을 구성하기 떄문이다.계층 구조를 지원하는 타입 필드 하위에 다른 필드가 들어가는 계층 구조의 데이터를 담는 타입으로 ojbect와 nested가 있다. 위 2가지가 유사하지만, 배열을 처리할 때는 동작이 다르다.object 타입 JSON 문서는 필드의 하위에 다른 필드를 여럿 포함하는 객체 데이터를 담을수 있다. object 타입은 이러한 형태의 데이터를 담는 필드 타입이다.### object 타입 인덱스 doc 추가PUT /object_test/_doc/1Host: localhost:9200Content-Type: application/json{ \"price\": 27770.75, \"spec\": { \"cores\":12, \"memory\": 128, \"storage\": 8000 }}### object 타입 인덱스 mapping 조회GET /object_test/_mappingsHost: localhost:9200Content-Type: application/json{ \"object_test\": {\t\"mappings\": {\t \"properties\": {\t\t\"price\": {\t\t \"type\": \"float\"\t\t},\t\t\"spec\": {\t\t \"properties\": {\t\t\t\"cores\": {\t\t\t \"type\": \"long\"\t\t\t},\t\t\t\"memory\": {\t\t\t \"type\": \"long\"\t\t\t},\t\t\t\"storage\": {\t\t\t \"type\": \"long\"\t\t\t}\t\t }\t\t}\t }\t} }} 응답을 살펴보면 spec 필드의 타입을 명시적으로 ojbejct라고 표현하지는 않았는데, 이는 기본값이 object 타입이기 떄문이다.object와 nested 타입의 배열 처리시 다른 동작 확인### object 타입 배열로 색인PUT /object_test/_doc/2Host: localhost:9200Content-Type: application/json{ \"spec\": [ { \"cores\":12, \"memory\": 128, \"storage\": 8000 }, { \"cores\":6, \"memory\": 64, \"storage\": 8000 }, { \"cores\":6, \"memory\": 32, \"storage\": 4000 } ]}### object 타입 배열에 대한 색인 확인GET /object_test/_searchHost: localhost:9200Content-Type: application/json{ \"query\": { \"bool\": { \"must\": [ { \"term\": { \"spec.cores\": \"6\" } }, { \"term\": { \"spec.memory\": \"128\" } } ] } }} bool 쿼리의 must 절에 쿼리를 여러 개 넣을 경우 각 쿼리가 AND조건으로 연결된다. 위 조건대로라면 이 두 조건을 동시에 만족하는 객체는 존재하지 않으므로 예상대로라면 검색 결과는 비어야 한다. 하지만, 결과는 다르다. { \"took\": 970, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 1,\t \"relation\": \"eq\"\t},\t\"max_score\": 2.0,\t\"hits\": [\t {\t\t\"_index\": \"object_test\",\t\t\"_id\": \"2\",\t\t\"_score\": 2.0,\t\t\"_source\": {\t\t \"spec\": [\t\t\t{\t\t\t \"cores\": 12,\t\t\t \"memory\": 128,\t\t\t \"storage\": 8000\t\t\t},\t\t\t{\t\t\t \"cores\": 6,\t\t\t \"memory\": 64,\t\t\t \"storage\": 8000\t\t\t},\t\t\t{\t\t\t \"cores\": 6,\t\t\t \"memory\": 32,\t\t\t \"storage\": 4000\t\t\t}\t\t ]\t\t}\t }\t] }} 위 검색 결과는 object 타입의 데이터가 어떻게 평탄화되는지를 상기해보면 이해할 수 있다. 위 문서는 내부적으로 다음과 같은 형태로 데이터 평탄화가 된다.{ \"spec.cores\": [12, 6, 6], \"spec.memory\": [128, 64, 32], \"spec.cores\": [8000, 8000, 4000]} object 타입의 배열은 배열을 구성하는 객체 데이터를 서로 독립적인 데이터로 취급하지 않는다. 만약 이들을 독립적인 데이터로 취급하기를 원한다면 nested 타입을 사용해야 한다.nested 타입 nested 타입은 ojbect 타입과느 ㄴ다르게 배열 내 각 객체를 독립적으로 취급한다.### nested type index 추가PUT /nested_testHost: localhost:9200Content-Type: application/json{ \"mappings\": { \"properties\": { \"spec\":{ \"type\": \"nested\", \"properties\": { \"cores\": { \"type\": \"long\" }, \"memory\": { \"type\": \"long\" }, \"storage\": { \"type\": \"long\" } } } } }}### nested type doc 추가PUT /nested_test/_doc/1Host: localhost:9200Content-Type: application/json{ \"spec\": [ { \"cores\":12, \"memory\": 128, \"storage\": 8000 }, { \"cores\":6, \"memory\": 64, \"storage\": 8000 }, { \"cores\":6, \"memory\": 32, \"storage\": 4000 } ]}### nested 타입 searchGET /nested_test/_searchHost: localhost:9200Content-Type: application/json{ \"query\": { \"bool\": { \"must\": [ { \"term\": { \"spec.cores\": \"6\" } }, { \"term\": { \"spec.memory\": \"128\" } } ] } }}{ \"took\": 352, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 0,\t \"relation\": \"eq\"\t},\t\"max_score\": null,\t\"hits\": [] }} object 타입과 동일한 검색쿼리로 조회했지만 결과는 나오지 않는다. nested 타입은 객체 배열의 각 객체를 내부적으로 별도의 루씬 문서로 분리해 저장한다. 만약 배열의 원소가 100개라면 부모 문서까지 해서 101개의 문서가 내부적으로 생성된다. 이런 nested의 동작은 ES 내에서 굉장히 특수하기 때문에 nested 쿼리라는 전용 쿼리를 이용해 검색해야 한다. ### nested 전용 쿼리 searchGET /nested_test/_searchHost: localhost:9200Content-Type: application/json{ \"query\": { \"nested\": { \"path\": \"spec\", \"query\": { \"bool\": { \"must\": [ { \"term\": { \"spec.cores\": \"6\" } }, { \"term\": { \"spec.memory\": \"64\" } } ] } } } }}{ \"took\": 4, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 1,\t \"relation\": \"eq\"\t},\t\"max_score\": 2.0,\t\"hits\": [\t {\t\t\"_index\": \"nested_test\",\t\t\"_id\": \"1\",\t\t\"_score\": 2.0,\t\t\"_source\": {\t\t \"spec\": [\t\t\t{\t\t\t \"cores\": 12,\t\t\t \"memory\": 128,\t\t\t \"storage\": 8000\t\t\t},\t\t\t{\t\t\t \"cores\": 6,\t\t\t \"memory\": 64,\t\t\t \"storage\": 8000\t\t\t},\t\t\t{\t\t\t \"cores\": 6,\t\t\t \"memory\": 32,\t\t\t \"storage\": 4000\t\t\t}\t\t ]\t\t}\t }\t] }} spec.memory 값을 64로 지정하면 검색에 걸리지만, 128로 지정하면 걸리지 않는다.nested 타입의 성능 문제 nested 타입은 내부적으로 각 객체를 별도의 문서로 분리해서 저장하기 때문에 성능 문제가 있을수 있다. ES에서는 무분별한 nested 타입의 사용을 막기 위해 인덱스 설정으로 2가지 제한을 걸어 놓았다. index.mapping.netsed_fields.limit : nested 타입을 몇개까지 지어할수 있는지 제한(default 50) index.mapping._nested_objects.limit : 한 문서가 nested 객체를 몇 개 까지 가질수 있는지 제한(default 10,000) 위 값들을 무리하게 높이면 OOM의 위험이 있음. object vs nested 간단 비교 타입 object nested 용도 일반적인 계층 구조에 사용 배열 내 각 객체를 독립적으로 취급해야 하는 특수한 상황에서 사용 성능 상대적으로 가벼움. 상대적으로 무겁다. 내부적으로 숨겨진 문서가 생성됨 검색 일반적인 쿼리를 사용 전용 nested 쿼리로 감싸서 사용해야 한다. 그 외 타입 ES 인덱스의 필드 탕비에는 이 외에도 다양한 비즈니스 요구사항을 위한 타입들이 있다. 그 중에 몇가지만 소개하자면 다음과 같다. 종류 설명 geo_point 위도와 경도를 저장 geo_shape 지도상에 특정 지점이나 선, 도형 등을 표현하는 타입 binary base64로 인코딩된 문자열을 저장하는 타입 long_rage, date_range, ip_range 등 경곗값을 지정하는 방법 등을 통해 수, 날짜, IP 등의 범위를 저장 completion 자동완성 검색을 위한 특수한 타입 text 타입과 keyword 타입 문자열 자료형을 담는 필드는 text와 keyword 타입 중 하나를 선택해 적용할 수 있다.text text로 지정된 필드 값은 애널라이저가 적용된 후 색인된다. 이는 문자열 값 그대로를 가지고 역색인을 구성하는 것이 아니라 값을 분석하여 여러 토큰으로 쪼개고, 토큰으로 역색인을 구성한다. term : 쪼개진 토큰에 지정한 필터를 적용하는 등의 후처리 작업 후 역색인이 들어가는 형태를 말한다.keywrod keyword로 지정된 필드에 들어온 문자열 값은 토큰으로 쪼개지 않고, 역색인을 구성한다. 애널라이저가로 분석하는 대신 노멀라이저를 적용한다. 노멀라이저는 간단한 전처리만을 거친 뒤 커다란 단일 term으로 역색인을 구성한다.text와 keyword의 차이점### doc 추가PUT /mapping_test/_doc/3Host: localhost:9200Content-Type: application/json{ \"keywordString\" : \"Hello, World!\", \"textString\" : \"Hello, World!\"}### textString 검색GET /mapping_test/_searchHost: localhost:9200Content-Type: application/json{ \"query\": { \"match\": { \"textString\": \"hello\" } }}{ \"took\": 3, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 1,\t \"relation\": \"eq\"\t},\t\"max_score\": 0.18232156,\t\"hits\": [\t {\t\t\"_index\": \"mapping_test\",\t\t\"_id\": \"3\",\t\t\"_score\": 0.18232156,\t\t\"_source\": {\t\t \"keywordString\": \"Hello, World!\",\t\t \"textString\": \"Hello, World!\"\t\t}\t }\t] }} textString으로 조회시 추가한 문서가 잘 조회되는것을 알수 있다.### keywordString 검색GET /mapping_test/_searchHost: localhost:9200Content-Type: application/json{ \"query\": { \"match\": { \"keywordString\": \"hello\" } }}{ \"took\": 1, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 0,\t \"relation\": \"eq\"\t},\t\"max_score\": null,\t\"hits\": [] }} 반면, keywordString으로 검색시에는 조회되지 않는것을 확인할 수 있다. 위 설명처럼 keywordString으로 문서를 찾고싶다면 정확히 일치하는 문자열로 검색을 하면 정상적으로 결과를 얻을수 있다. text 타입과 keyword 타입은 색인 과정에서도 위에 보이는것처럼 각각 애널라이저, 노멀라이저에 의해 색인이 이루어진다.### textString match 검색GET /mapping_test/_searchHost: localhost:9200Content-Type: application/json{ \"query\": { \"match\": { \"textString\": \"THE WORLD SAID HELLO\" } }} 추가로 text 타입의 match 질의 과정을 살펴보면 match 질의 질의어도 쪼개 생성한 4개의 텀을 역색인에서 찾는다. 위 예제에서는 world 텀과 hello 텀을 역색인에서 찾을 수 있으므로 검색 결과에 색인한 문서가 조회된다. 위 색인 방식에서 알수 있듯이 text 타입은 주로 전문 검색에 적합하고, keyworc 타입은 일치 검색에 적합하다. 이 두 타입은 정렬과 집계, 스크립트 작업을 수행할 때에도 동작의 차이가 있다. 정렬과 집계, 스크립트 작업의 대상이 될 필드는 text 타입보다는 keyword 타입을 쓰는 편이 낫다. keyword 타입은 doc_values라는 캐시를 사용하고, text 타입은 fielddata라는 캐시를 사용하기 때문이다. 이와 관련된 내용은 이어서 설명한다. 3.2.3 doc_values ES의 검색은 역색인을 기반으로 한 색인을 이용한다.(텀을 보고 역색인에서 문서를 찾는 방식) 정렬, 집계, 스크립트 작업시에는 접근법이 다르다. doc_values는 디스크를 기반으로 한 자료구조로 파일 시스템 캐시를 통해 효율적으로 정렬, 집계, 스크립트 작업을 수행할수 있도록 설계되었다. ES에서는 text와 annotated_text 타입을 제외한 거의 모든 필드 타입이 doc_values를 지원한다. doc_valuesㄱ를 끄고싶다면 맵핑 지정시 아래와 같인 속성값을 false로 할 수 있다.(doc_values가 지원되는 타입의 기본값은 true)### doc_values mappaingPUT /mapping_test/_mappingHost: localhost:9200Content-Type: application/json{ \"properties\": { \"notForSort\":{ \"type\": \"keyword\", \"doc_values\": false } }}3.2.4 fielddata text 타입의 경우 파일 시스템 기반의 캐시인 doc_values를 사용할 수 없다. text 필드를 대상을 정렬, 집계, 스크립트 작업을 수행할 때에는 fielddata라는 캐시를 이용한다. fielddata를 사용한 정렬이나 집계 등의 작업시에는 역색인 전체를 읽어들여 힙 메모리에 올려서 처리한다. 이런 동작 방식은 힙을 순식간에 차지해 OOM 등 많은 문제가 발생할 수 있어 기본값은 비활성화 상태이다. 특별한 이유가 있는 경우 이를 활성화해야 한다면 다음과 같이 mapping properties 를 지정해줄수 있다.### fielddata mappaingPUT /mapping_test/_mappingHost: localhost:9200Content-Type: application/json{ \"properties\": { \"sortableText\":{ \"type\": \"text\", \"fielddata\": true } }}fielddata를 사용하는 경우 주의사항 text 필드의 fielddata를 활성화 하는 것은 매우 신중해야 한다. “Hello, World!”의 역색인 텀은 이미 분석이 완료된 “hello”와 “world” 텀인데, 이를 이용해서 집계를 수행하기 떄문에 의도와 다른 결과가 나올수 있다. 무거운 작업으로 OOM이 발생하면 클러스터 전체에 장애를 불어올수도 있다.doc_values vs fielddata   doc_values fielddata 적용 타입 text, annotated_text를 제외한 모든 타입 text, annotated_text 동작 방식 디스크 기반이며 파일 시스템 캐시를 활용한다. 메모리에 역색인 내용 전체를 올린다.(OOM 유의) 기본값 기본적으로 활성화 기본적으로 비활성화 text vs keyword   text keyword 분석 애널라이저로 분석하여 여러 토큰으로 쪼개진 텀을 역색인 노멀라이저로 전처리한 단일 텀을 역색인 검색 전문 검색에 적합 단순 완전 일치 검색에 적합 정렬, 집계, 스크립트 fielddata를 사용하므로 적합하지 않음. doc_values를 사용하므로 적합함. 3.2.5 _source _source 필드는 문서 색인 시점에 ES에 전달된 원본 JSON 문서를 저장하는 메타데이터 필드 _source 필드는 JSON 문서를 통째로 담기 때문에 디스크를 많이 사용한다._source 비활성화 아래처럼 mappings에 설정을 통해 메타 데이터를 저장하지 않도록 설정할 수 있다.### _sourcePUT /no_source_testHost: localhost:9200Content-Type: application/json{ \"mappings\": { \"_source\": { \"enabled\": false } }}_source 비활성화할 경우 발생할수 있는 문제 update와 updated_by_query API를 이용할 수 없다. ES의 소개에서 이야기한것처럼 ES 세그먼트는 불변인데, 기존 문서를 삭제하고 업데이트된 새 문서를 색인하는 과정에서 _source를 참조해야 하는데 update를 이용할 수 없다. reindex API를 사용할 수 없다. reindex는 원본 인덱스에서 내용을 대상 인덱스에 새로 색인하는 작업인데, 이떄에도 _source의 원본 JSON 데이터를 읽어 재색인 작업을 수행한다. reindex는 ES 운영과 데이터 관리에 있어 핵심 API이다. reindex를 못하는게 _source를 비활성화 할수 없는 충분한 이유가 된다. ES major 버전업시 문제가 될수 있다. 일반적으로 major version이 2 이상 차이나는 경우 읽을수 없음 major 버전 업그레이드 전 reindex를 수행해 예쩐에 생성된 인덱스를 재생성해야 한다. 인덱스 코덱 변경 다른 성능을 희생하더라도 디스크 공간을 절약해야만 하는 상황이라면 _soruce의 비활성화보다는 차라리 인덱스 데이터의 압축률을 높이는 편이 낫다.### index codec 변경PUT /codec_testHost: localhost:9200Content-Type: application/json{ \"settings\": { \"index\": { \"codec\": \"best_compression\" } }} default (LZ4 압축) / best_compression( DEFLATE 압축) 중 1가지를 선택할 수 있다. 이 설정은 동적으로 변경이 불가능하다.synthetic source ES 8.4 부터 synthetic source 라는 기능이 도입되었다. 이를 사용하는 인덱스는 _source에 JSON 원문을 저장하지 않는데, _source 비활성화와 다른점은 _source를 읽어야 하는 떄가 되면 문서 내 각 필드의 doc_values를 모아 _source를 재조립해 동작하는 점이다. 인덱스의 모든 필드가 doc_values를 사용하는 필드여야 한다는 제약이 있고, source를 읽어야 하는 작업의 성능이 떨어질수 있다. 원문 JSON과 100% 일치하지는 않을수 있음(필드명, 배열내 순서 등) ### synthetic source 지정PUT /synthetic source_testHost: localhost:9200Content-Type: application/json{ \"mappings\": { \"_source\": { \"mode\": \"synthetic\" } }}_source 관련 설정 주의사항 _source와 관련된 설정은 운영에 영향이 매우 크므로 특별한 경우에 한해 심사숙고하여 결정해야 한다.3.2.6 index index 속성은 해당 필드의 역색인을 만들것인지를 지정한다.(기본값 true) false로 설정하면 해당 필드는 역색인이 없기 떄문에 일반적인 검색 대상이 되지 않는다. 역색인을 생성하지 않는 것뿐이기 때문에 doc_values를 사용하는 타입의 필드라면 정렬이나 집계의 대상으로는 사용할 수 있다. ES 8.1 이상부터는 index 속성을 false로 설정하는것만으로 검색 대상에서 제외되지는 않는다. doc_values를 사용하는 필드 타입의 경우 index가 false이더라도 역색인 대신 doc_values를 이용해 검색을 수행한다. 다만, doc_values는 검색을 위해 설계된 자료형이 아니므로 검색 성능이 많이 떨어진다. 데이터 설계나 서비스 설계상 검색 대상이 될 가능성이 없거나 검색 대상이 되더라도 관리적인 목적으로 아주 가끔 소규모의 검색만 필요한 경우 index 값을 false로 지정한 성능&amp;저장공간상 이득을 볼수 있다.### no indexPUT /mapping_test/_mappingHost: localhost:9200Content-Type: application/json{ \"properties\": { \"notSearchableText\": { \"type\": \"text\", \"index\": false }, \"docValueSearchableText\": { \"type\": \"keyword\", \"index\": false } }}### no index doc addPUT /mapping_test/_doc/4Host: localhost:9200Content-Type: application/json{ \"textString\": \"Hello, World!\", \"notSearchableText\": \"World, Hello!\", \"docValueSearchableText\": \"hello\"}### no index search errorGET /mapping_test/_searchHost: localhost:9200Content-Type: application/json{ \"query\": { \"match\": { \"notSearchableText\": \"hello\" } }} 위와 같이 검색하게 되면 index가 없어서 검색이 불가능하다는 응답을 받게 된다.### no index doc_values searchGET /mapping_test/_searchHost: localhost:9200Content-Type: application/json{ \"query\": { \"match\": { \"docValueSearchableText\": \"hello\" } }}{ \"took\": 4, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 1,\t \"relation\": \"eq\"\t},\t\"max_score\": 1.0,\t\"hits\": [\t {\t\t\"_index\": \"mapping_test\",\t\t\"_id\": \"4\",\t\t\"_score\": 1.0,\t\t\"_source\": {\t\t \"textString\": \"Hello, World!\",\t\t \"notSearchableText\": \"World, Hello!\",\t\t \"docValueSearchableText\": \"hello\"\t\t}\t }\t] }} 반면, docValueSearchableText 를 이용하면 index가 없음에도 조회되는것을 알수 있다.3.2.7 enabled enabled 설정은 object 타입의 필드에만 적용된다. enabled가 false로 지정된 필드는 ES가 파싱조차 수행하지 않는다. _source에는 데이터가 있으나, 다른 어느곳에도 저장되지 않는다. 역색인을 생성하기 않으므로 검색도 불가능하다. 정렬이나 집계의 대상도 될수 없다. ### endabled mappingPUT /mapping_test/_mappingHost: localhost:9200Content-Type: application/json{ \"properties\": { \"notEnabled\": { \"type\": \"object\", \"enabled\": false } }}### endabled doc addPUT /mapping_test/_doc/5Host: localhost:9200Content-Type: application/json{ \"notEnabled\": { \"mixedTypeArray\": [ \"hello\", 4, false, {\"foo\": \"bar\"}, null, [2, \"E\"] ] }}### endabled doc add2PUT /mapping_test/_doc/6Host: localhost:9200Content-Type: application/json{ \"notEnabled\": \"world\"} 서비스 설계쌍 최종적으로 데이터를 _source에서 확인만 하면 되고, 그 외 어떤 활용도 필요치 않은 필드가 있다면 enabled를 false로 지정하는것을 고려해볼 수 있다.Reference ." }, { "title": "[엘라스틱서치 바이블] 2. 엘라스틱 서치 기본 동작과 구조", "url": "/posts/es-bible-2/", "categories": "DevLog, Elasticsearch", "tags": "Elasticsearch", "date": "2023-11-17 20:02:00 +0900", "snippet": "2. 엘라스틱 서치 기본 동작과 구조 엘라스틱서치의 구조를 어느정도 먼저 이해한 상태에서 상세 내용을 학습해야 전체 학습에 깊이가 생긴다. 이번 장을 마치고 나면 엘라스틱서치가 어떻게 동작하는지 큰 그림을 머릿속에 그릴수 있을것이다.2.1 엘라스틱서치 기본 동작 빠르게 둘러보기 실습 과정에서는 기본적으로 REST API를 호출해 진행한다. Ki...", "content": "2. 엘라스틱 서치 기본 동작과 구조 엘라스틱서치의 구조를 어느정도 먼저 이해한 상태에서 상세 내용을 학습해야 전체 학습에 깊이가 생긴다. 이번 장을 마치고 나면 엘라스틱서치가 어떻게 동작하는지 큰 그림을 머릿속에 그릴수 있을것이다.2.1 엘라스틱서치 기본 동작 빠르게 둘러보기 실습 과정에서는 기본적으로 REST API를 호출해 진행한다. Kibana에 접속하여 화면 상단의 햄버거 버튼을 클릭해 management 메뉴 아래의 dev tools 하위 메뉴로 들어가면 API의 자동완성도 해주기 떄문에 편리하게 실습할수 있다. 문서에서는 Intellij Http Client을 이용해서 테스트를 진행한다.2.1.1 문서 색인_id를 지정하여 색인PUT [인덱스 이름]/_doc/[_id값]{ [문서 내용]}PUT /my_index/_doc/2Host: localhost:9200Content-Type: application/json{ \"title\": \"Hello world2\", \"views\": 1234, \"public\": true, \"created\" : \"2023-11-18T18:05:00.000Z\"}_id를 지정하지 않고 색인PUT [인덱스 이름]/_doc/{ [문서 내용]}POST /my_index/_docHost: localhost:9200Content-Type: application/json{ \"title\": \"Hello world3\", \"views\": 1234, \"public\": true, \"created\" : \"2023-11-18T18:06:00.000Z\"}2.1.2 문서 조회GET [인덱스 이름]/_doc/[_id값]{ [문서 내용]}GET /my_index/_doc/1Host: localhost:92002.1.3 문서 업데이트POST [인덱스 이름]/_update/[_id값]{ [문서 내용]}POST /my_index/_update/1Host: localhost:9200Content-Type: application/json{ \"doc\": { \"title\": \"Hello world updated\" }}2.1.4 문서 검색GET [인덱스 이름]/_searchPOST [인덱스 이름]/_search 아래 예제는 match 쿼리를 사용한 요청이다.GET /my_index/_searchHost: localhost:9200Content-Type: application/json{ \"query\": { \"match\": { \"title\": \"hello world\" } }}{ \"took\": 45, \"timed_out\": false, \"_shards\": {\t\"total\": 1,\t\"successful\": 1,\t\"skipped\": 0,\t\"failed\": 0 }, \"hits\": {\t\"total\": {\t \"value\": 3,\t \"relation\": \"eq\"\t},\t\"max_score\": 0.7026868,\t\"hits\": [\t {\t\t\"_index\": \"my_index\",\t\t\"_id\": \"1\",\t\t\"_score\": 0.7026868,\t\t\"_source\": {\t\t \"title\": \"Hello world updated\",\t\t \"views\": 1234,\t\t \"public\": true,\t\t \"created\": \"2023-11-18T18:04:00.000Z\"\t\t}\t },\t {\t\t\"_index\": \"my_index\",\t\t\"_id\": \"2\",\t\t\"_score\": 0.110377684,\t\t\"_source\": {\t\t \"title\": \"Hello world2\",\t\t \"views\": 1234,\t\t \"public\": true,\t\t \"created\": \"2023-11-18T18:05:00.000Z\"\t\t}\t },\t {\t\t\"_index\": \"my_index\",\t\t\"_id\": \"ZPyw4YsBcsUuSBAASzRQ\",\t\t\"_score\": 0.110377684,\t\t\"_source\": {\t\t \"title\": \"Hello world3\",\t\t \"views\": 1234,\t\t \"public\": true,\t\t \"created\": \"2023-11-18T18:06:00.000Z\"\t\t}\t }\t] }} title 값이 일치하지 않는 항목도 조회된것을 확인할수 있는데, _score에서 유사도 점수를 확인할 수 있다. 단순히 주어진 텍스트와 매칭되는 문서를 찾는 것이 아니라 문서를 분석해서 역색인을 만들어두고 검색어를 분석해서 둘 사이의 유사도가 높은 문서를 찾을수 있다.2.1.5 문서 삭제DELETE [인덱스 이름]/_doc/[_id값]DELETE /my_index/_doc/1Host: localhost:9200Content-Type: application/jsonResponse{ \"_index\": \"my_index\", \"_id\": \"1\", \"_version\": 3, \"result\": \"deleted\", \"_shards\": {\t\"total\": 2,\t\"successful\": 2,\t\"failed\": 0 }, \"_seq_no\": 4, \"_primary_term\": 1}2.2 엘라스틱서치 구조 개괄 책에 있는 이미지와는 다르지만, 하나의 index의 문서가 여러 node에 복제본을 어떻게 유지하는지에 대한 모습이다.엘라스틱서치 주요 용어 문서(document) : 엘락스틱서치가 저장하고 색인을 생성하는 JSON 문서 인덱스 : 문서를 모아 놓은 단위가 인덱스다. 클라이언트는 이 인덱스 단위로 ES에 검색을 요청하게 된다. 샤드 : 인덱스는 그 내용을 여러 샤드로 분리하여 분산 저장한다. 또한 ES는 고가용성을 제공하기 위해 샤드의 내용을 복제해둔다. 원본 역할을 담당하는 샤드를 주 샤드(primary shard)라고 하고, 복제본을 복제본 샤드(replication shared)라고 한다. _id : 인덱스 내 문서에 부여되는 고유한 구분자다. 인덱스 이름과 _id의 조합은 엘라스틱서치 클러스터 내에서 고유하다. 타입 : ES는 과거에 하나의 인덱스 안에 있는 여러 문서를 묶어서 타입이라는 논리 단위로 나눴다. 현재는 이 개념은 폐기 됐으며 더 이상 사용되지 않는다. 지금은 문서의 묶음을 논리적으로 구분해야 할 필요가 있다면 별도의 인덱스로 독립시켜야 한다. ES6부터 인덱스 하나에 타입 하나만 둘 수 있도록 제한되었다. ES7부터 이와 관련된 api가 deprecated되었고, 타입 이름이 들어가야 하던 자리에는 기본값이 _doc이 들어간 API를 사요해야 한다. 노드 : ES 프로세스 하나가 노드 하나를 구성한다. 노드 하나는 여러 개의 샤드를 가진다. 고용성을 제공하기 위해 같은 종류의 샤드를 같은 노드에 배치하지 않는다. 클러스터 : ES 노드 여러개가 모여 하나의 클러스터를 구성한다. 노드의 역할 : ES 노드는 데이터 노드, 마스터 노드, 조정 노드 등 여러 역할 중 하나 이상의 역할을 맡아 수행한다. 데이터 노드 : 샤드를 보유하고샤드에 실제 읽기와 쓰기 작업을 수행하는 노드이다. 마스터 노드 : 클러스터를 관리하는 중요한 역할 수행, 마스터 노드는 마스터 후보 노드 중에서 1대가 선출된다. 조정 노드 : 클라이언트의 요청을 받아서 데이터 노드에 요청을 분배하고, 클라이언트에게 응답을 돌려주는 노드이다. 2.3 엘라스틱서치 내부 구조와 루씬 ES는 아파치 루씬을 코어 라이브러리로 사용하고 있다. 루씬은 문서를 색인하고 검색하는 라이브러리이다.2.3.1 루씬 flush 문서 색인 요청이 들어오면 루씬은 문서를 분석해서 역색인을 생성한다. 최초 생성 자체는 메모리 버퍼에 들어가고, 색인 &amp; 업데이트 &amp; 삭제 등의 작업이 수행되면 루씬은 이러한 변경들을 메로리에 들고 있다가 주기적으로 디스크에 flush 한다. 루씬은 색인한 정보를 파일로 저장하기 때문에 루씬에서 검색을 하려면 먼저 파일을 열어야 한다. 엘라스틱서치는 내부적으로 루씬의 DirectoryReader라는 클래스를 이용해 파일을 열고 루씬의 색인에 접근할 수 있는 IndexReader 객체를 얻는다. DirectoryReader.openIfChanged를 호출해 변경 사항이 적용된 새 IndexReader를 열어 준 뒤 기존 IndexReader를 안전하게 닫는다. ES에서는 refresh라고 부름. 2.3.2 루씬 commit 루씬의 flush는 시스템의 페이지 캐시에 데이터를 넘겨주는 것까지만 보장할 뿐 디스크에 파일이 실제로 안전하게 기록되는것까지는 보장하지 않는다. 따라서 루씬은 fsync 시스템 콜을 주기적으로 커널 시스템의 페이지 캐시의 내용과 실제로 디스크에 기록된 내용의 싱크를 맞추는 작업을 수행한다. 이를 루씬의 commit이라고 한다. ES의 flush 작업은 이 루씬의 commit을 거친다. 루씬의 flush와 ES의 flush는 다른 개념이므로 혼동하지 말아야 한다. ES flush는 refresh 보다 훨씬 비용이 많이 드는 작업이다. 적절한 주기로 실행됨. 간혹 fsync 중에도 하드웨어 캐시를 사용하는 일부 저장 장치가 있다. 루씬은 그러한 장비를 사용하는 환경에서는 하드웨어 실패가 나는 상황까지 데이터 정합성을 보장하지는 않는다.2.3.3 세그먼트 앞의 작업을 거쳐 디스크에 기록된 파일들이 모이면 세그먼트라는 단위가 된다.(세그먼트가 루씬의 검색 대상) 세그먼트 자체는 불변(immutable)인 데이터로 구성되어 있다. 문서 생성시 새로운 세그먼트를 만들지만, 기존 문서를 삭제하는 경우 삭제 플래그만 표시하고, 업데이트의 경우에도 기존 문서에 삭제 플래그를 표시하고, 새 세그먼트를 생성한다. 루씬의 검색은 모든 세그먼트를 대상으로 수행되는데 불변인 세그먼트 개수를 무작정 늘려갈 수 없기 때문에 중간중간 적당한 세그먼트 병합을 수행한다. 이 과정에서 삭제된 플래그가 표시된 데이터를 삭제하는 작업도 수행된다. 세그먼트 병합은 비용이 비싸지만, 병합을 하고나면 검색 성능 향상을 기대해볼 수 있다. forcemerge API를 통해 명시적으로 세그먼트 병합을 수행할 수도 있다. 더 이상 추가 데이터 색인이 없을것이 보장될떄만 수행하는것을 권장한다. 크기가 매우 큰 세그먼트가 이미 존재하는 상황에서 추가적인 데이터 변경이 일어나면 추가된 작은 세그먼트는 자동 세그먼트 병합 대상 선정에서 영원히 누락될 수 있음. 2.3.4 루씬 인덱스와 엘라스틱서치 인덱스 여러 세그먼트가 모이면 하나의 루씬 인덱스가 된다. ES 샤드는 이 루씬 인덱스 하나를 래핑(warp)한 단위이다. ES 샤드 여러개가 모이면 ES 인덱스가 된다. ES 레벨에서는 여러 샤드에 있는 문서를 모두 검색해볼 수 있다.ES가 하는 주된 역할 새 문서가 들어오면 해당 내용을 라우팅하여 여러 샤드에 분산시켜 저장/색인 이후 클라이언트가 검색요청을 보내면 각 샤드를 대상을 검색을 하고, 그 결과를 모아 병합하여 최종 응답을 생성한다. 이런 구조를 통해 루씬에서 불가능한 분산 검색을 ES레벨에서는 가능하게 만든것이다.2.3.5 translog 엘라스틱서치에 색인된 문서들은 루씬 commit까지 완료되어야 디스크에 안전하게 기록된다. 그렇다고 문서 변경사항이 있을때마다 루씬 commit을 수행하기에는 비용이 많이 든다. 또 변경사항을 모아서 commit한다면, 장애가 발생할 때 미처 commit되지 않은 데이터가 유실될 우려가 있다. ES에서는 위와 같은 문제를 방지하기 위해 translog라는 이름의 작업 로그를 남긴다.translog 과정 translog는 색인, 삭제 작업이 루씬 인덱스에 수행된 직후에 기록된다. translog 기록까지 끝난 이후에야 작업 요청이 성공으로 승인된다.ES 장애복구과정에서 translog 활용 ES에 장애가 발생한 경우 ES 샤드 복구 단계에서 translog를 읽는다. translog 기록은 성공했지만 루씬 commit에 포함되지 못했던 작업 내용이 있다면 샤드 복구 단계에서 복구된다. translog가 너무 커지면 샤드 복구에 시간이 오래 걸리게 되는데, 이르 방지하기 위해 translog의 크기를 적절히 유지해줄 필요가 있다.translog와 ES flush ES flush는 루씬 commit을 수행하고, 새로운 translog를 만드는 작업이다. ES flush가 백그라운드에서 주기적으로 수행되며, translog의 크기를 적절한 수준으로 유지한다.translog fsync 주기 관련 옵션 일반적인 상황에서는 설정하지 않는 옵션이지만, index.translog.sync_interval 옵션을 설정한 경우 특정 주기에 의해서만 translog fsync를 수행할 수 있다. translog fsync 기본 값은 5초인데, 이 사이에 문제가 발생하면 데이터가 유실될 수 있다.Reference ." }, { "title": "[엘라스틱서치 바이블] 1. 엘라스틱 서치 소개", "url": "/posts/es-bible-1/", "categories": "DevLog, Elasticsearch", "tags": "Elasticsearch", "date": "2023-11-16 18:44:00 +0900", "snippet": "1. 엘라스틱 서치 소개 Elasticsearch는 2010년 2월 샤이 베논이 아파치의 루씬 라이브러리를 기반으로 만든 분산 검색 엔진 2013년에 데이터를 손쉽게 시각화하는 Kibana와 색인 데이터 수집&amp;변환을 하는 Logstash 프로젝트에 엘라스틱에 합류 엘라스틱서치는 현재 검색 엔진 분야에서 가장 각광받고 있는 시스템이다. 분산...", "content": "1. 엘라스틱 서치 소개 Elasticsearch는 2010년 2월 샤이 베논이 아파치의 루씬 라이브러리를 기반으로 만든 분산 검색 엔진 2013년에 데이터를 손쉽게 시각화하는 Kibana와 색인 데이터 수집&amp;변환을 하는 Logstash 프로젝트에 엘라스틱에 합류 엘라스틱서치는 현재 검색 엔진 분야에서 가장 각광받고 있는 시스템이다. 분산처리나 고가용성, 수평적 확장성 등 엔터프라이즈 영역에 필요한 주요 기능을 제공하고 있으며, 이러한 요소들의 효용성이 많은 기업 사용자에 의해 다년간 충분히 검증되었다. 엘라스틱서치는 JSON 기반의 문ㅅ를 저장, 색인, 검색하기에 분산 NoSQL 스토리지의 성격을 가지고 있다.1.1 엘라스틱서치의 기본 콘셉트 루씬은 데이터를 색인하고 검색하는 기능을 제공하는 검색엔진의 코어 라이브러리이다.검색 엔진 엘라스틱서치는 기본적으로 검색엔진이다. 다른 RDBMS나 경쟁 NoSQL에 비해 매우 강력한 검색 기능을 지원한다. 단순한 텍스트 매칭 검색이 아닌 본격적인 전문 검색(full-text search)이 가능하고, 다양한 종류의 검색 쿼리를 지원하다. 역색인을 사용하여 검색 속도도 매우 빠르다. 다양한 analzyer)를 조합하여 여러 비즈니스 요구사항에 맞는 색인을 구성할수 있고, 형태소 분석도 가능하다.분산 처리 데이터를 여러 노드에 분산 저장하며 검색이나 집계 작업 등을 수행할 때도 분산 처리를 지원한다.고가용성 제공 클러스터를 구성하고 있는 일부 노드에 장애가 발생해도 복제본 데이터를 이용해 중단 없이 서비스를 지속할 수 있다. 장애 상황에서 엘라스틱 서치는 다시 복제본을 만들어 복제본 개수를 유지하면서 노드 간 데이터의 균형을 자동으로 맞춘다.수평적 확장성 요청 수나 데이터의 양 증가에 따라 더 많은 처리 능력이 요구되는 때가 생기는데, 엘라스틱서치는 이런 상황을 대비하여 수평적 확장을 지원한다.JSON 기반의 REST API 제공 JSON 형태의 문서를 저장, 색인, 검색한다. 특별한 클라이언트 라이브러리 사전 설치 없이 환경에 구애받지 않고 HTTP API를 통해 쉽게 이용 가능하다.데이터 안정성 높은 데이터 안정성을 제공한다. 데이터 색인 요청 후 200 OK를 받았다면 그 데이터는 확실히 디스크에 기록됨을 보장한다.다양한 플러그인을 통한 기능 확장 지원 다양한 플러그인을 사용해 기능을 확장하고 변경할수 있다. 또, 핵심적적인 기능들도 플러그인을 통해 제어할수 있도록 설계됐다.준실시간 검색 엘라스틱서치는 준실시간 검색(near real-time search)을 지원한다. 데이터를 색인하자마자 조회하는 것은 가능하지만, 색인 직후의 검색 요청은 성공하지 못할 가능성이 높다. 기본 설정 운영시 최대 1초 정도 지연시간이 발생한다.트랜잭션이 지원되지 않음. 일반적인 RDBMS와는 다르게 트랜잭션을 지원하지 않는다. 서비스와 데이터 설계시 이 점을 고려해야 한다.사실상 조인을 지원하지 않음. 기본적으로 조인을 염두에 두고 설계되지 않았다. join 이라는 특별한 데이터 타입이 있지만, 이는 굉장히 제한적인 상황을 위한 기능이고, 성능도 떨어진다. 설계시 RDBMS와는 다르게 데이터를 비정규화하고, 가급적 조인을 아예 사용할수 없다고 가정하여 설계를 해야 한다.1.2 라이선스 정책 엘라스틱 라이선스와 SSPL(Server Side Public License)라는 2가지 라이선스 정책을 채택한다. github에 공개되는 소스코드는 사용자가 원하는 라이선스 정책을 선택해 사용할 수 있다. SSPL은 소스코드에 비롯된 파생작업을 만드는 경우 파생 작업물도 SSPL 라이선스로 공개해야 하는 카피레프트 성격의 라이선스이다. (상업 서비스에서 SSPL 라이선스로 소스코드를 이요하려면 사전검토가 필요하다.) 특수한 사정이 있는 경우 7.10 버전 이하의 구버전을 선택하는 방법도 고려해볼만 하다. AWS OpenSearch 배포판을 선택하는 방법도 있다.1.3 실습환경 구축 실습을 위해 elasticsearch와 kibana를 설치해보자. 책에서는 OS 환경별로 직접 app 설치하는 방법으로 정의되어있으나, 편의상 docker-compose를 이용해서 띄우는 방식으로 환경 구축을 진행합니다.version: '3.7'services: es01: image: docker.elastic.co/elasticsearch/elasticsearch:8.5.2 container_name: es01 environment: - node.name=es01 - cluster.name=es-docker-cluster - discovery.seed_hosts=es02,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - xpack.security.enabled=false - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data01:/Users/sungsupark/dev/personal/kotlin-spring-multi-module-starter/db/elasticsearch/starter/data ports: - 9200:9200 networks: - internal-network es02: image: docker.elastic.co/elasticsearch/elasticsearch:8.5.2 container_name: es02 environment: - node.name=es02 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - xpack.security.enabled=false - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data02:/Users/sungsupark/dev/personal/kotlin-spring-multi-module-starter/db/elasticsearch/starter/data networks: - internal-network es03: image: docker.elastic.co/elasticsearch/elasticsearch:8.5.2 container_name: es03 environment: - node.name=es03 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es02 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - xpack.security.enabled=false - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data03:/Users/sungsupark/dev/personal/kotlin-spring-multi-module-starter/db/elasticsearch/starter/data networks: - internal-network kibana: image: docker.elastic.co/kibana/kibana:8.5.2 container_name: kibana ports: - 5601:5601 environment: ELASTICSEARCH_URL: http://es01:9200 ELASTICSEARCH_HOSTS: http://es01:9200 networks: - internal-network depends_on: - es01 cerebro: image: lmenezes/cerebro:0.9.4 container_name: cerebro environment: - CEREBRO_PORT=8080 - ELASTICSEARCH_HOSTS=http://es01:9200 ports: - 8080:8080 networks: - internal-network depends_on: - es01volumes: data01: driver: local data02: driver: local data03: driver: localnetworks: internal-network: driver: bridge 개인 로컬 개발환경에서는 프로젝트 경로에 .gitignore 처리되어있는 ${project}/db/~ 하위로 es 볼륨을 정의했습니다. - xpack.security.enabled=false 이 옵션은 배포 환경에서는 빠져야하는데 es8부터 해당 옵션이 빠지면 es 컨테이너 구동이 되지 않아 추가됨.1.3.1 ES1.3.2 Kibana 키바나는 ES와 함께 엘라스틱에서 배포하고 있는 스택의 일원으로 데이터를 효과적으로 시각화하는 솔루션이다. 샘플 데이터 추가 Home &gt; Try sample data &gt; Other sample data sets &gt; Add data 1.3.3 Cerebro ES 클러스터의 상태 화인과 관리를 도와주는 서드파티 솔루션이다.Reference ES docker-compose" }, { "title": "[스파크 완벽 가이드] 19. 성능 튜닝", "url": "/posts/spark-guide-19/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2023-02-18 18:10:00 +0900", "snippet": "[스파크 완벽 가이드] 19. 성능 튜닝스파크 잡의 최적화 주요 영역 코드 수준의 설계(RRR, DataFrame 중 선택)보관용 데이터조인집계데이터 전송애플리케이션별 속성익스큐터 프로세스의 JVM워커 노드클러스터와 배포 환경 속성19.1 간접적인 성능 향상 기법 하드웨어 개선 같은 방법도 있지만 이부분은 제외합니다.19.1.1 설계 방안 성능 ...", "content": "[스파크 완벽 가이드] 19. 성능 튜닝스파크 잡의 최적화 주요 영역 코드 수준의 설계(RRR, DataFrame 중 선택)보관용 데이터조인집계데이터 전송애플리케이션별 속성익스큐터 프로세스의 JVM워커 노드클러스터와 배포 환경 속성19.1 간접적인 성능 향상 기법 하드웨어 개선 같은 방법도 있지만 이부분은 제외합니다.19.1.1 설계 방안 성능 최적화를 위해서는 좋은 설계 방안 필요합니다.스칼라 vs 자바 vs 파이썬 vs R 언어 선택에 대한 정답은 없음. 가장 편안하게 사용하는 언어나 상황에 따라 선택하는 것이 좋음. 구조적 API로 만들 수 없는 사용자 정의 트랜스포메이션을 사용해야 한다면 R과 파이썬은 사용하지 않는 것이 좋음. 주 언어가 파이썬이었다고 했을때, 필요한 영역에만 스칼라로 정의해 사용하는 방법이 있다. DataFrame vs SQL vs Dataset vs RDD 모든 언어에서 DataFrame, Dataset, SQL의 속도는 동일하다. DataFrame을 어떤 언어에서 사용하더라도 성능 차이가 없음. 단, 파이썬이나 R에서 UDF(사용자 정의 함수)를 정의한다면 성능 저하가 발생할 가능성이 있으므로 이때는 자바나 스칼라를 사용해 UDF를 정의하는 것이 좋음. 근본적으로 성능을 개선하고 싶다면 UDF 대신 DataFrame이나 SQL을 사용해야 한다. 이렇게 만들어진 코드는 결과적으로는 RDD로 컴파일되는데, 이때 보통 사용자가 직접 RDD 코드를 작성하는 것보다 스파크 최적화 엔진으로 만들어진 코드가 더 나은 경우가 대부분임. 또, 사용자가 저수준 API를 사용하는것보다 훨씬 더 쉽게 사용할수 있음. RDD를 사용하고자 한다면 스칼라나 자바를 이용해서 애플리케이션을 개발하는것이 좋음.(그게 안될 경우 사용 영역을 최소화) 파이썬에서 RDD를 사용한다고 했을때 큰 데이터 직렬화 비용, 안정성 문제가 있음. 19.1.2 RDD 객체 직렬화 Kryo 직렬화가 Java 직렬화보다 효율적이나 애플리케이션에 사용할 클래스 등록해야 하는 번거로움이 있음.19.1.3 클러스터 설정동적 할당 워크로드에 따라 애플리케이션이 차지할 자원을 동적으로 조절하는 매커니즘을 제공. spark.dynamicAllocation.enabled=true로 변경 가능 19.1.4 스케줄링 스케줄러 풀, 동적 할당, max-executor-cross 설정과 같이 병렬 실행 관련된 최적화 방법들이 있음. 스케줄링 최적화는 실제로 설정값을 바꿔가면서 테스트해서 최적화 지점을 찾는것이 중요합니다.19.1.5 보관용 데이터 자주 발생하지 않지만, 여러 가지 분석을 수행하기 위해 동일한 데이터셋을 여러 번 읽는 경우가 있을수 있음. 적절한 저장소, 데이터 포맷을 선택하고, 파티셔닝을 활용해야 한다.파일 기반 장기 데이터 저장소 csv파일, 바이너리 blob 파일, Parquet(파케이) 다양한 포맷 제공합니다. 데이터 저장시 모범 사례를 따르는게 가장 쉬운 최적화 방법이다. 데이터를 바이너리로 저장하려면 구조적 API를 사용하는것이 좋음. CSV같은 파일은 구조화되어 있는것처럼 보이지만 실제로 파싱 속도가 느리고 예외 케이스가 자주 발생함. 분할 가능한 파일 포맷과 압축 분할 가능한 포맷인지 확인 해야 한다. 여러 태스크가 파일의 서로 다른 부분을 동시에 읽을 수 있어야 함. JSON 파일처럼 분할 불가능한 포맷을 사용하면 단일 머신에서 전체 파일을 읽어야 하므로 병렬성이 떨어짐. 압축 포맷도 분할할 수 없어서 병렬로 읽을수 없음.(10 코어 장비에서 단일 코어만을 사용해서 데이터를 읽어야 함) 테이블 파티셔닝 데이터의 날짜 필드 같은 키를 기준으로 개별 디렉터리에 파일을 저장하는것을 의미(아파치 하이브 같은) 키를 기준으로 데이터가 분할되어있다면, 특정 범위의 데이터를 조회할떄 ㅇ를 기준으로 관련 없는 파일을 건너뛸수 있음. customerId나 date 컬럼을 기준으로 자주 필터링 되는 데이터라면 이 2가지 중 하나를 기준으로 파티션을 생성하는것이 좋음. 너무 작은 단위로 분할하면 작은 크기의 파일이 대량으로 생성될수 있으므로 주의(전체 파일 목록을 읽을 떄 오버헤드 발생 가능) 버켓팅 조인이나 집계를 수행하는 방식에 따라 사전 분할(pre-partition) 할수 있음. 버켓팅을 사용하면 데이터를 1~2개의 파티션에 치우치지 않고 전체파티션에 균등하게 분산시킬수 있음. =&gt; 성능, 안전성 향상 조인 전에 셔플을 미리 방지할 수 있어서 데이터 접근 속도를 높일 수 있음.파일 수 데이터를 파티션이나 버켓으로 구성하려면 파일 수와 파일 크기도 고려해야 합니다. 데이터를 저장하는 방법에서는 트레이드 오프를 고려해야 함. 많은 수의 크기가 작은 파일인 경우 스케줄러는 그만큼 많은 읽기 태스크를 실행해야 하므로 네트워크와 잡 스케줄링 부하가 증가 적은 수의 대용량 파일이 있따면 스케줄러의 부하는 줄일 수 있지만, 태스크 수행시간이 오래 걸립니다. 적은 수의 대용량 파일은 입력 파일 수보다 더 많은 태스크 수를 스파크에 설정해 병렬성을 높일수는 있습니다.-입력 데이터 파일이 최소 수십 메가바이트의 데이터를 갖도록 크기를 조정하는것이 좋음.데이터 지역성(data locality) 기본적으로 네트워크를 통해 데이터 블록을 교환하지 않고, 특정 데이터를 가진 노드에서 동작할 수 있도록 저장하는 것을 의미함. 저장소 시스템과 스파크가 동일한 노드에 있고, 해당 시스템이 데이터 지역성 정보를 제공한다면 스파크는 입력 데이터 블록과 최대한 가까운 노드에서 태스크를 할당한다.통계 수집 구조적 API를 사용하면 비용 기반 쿼리 옵티마이저가 내부적으로 동작함.(입력 데이터의 속성을 기반으로 쿼리 실행 계획을 만듬) 비용 기반 옵티마이저를 작동시키려면 통계 를 수집하고 유지해야 한다.(이름이 지정된 테이블에서만 사용 가능)// 테이블 수준 통게 수집ANALYZE TABLE table_name COMPUTE STATISTICS// 컬럼 수준 통계 수집ANALYZE TABLE table_name COMPUTE STATISTICS FOR COLUMNS column_name1, column_name2, ...19.1.6 셔플 설정 외부 셔플 서비스를 설정하면 머신에서 실행되는 익스큐터가 바쁜 상황에서도(ex. GC) 원격 머신에서 셔플 데이터를 읽을 수 있으므로 성능을 높일수 있음.(코드가 복잡해지고 유지하기 어려워서 운영환경에서는 비추천) 외부 셔플 서비그 구성, 익스큐터당 동시 연결 수 등 셔플 관련 여러 설정들은 기본값을 사용하는것을 권장함.셔플 파티션 수에 따른 트레이드 오프 파티션 수가 너무 적으면 소수의 노드만 작업하여 데이터 치우침 현상 발생 파티션 수가 너무 많으면 태스크를 많이 실행해야 하므로 부하 발생.19.1.7 메모리 부족과 가비지 컬렉션 스파크 잡 실행 과정 중에 익스큐터나 드라이버 머신의 메모리가 부족하거나 메모리 압박(memory presure)으로 인해 태스크를 완료하지 못할 수 있다. 첫째, 애플리케이션 실행 중 메모리를 너무 많이 사용한 경우 둘째, 가비지 컬렉션이 자주 수행되는 경우 셋쨰, JVM 내에서 객체가 너무 많이 생성되어 더 이상 사용하지 않은 객체를 GC로 정리하면서 실행속도가 느려지는 경우 이는 구조적 API를 활용해 해결할수 있음.(잡의 효율성 증대, JVM 객체를 전혀 생성하지 않음.) 가비지 컬렉션 영향도 측정 GC 발생 빈도와 소요시간에 대한 통계를 모으는게 첫번째 단계이다. spark.executor.extraJavaOptions 속성에 JVM 옵션으로 -verbose:gc, -XX:+PrintGCTimeStamps 값을 추가해 통계를 모을수 있음.가비지 컬렉션 튜닝 spark.executor.extraJavaOptions 속성에 익스큐터에 대한 가비지 컬렉션 튜닝 옵션을 추가할 수 있음. 주기적인 GC 모니터링을 통해 튜닝 포인트를 찾아야 함.19.2 직접적인 성능 향상 기법19.2.1 병렬화 특정 스테이지의 처리 속도를 높이려면 병렬성을 높이는 작업부터 시작해야 함. spark.default.parallelism과 spark.sql.shuffle.partitons 값을 클러스터 코어수에 따라 설정.19.2.2 향상된 필터링 상황에 따라 데이터소스에 필터링을 위임하여 최종 결과와 무관한 데이터를 스파크에서 읽지 않고 작업을 진행할수 있도록 해야 합니다.19.2.3 파티션 재분배와 병합 파티션 재분배 과정은 셔플을 수반합니다. 하지만 클러스터 전체에 걸쳐 데이터가 균등하게 분배되므로 잡의 전체 실행 단계를 최적화 할 수 있음. 동일 노드의 파티션을 하나로 합치는 coalesce를 실행해 DataFrame이나 RDD의 전체 파티션 수를 줄이는것이 좋음. repartition 메소드는 부하를 분산하기 위해 네트워크로 데이터를 셔플링 파티션 재분배 과정은 부하를 유발하지만 애플리케이션 전체적인 성능과 스파크 잡의 병렬성을 높일 수 있음을 기억하자사용자 정의 파티셔닝 잡이 여전히 느리거나 불안정하다면 RDD를 이용한 사용자 정의 파티셔닝 기법을 적용할 수 있다. DataFrame보다 더 정밀한 수준으로 클러스터 전반의 데이터 체계를 제어(아주 드물게 사용되지만, 최적화 기법 중 하나)19.2.4 사용자 정의 함수(UDF) UDF 사용을 최대한 피하는 것도 좋은 최적화 방법 중 하나이다. UDF는 데이터를 JVM 객체로 변환하고 쿼리에서 레코드당 여러번 수행되므로 많은 자원을 소모함. 최대한 구조적 API를 활용하도록 하자.19.2.5 임시 데이터 자장소(캐싱) 같은 데이터셋을 계속해서 재사용한다면 캐싱을 사용해서 최적화 할 수 있음. 캐싱은 클러스터의 익스큐터 전반에 걸쳐 만들어진 임시 저장소(메모리나 디스크)에 DataFrame, 테이블 또는 RDD를 보관해 빠르게 접근할 수 있도록 한다. DataFrame이나 RDD의 cache 메소드를 사용해 데이터셋을 캐싱할수 있음.캐싱이 필요한 상황 스파크의 대화형 세션이나 스탠드얼론 앱에서 특정 데이터셋을 다시 사용하려 할 경우 데이터를 읽고 정제한 다음 다수의 처리에서 동일한 데이터를 재사용하는 경우캐시 사용시 주의사항 RDD는 물리적 데이터를 캐시에 저장함 반면 구조적 API 캐싱은 물리적 실행 계획 기반으로 이뤄짐. 실행 계획을 키로 저장하고 처리 과정 동안 물리적 실행 계획을 참조함. 누군가 원시 데이터를 읽으려 시도했지만, 먼저 캐시해놓은 버전의 데이터를 읽게 되면서 혼란이 발생할 수 있음. 데이터 캐시 저장소 레벨캐시된 DataFrame 위와 같은 과정을 통해 CSV 파일을 읽어 데이터를 파싱하는 부분을 반복적으로 하지 않을수 있음.# DataFrame을 캐싱하지 않는 원본 코드DF1 = spark.read.format(\"csv')\\ .option(\"inferSchema\", \"true\")\\ .option(\"header\", \"true)\\ .load(\"/data/flight-data/csv/2015-summary.csv\")DF2 = DF1 .groupBy(\"DEST_COUNTRY_NAME\").count( ).collect( )DF3 = DF1 groupBy(\"0RIGIN_COUNTRY_NAME\").count().collect()DF4 = DF1..groupBy( \"count'' ).count( ).collect( )# 캐싱한다면?DF1.cache()// 캐싱은 지연처리이므로, 데이터를 즉시 캐싱하기 위해 count 메서드를 사용.DF1.count()DF2 = DF1 .groupBy(\"DEST_COUNTRY_NAME\").count( ).collect( )DF3 = DF1 groupBy(\"0RIGIN_COUNTRY_NAME\").count().collect()DF4 = DF1..groupBy( \"count'' ).count( ).collect( ) 만약 클러스터의 전체 메모리가 가득 찼다면 데이터셋의 일부 데이터만 캐싱함.19.2.6 조인 최적화를 위한 공통 영역으로, 조인 타입에 따른 특성과 동작 방식을 이해해야 최적화를 잘 할 수 있음. 조인 순서 변경하는 간단한 작업만으로도 성능을 크게 높일 수 있음. 내부 조인을 사용해 필터링하는 것과 동일한 효과 브로드캐스트 조인 힌트를 사용하면 스파크가 쿼리 실행계획을 생성할 때 지능적으로 계획을 세울 수 있음. 안정성과 최적화를 위해 카테시안 조인이나 전체 외부 조인 사용은 지양해야 함.19.2.7 집계 집계 전에 충분히 많은 수의 파티션을 가질 수 있도록 데이터를 필터링하는것이 최선의 방법 RDD를 사용하여 집계 수행 방식을 정확하게 제어하여 성능과 안정성을 개선할 수 있음(groupByKey, reduceByKey)19.2.8 브로드캐스트 변수 앱에서 사용되는 다수의 UDF에서 큰 데이터 조각을 사용한다면 이 데이터 조각을 개별 노드에 전송해 읽기 전용 복사본으로 저장할 수 있다. 이 기능을 활용하면 잡마다 데이터 조각을 재전송하는 과정을 스킵할 수 있으므로 성능 향상에 도움이 된다.Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 18. 모니터링과 디버깅", "url": "/posts/spark-guide-18/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2023-02-18 18:07:00 +0900", "snippet": "[스파크 완벽 가이드] 18. 모니터링과 디버깅18.1 모니터링 범위 스파크 잡의 어느 지점에서 오류가 발생했는지 파악하려면 스파크 잡을 모니터링해야 한다. 실제 모니터링 대상과 모니터링에 필요한 옵션을 알아야 한다.스파크 애플리케이션과 잡 클러스터에서 사용자 애플리케이션이 실행되는 상황을 파악하거나 디버깅하려면 먼저 스파크 UI와 스파크 로그를...", "content": "[스파크 완벽 가이드] 18. 모니터링과 디버깅18.1 모니터링 범위 스파크 잡의 어느 지점에서 오류가 발생했는지 파악하려면 스파크 잡을 모니터링해야 한다. 실제 모니터링 대상과 모니터링에 필요한 옵션을 알아야 한다.스파크 애플리케이션과 잡 클러스터에서 사용자 애플리케이션이 실행되는 상황을 파악하거나 디버깅하려면 먼저 스파크 UI와 스파크 로그를 확인해야 한다. 스파크 UI와 스파크 로그는 실행중인 앱의 RDD와 쿼리 실행 계획 같은 개념적 수준의 정보를 제공한다.JVM 스파크는 모든 익스큐터를 개별 자바 가상 머신(JVM)에서 실행한다. 따라서 코드가 실행되는 과정을 이해하기 위해 JVM을 모니터링해야 한다. JVM 도구에는 스택 트레이스를 제공하는 jstack, heap dump를 생성하는 jmap, 시계열 통계 리포트를 제공하는 jstat, 다양한 JVM 속성 변수를 시각화된 방식으로 탐색할수 있는 jconsole 등이 있다. jvisualvm을 이용해 스파크 잡의 동작 특성을 알아볼수 있다. OS와 머신 JVM은 OS에서 실행되는데, 머신의 상태를 모니터링해 정상 작동 중인지 확인하는것이 매우 중요하다. CPU, 네트워크, I/O 등의 자원에 대한 모니터링도 함께 해야 한다. dstat, iostat, iotop 같은 OS 명령어를 통해 세밀하게 모니터링 할수 있다.클러스터 스파크 앱이 실행되는 클러스터도 모니터링해야 한다. YARN, 메소스, 스탠드얼론 클러스터 매니저가 모니터링 대상이다. 모니터링 솔루션을 활용하면 클러스터가 동작하지 않는 상황을 빠르게 알 수 있다. 클러스터 모니터링 도구로 Ganglia, Prometheus가 있다.18.2 모니터링 대대상 실행 중인 사용자 애플리케이션의 프로세스(CPU, 메모리 사용률 등) 프로세스 내부에서의 쿼리 실행 과정(ex. Job, Task)18.2.1 드라이버와 익스큐터 프로세스 스파크 앱을 모니터링할 때는 스파크 드라이버를 유심히 관찰해야 한다. 드라이버에는 모든 앱의 상태가 보관되어 있으며, 안정적으로 실행중인지 확인할 수 있다. 스파크는 수월한 모니터링을 지원하기 위해 Dropwizard Metrics Library 기반의 metric system 을 갖추고 있다. $SPARK_HOME/conf/metric.properties 파일을 구성하여 클러스터 모니터링 솔루션을 포함해 다양한 시스템으로 내보낼 수 있다.18.2.2 쿼리, 잡, 스테이지, 태스크 특정 쿼리에서 무슨 일이 일어나고 있는지 알아야 할 때도 있다. 스파크의 쿼리, 잡 스테이지, 태스크의 각각의 정보를 확인할 수 있다. 이러한 정보는 클러스터에서 특정 시점에 실행되는 작업을 파악할 수 있으며, 성능 개선이나 디버깅 시 매우 유용하다.18.3 스파크 로그 스파크 앱의 로그나 스파크 자체의 로그에서 발견된 이상한 이벤트는 잡의 실패 지점이나 원인을 파악하는 데 큰 도움이 됩니다. 템플릿에 설정된 로깅 프레임워크를 통해 스파크 로그 및 앱 로그를 모두 확인할 수 있다. 파이썬에서는 자바 기반 로깅 라이브러리를 사용할 수 없으므로 파이썬의 logging 모듈 혹은 print 구문을 사용해 표준 오류를 결과를 출력해야 한다.18.4 스파크 UI 스파크UI를 통해 실행중인 앱과 스파크 워크로드에 대한 평가지표를 모니터링 할 수 있다. 모든 SparkContext는 실행 시 앱과 관련된 유용한 정보를 제공하는 웹 UI를 4040포트로 기본 실행된다.스파크 UI에서의 모니터링 항목 Job : 스파크 잡에 대한 정보 제공 Stages : 개별 스테이지(스테이지의 태스크를 포함) 관련 정보 제공 Storage : 스파크 앱에 캐싱된 정보와 데이터 정보 제공 Environment : 스파크 앱의 구성과 설정 관련 정보 제공 Executors : 앱에서 사용중인 익스큐터의 상세 정보 제공 SQL : SQL과 Dataframe을 포함한 구조적 API 쿼리 제공예제spark.read\\ .option(\"header\", \"true\")\\ .csv(\"/data/retail-data/all/online-retail-dataset.csv\")\\ .repartition(2)\\ .selectExpr(\"instr(Description, 'GLASS') &gt;= 1 as is_glass\")\\ .groupBy(\"is_glass\")\\ .count()\\ .collect()쿼리의 요약 통계 정보스테이지 1 CSV 파일을 모두 스캔하는 스테이지 하단의 박스 영역은 파티션 재분배로 인해 발생하는 셔플 스테이지를 나타낸다.(파티션 확정은 아니고 일단 2개로 분할)스테이지 2 프로젝션(컬럼 선택/추가/필터링)과 집계를 수행 데이터를 셔플하기 전에 파티션별로 집계를 수행하고, 위 예제에서는 해시 기반의 집계를 수행한다. 파티션별 집계 결과 수와 파티션 수를 곱해 결과로 반환되는 로우수를 계싼할 수 있다. 스테이지3 이전 스테이지의 결과에 대한 집계를 수행.Jobs 탭에서 확인할 수 있는 내용 잡에서는 3개의 스테이지로 나뉘어져 있다.(SQL 탭과 일치) Description의 링크 중 하나를 클릭하면 스테이지에 대한 자세한 내용을 확인할 수 있다. 3개의 스테이지로 구성되어 있으며, 각각 8,2, 200개의 태스크를 실행했다. 스테이지1에서 8개의 태스크를 가진다. CSV파일은 분할 가능하기 때문에 스파크는 작업을 나눌 수 있다. 작업을 머신의 여러 코어에 비교적 고르게 배분 스테이지2에서 2개의 파티션으로 나누기 위해 repartition을 수행했으므로 2개의 태스크를 가진다. 마지막 스테이지에서 셔플 파티션의 기본값은 200이므로 200개의 태스크를 가진다.스파크 태스크 정보 Summary Metrics 부부분에서 다양한 메트릭 관한 요약 통계를 제공한다. 균일하지 않게 분산된 값을 눈여겨보아야 한다.스파크 UI 설정하기 다양한 속성을 사용해 스파크 UI를 설정할 수 있다.18.4.1 스파크 REST API 스파크 UI 외에도 REST API로 스파크의 상태와 매트릭을 확인할 수 있다. http://localhost:4040/api/v1 으로 API를 처리할 수 있다.18.4.2 스파크 UI 히스토리 서버 스파크 UI는 SparkContenxt가 실행되는 동안 사용할 수 있다. 정상적으로 종료되거나 비정상적으로 종료된 앱의 정보를 확인하려면 스파크 히스토리 서버를 이용해야 한다. 이벤트 로그를 저장하도록 스파크 앱을 설정하면 히스토리 서버를 이용해 스파크 UI와 REST API를 재구성할 수 있다.18.5 디버깅 및 스파크 응급 처치 이를 위해 스파크 내부에서 발생하는 문제(ex. Out Of Memory Error 등)와 사용자가 경험할 수 있는 증상(느린 태스크)을 포함해 스파크 잡에서 발생할 수 있는 다양한 문제를 알아보자.18.5.1 스파크 앱이 시작되지 않는 경우징후와 증상 스파크 잡이 실행되지 않음. 스파크 UI가 드라이버 노드를 제외한 클러스터 노드의 정보를 전혀 표시하지 못함. 스파크 UI가 잘못된 정보를 표시하는 것 같음.잠재적 대응법 클러스터나 앱의 실행에 필요한 자원을 적절하게 설정하지 않았을 경우 발생. IP를 잘못 입력했거나, 포트가 열리지 않은 경우 대부분 클러스터 영역, 머신, 설정과 관련된 문제인 경우가 많음. 익스큐터 자원을 클러스터 매니저의 유휴 자원 이상으로 요청하는 경우 드라이버는 익스큐터가 실행될 때까지 무한정 대기한다.18.5.2 스파크 앱 실행 전에 오류가 발생한 경우 새로운 애플리케이션을 개발해 클러스터에서 실행할 때 발생.징후와 증상 명령이 전혀 실행되지 않으며 오류 메시지 출력 스파크 UI에서 잡, 스테이지, 태스크 정보를 확인할 수 없음.잠재적 대응법 스파크 UI의 Environment 탭에서 앱의 정보가 올바른지 확인하고, 코드를 검토한다. 단순 오타나 잘못된 컬럼명을 사용하는 경우 스파크 실행계획을 만드는 과정에서 오류가 발생할 수 있다.18.5.3 스파크 앱 실행 중에 오류가 발생한 경우징후와 증상 하나의 스파크 잡이 전체 클러스터에서 성공적으로 실행되지만 다음 잡은 실패 여러 단계로 처리되는 쿼리의 특정 단계가 실패 어제 정상 동작한 예약 작업이 오늘 실패 오류 메시지를 해석하기 어려운 경우잠재적 대응법 데이터가 존재하는지, 데이터가 올바른 포맷인지 확인 쿼리 실행 즉시 오류가 발생한다면 실행 계획을 만드는 단게에서 발생해 분석 오류일 가능성이 높다. 쿼리 자체에 컬럼명, 뷰, 테이블 존재 여부를 확인. 어떤 컴포넌트가 연관되어 있는지 알아내기 위헤ㅐ 스택 트레이스를 분석해 단서를 찾아야 한다. 입력 데이터와 데이터 포맷을 확인해본다. 잡의 태스크가 잠시 실행되다가 비정상적으로 종료된다면 입력 데이터 자체의 문제일 수 있다.(null값 존재 유무 체크)18.5.4 느리거나 뒤처진 태스크 앱 최적화할떄 매우 흔하게 발생. 머신간의 작업이 균등하게 분배되지 않거나(데이터 치우침) 특정 머신이 다른 머신에 비해 처리 속도가 느린 경우(하드웨어 문제)에도 발생.징후와 증상 스파크 스테이지에서 대부분의 태스크가 정상적으로 실행되었으며 소수의 태스크만 남아 있는 경우(오래 실행되는 경우) 스파크 UI에서 첫번쨰 증상과 같은 태스크가 확인되고, 동일한 데이터셋을 다룰때 항상 발생 여러 스테이지에서 번갈아가며 두번쨰 증상과 같은 현상이 발생 여러 스테이지에서 번갈아 가며 두번째 증상과 같은 현상이 발생 머신 수를 늘려도 상황이 개선되지 않고 여전히 특정 태스크가 오래 걸리는 경우 매트릭을 보면 특정 익스큐터가 다른 익스큐터에 비해 훨씬 많은 데이터를 읽거나 쓰고 있는 경우잠재적 대응법 느린 태스크를 낙오자(straggler)라 부르는데, DataFrame이나 RDD 파티션에 데이터가 균등하게 분할되지 않는 경우에 주로 발생.다양한 원인으로 낙오자가 발생하므로 디버깅하기 가장 어려운 문제 중 하나이다. 대부분의 경우 데이터 치우침 현상으로 인해 발생하므로 이를 우선 확인해보면 좋다. 파티션별 데이터양을 줄이기 위해 파티션 수를 증가시킨다. 다른 컬럼을 조합해 파티션을 재분배한다. 가능하다면 익스큐터의 메모리를 증가시킨다. 익스큐터에 문제가 있는지 모니터링하고, 문제가 있다면 해당 머신의 다른 잡에서도 동일한 문제가 있는지 확인 사용자 정의함수를 구현할 때 객체 할당이나 비즈니스 로직에 쓸모없는 부분이 있는지 확인하고 DataFrame 코드로 변환18.5.5 느린 집계 속도 데이터 치우침이 있는지 먼저 확인하고 그럼에도 발생할 경우 추가 확인징후와 증상 groupBy 호출시 느린 태스크가 발생 집계 처리 이후의 잡도 느린 경우잠재적 대응법 집계 연산 전에 파티션 수를 증가시키면 태스크별로 처리할 키 수를 줄일 수 있다. 익스큐터의 메모리를 증가시킨 경우 많은 키를 처리하는 익스큐터는 다른 키를 처리하는 익스큐터에 비해 느릴수 있지만 개선의 여지가 있다. 집계 처리가 끝나고 이어서 실행되는 태스크가 느리다면 집계 처리된 데이터셋에 불균형 현상이 남아 있다는것을 의미하는데, 이 경우 파티션을 임의로 재부분핼사 웄디록 repartition을 수행한다. 모든 필터와 SELECT 구문이 집계 연산보다 먼저 처리된다면 필요한 데이터만을 이용해서 집계를 수행할 수 있다. null 값을 나타내기 위해 \" \" 또는 \"EMPTY\"와 같은 값을 대체 값으로 사용하는지 확인 실제 null로 값이 셋팅되어 있어야 최적화를 수행할 수 있다. 일부 집계 함수는 달느 함수에 비해 태생적으로 늘니 경우가 있는데 이는 꼭 필요한 경우가 아니라면 사용을 자제해야 한다. collect_list, collect_set은 모든 객체를 드라이버로 전송하기 때문에 아주 느리게 동작한다. 18.5.6 느린 조인 속도 조인과 집계는 모두 셔플을 유발하기 때문에 동일한 증상과 대응법을 가진다.징후와 증상 조인 스테이지의 처리 시간이 오래 걸리는 경우 조인 전후의 스테이지는 정상적으로 동작잠재적 대응법 많은 조인 연산은 다른 조인 타입으로 변경해 최적회(자동 또는 수동)하낟. 조인 순서를 변경하면서 잡의 처리 속다가 올라가는지 테스트 조인을 수행하기 전에 데이터셋을 분할하면 클러스터 노드간 데이터 이동을 줄일 수 있다. 데이터 치우침 현상은 느린 조인을 유발할 수 있으므로 익스큐터의 자원 할당량을 늘리면 동무이 될 수 있다. 나머지는 집계쪽과 동일한 대응방법을 사용한다.18.5.7 느린 읽기와 쓰기 속도 느린 I/O는 진단이 어려울 수 있다.징후와 증상 분산 파일 시스템이나 외부 시스템의 데이터를 읽는 속도가 느린 경우 네트워크 파일 시스템이나 blob 저장소에 데이터를 쓰는 속도가 느린 경우잠재적 대응법 스파크의 투기적 실행(spark.speculation 속성 = true)을 사용하면 느린 읽기와 쓰기 속도를 개선하는데 도움이 될 수 있다. 일관성을 보장하는 파일 시스템과 함께 사용하는것이 좋다. aws s3와 같이 eventually consistency 방식을 사용하는 저장소에서는 데이터 중복이 발생할 수 있다. 스파크 클러스터와 저장소 시스템 간의 네트워크 대역폭이 충분하지 않을 수 있으므로 네트워크 성능 문제가 없는지 반드시 확인 단일 클러스터에서 스파크 HDFS 같은 분산파일 시스템을 함께 구성하려면 클러스터의 노드마다 스파크의 분산 파일 시스템 모두를 동일한 호스트명을 인식하는지 확인한디. 스파크의 지역성을 고려해 스케줄링하며, 스파크 UI의 locality 컬럼에서 지역성 정보를 확인 18.5.8 드라이버 OutOfmemoryError 또는 응답 없음 드라이버에 너무 많은 데이터를 전송해 메모리를 모두 소비한 경우 자주 발생.징후와 증상 스파크 앱이 응답하지 않거나 비정상적으로 종료 드라이버 로그에 OOM 또는 GC 관련된 메시지가 출력 명령이 장시간 실행되거나 실행되지 않는 경우 반응이 거의 없는 경우 드라이버 JVM 메모리 사용량이 높은 경우잠재적 대응법 사용자 코드에서 collect 메서드 같은 연산을 실행해 너무 큰 데이터셋을 드라이버로 전송하려고 시도했는지 확인 브로드캐스트하기에 너무 큰 데이터를 브로드캐스트 조인했는지 확인 장시간 실행되는 앱은 드라이버에 많은 양의 객체를 생성하고 해제하지 못할 수 있다. JVM 힙 덤프 등을 통해 확인해볼 필요가 있다. 가능하다면 드라이버의 가용 메모리를 늘린다. JVM 메모리 부족은 파이썬과 같은 다른 언어를 함꼐 사용하는 경우 발생할 가능성이 높음. 두 언어간의 데이터 변환 과정에서 과도한 메모리가 사용될 수 있다. SQL JDBC 서버와 노트북 환경을 이용해 다른 사용자와 SparkContext를 공유하는 상황이라면 동시에 대량의 데이터를 드라이버 메모리로 전송할 수 있는 명령을 실행하지 못하도록 막아야 한다.18.5.9 익스큐터 OutOfMemoryError 또는 응답 없음 경우에 따라 스파크 앱이 자동으로 복구하는 경우도 있음징후와 증상 익스큐터 로그에 OOM 또는 GC 관련 메시지가 출력되며 스파크 UI에서도 확인할 수 있다. 익스큐터가 비정상적으로 종료되거나 응답하지 않는 경우 특정 노드의 느린 태스크가 복구되지 않는 경우잠재적 대응법 익스큐터의 가용 메모리와 익스큐터 수를 늘린다. 관련 파이썬 설정을 변경해 pyspark 워커의 크기를 증가시킨다. GC가 발생했다면 사용자정의함수 등에서 문제가 있는지 확인하고, 데이터 파티션을 재분배하여 병렬성을 높이는 방법을 고민해봐야 한다. null 값을 정확히 제어하기 위해 \" \" 또는 “EMPTY” 같은 값을 사용한것은 아닌지 체크 RDD와 Dataset은 객체를 생성하므로 문제가 발생할 가능성이 더 크다. 가급적 사용자 정의 함수 사용을 지양해야 한다. 힘 덤프를 통해 익스큐터의 상태를 확인해야 한다. 키-값 저장소 같이 다른 워크로드를 처리하는 노드에 익스큐터가 위치한다면 스파크 잡을 다른 작업과 분리해야 한다.18.5.10 의도하지 않은 null 값이 있는 결과 데이터징후와 증상 트랜스포메이션이 실행된 결과에 의도치 않은 null값이 발생 잘 동작하던 운영환경의 예약 작업이 더 동작하지 않거나 정확한 결과를 생성하지 못하는 경우잠재적 대응법 비즈니스 로직을 변경하지 않은 경우 데이터 포맷 변경여부 확인(이전에 잘 동작했다면) 어큐뮬레이터를 사용해 레코드나 특정 데이터 타입의 수를 확인(비정상 레코드를 확인할 수 있음) 트랜스포메이션이 실제 유효한 쿼리 실행 계획을 생성하는지 확인. 스파크 SQL에서 암시적 형변환을 수행하는 경우 의도치 않은 값이 나올 수 있다. 5 * \"23\" =&gt; 113, 5 * \" \" =&gt; null 18.5. 11 디스크 공간 없음 오류징후와 증상 no space left on disk 오류 메시지와 함꼐 잡이 실패잠재적 대응법 더 많은 디스크 공간을 확보 제한된 용량의 저장소를 가진 클러스터인 경우 데이터 치우침 현상으로 인해 문제가 생길수 있음. 데이터 파티션을 재분배하여 해결 몇가지 저장소 설정을 실험해보고 원인 파악 문제가 되는 머신의 오래된 로그 파일과 셔플 파일을 수동으로 제거(단기적 문제 해결방법)18.5.12 직렬화 오류징후와 증상 직렬화 오류와 함꼐 잡이 실패잠재적 대응법 구조적 API를 사용하는 경우 보통 에러가 나타나지 않는다. (이를 사용하는 방향으로 변경) UDF, RDD을 이용하는 경우 문제가 발생하는 경우 있음. 자바나 스칼라 클래스에서 UDF를 생성할 때는 enclosing object의 필드를 참조하면 안된다. enclosing object : 내부 클래스를 감싸고 있는 외부 클래스의 객체 인스턴스 스파크는 전체 인클로징 객체를 직렬화하려고 시도하지만 불가능할 수 있다. 관련 필드를 클로저와 동일한 범주의 로컬 변수로 복사하고 사용해야 한다. Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 17. 스파크 배포 환경", "url": "/posts/spark-guide-17/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2023-02-04 20:53:00 +0900", "snippet": "[스파크 완벽 가이드] 17. 스파크 배포 환경 클러스터 배포 시 선택 사항스파크가 지원하는 클러스터 매니저배포시 고려사항과 배포 환경 설정17.1 스파크 앱 실행을 위한 클러스터 환경 클러스터를 구성할 수 있는 환경은 크게 2가지로 나눌수 있다.17.1.1 설치형 클러스터 배포 환경 자체 데이터 센터를 운영하는 조직에 배포 환경에 적합한 배포환...", "content": "[스파크 완벽 가이드] 17. 스파크 배포 환경 클러스터 배포 시 선택 사항스파크가 지원하는 클러스터 매니저배포시 고려사항과 배포 환경 설정17.1 스파크 앱 실행을 위한 클러스터 환경 클러스터를 구성할 수 있는 환경은 크게 2가지로 나눌수 있다.17.1.1 설치형 클러스터 배포 환경 자체 데이터 센터를 운영하는 조직에 배포 환경에 적합한 배포환경이다. 그 밖의 모든 상황에서 트레이드가 존재함. 장점 하드웨어를 완전히 제어할 수 있으므로 특정 워크로드의 성능을 최적화 할수 있다.단점 설치형 클러스터의 크기가 제한적인데, 분석 워크로드에 필요한 자원은 상황에 따라 달라질수 있다. 너무 작게 만들면 리소스가 많이 자는 작업을 할수 없고, 너무 크게 만들면 자원낭비가 심해진다. HDFS나 분산 키-값 저장소같은 자체 저장소 시스템을 선택하고 운영해야 한다. 상황에 따라 지리적 복제, 재해 복구 쳬계도 함께 구축/고민되어야 한다.구성시 유의사항 클러스터 매니저를 사용해서 자원 활용 문제를 해결해야 한다.17.1.2 공개 클라우드 초기 빅데이터 시스템은 설치형 클러스터에 적합하게 설계되었으나 시간이 지날수록 클라우드 환경이 일반적인 스파크 운영 환경으로 자리잡고 있다.장점 자원을 탄력적으로 늘리고 줄이는것이 가능하다. 필요에 따라 비용만 지불하고 반납하면 그만이다. 공개 클라우드 환경이 비용이 저렴하고 지리적 복제 기능을 지원하는 경우도 많이 있다.단점 설치형에서 클라우드로 이관하는 경우 기존 방식으로 실행하는것은 좋지 않을수 있음. 아마존 S3, 애저 Blob, 구글 클라우드 저장소 등을 사용하는 방식으로 이관이 필요하다. 설칠형 구축 클러스터에 비해 세밀하게 컨트롤하지 못하는 부분이 있을수 있음.17.2 클러스터 매니저 클라우드 환경에서 매니지드 서비스로 이용하는것이 아니라면 별도의 클러스터 매니저를 선택해야 한다.17.2.1 스탠드얼론 모드 스파크의 스탠드얼론 클러스터 매니저는 매니저는 아파치 스파크 워크로드용으로 특별히 제작된 경량화 플랫폼이다. 다른 클러스터 매니저보다 제한적인 기능을 가지고 있다. 스파크 애플리케이션에서만 실행할 수 있다. 스탠드얼론 클러스터 시작하기// 마스터 프로세스 실행$SPARK- HOME/sbin/start.master.sh// 워커노드 프로세스 실행$SPARK_HOME/sbin/start.slave.sh &lt;마스터-스파크-프로세스-URI&gt;스크립트를 이용한 스탠드얼론 클러스터 시작하기 시작 스크립트를 설정해 스탠드얼론 클러스터의 시작을 자동화할 수 있다. start-all.sh, stop-master.sh, stop-slaves.sh, stop-all.sh 등을 제공한다.스탠드얼론 클러스터 설정 앱 튜닝에 필요한 여러 가지 설정을 가지고 있다. 이러한 설정은 종료된 앱의 워커별작업 파일에서부터 워커의 코어와 메모리 등 모든것을 제어할 수 있다.애플리케이션 제출하기 마스터 프로세스의 URI를 이용해 spark-submit 명령을 사용할 수 있는 머신에서 애플리케이션을 제출할 수 있다.17.2.2 YARN에서 스파크 실행하기 하둡 YARN은 잡 스케줄링과 클러스터 자원 관리용 프레임워크이다. 스파크는 기본적으로 하둡 YARN 클러스터 매니저를 지원하기는 하지만 하둡 자체가 필요한것은 아니다. 스파크와 하둡은 직접적으로는 거의 관련이 없다. YARN은 다양한 실행 프레임워크를 지원하는 통합 스케줄러로, 스탠드언론 모드에 비해 많은 기능을 사용할 수 있다.애플리케이션 제출하기 --master 인수 값을 yarn으로 지정하는 차이점이 있다.17.2.3 YARN환경의 스파크 애플리케이션 설정하기 YARN에 스파크 애플리케이션을 배포하려면 다양한 설정과 스파크 앱에 미치는 영향을 이해해야 한다.하둡 설정 스파크를 이용해 HDFS의 파일을 읽고 쓸며ㅕㄴ 스파크 클래스패스에 두개의 하둡 설정 파일을 포함시켜야 한다. hdfs-site.xml : HDFS 클라이언트 동작 방식 결정 core-site.xml : 기본 파일 시스템의 이름을 설정 스파크에서 하둡 설정 파일을 사용하려면 $SPARK_HOME/spark-env.sh 파일의 HADOOP_CONF_DIR 변숫값을 하둡 설정 파일 경로로 지정하거나 spark-submit 명령을 사용해서 환경변수를 지정해야 한다.17.2.4 메소스에서 스파크 실행하기 메소스는 스파크를 실행할 수 있는 또 다른 클러스터 매니저이다. 여러 스파크 개발자들이 개발한 오픈소스메소스 정의 CPU, 메모리, 저장소, 그리고 다른 연산 자원을 머신에서 추상화한다. 이를 통해 내고장성(fault-tolerant) 및 탄력적 분산 시스템을 쉽게 구성하고 효과적으로 실행할 수 있다.메소스 특징 가장 무거운 클러스터 매니저이므로, 대규모의 메소스 배포 환경이 있는 경우에만 사용하는 것이 좋다. 거대한 인프라 구조이고, 메소스 클러스터를 배포하고 유지하는 방법을 별도 숙지하여야 한다.애플리케이션 제출하기 다른 클러스터 매니저와 유사한 방식으로 메소스 클러스터에 스파크 앱을 제출할 수 있다. spark-env.sh에 환경변수를 선언하고 사용할 수 있다.export MESOS_NATIVE_JAVA_LIBRARY=&lt;libmesos.so 파일의 경로&gt;export SPARK_EXECUTOR_URI=&lt;업로드한 spark-2.2.0.tar.gz 파얼의 URL&gt;import org.apache.spark.sql.SparkSessionval spark = SparkSession.builder .master(\"mesos://HOST:5050\") .appName(\"my app\") .config(\"spark.executor.uri\", \"&lt;path to spark-2.2.0.tar.gz uploaded above&gt;\") .getOrCreate()메소스 설정하기 실행과 관련된 설정을 제공한다. 자세한 내용은 공식 문서를 ㅊ마고17.2.5 보안 관련 설정 스파크는 신뢰도가 낮은 환경에서 애플리케이션을 안전하게 실행할 수 있도록 저수준 API 기능을 제공한다. 인증, 네트워크 구간 암호화, TLS/SSL을 설정할 수 있다.17.2.6 클러스터 네트워크 설정 더 나은 네트워크 실행 환경을 위해 시도할수 있는 튜닝 방법이다. 클러스터 노드 사이에 proxy를 사용하기 위해 스파크 클러스터에 사용자 정의 배포 설정을 적용하는 경우가 있을수 있다.17.2.7 애플리케이션 스케줄링 연산 과정에서 필요한 자원을 스케줄링 할 수 있는 몇가지 기능을 제공한다. 첫째, 각 스파크 애플리케이션은 독립적인 익스큐터 프로세스를 실행한다. 클러스터 매니저는 앱 전체에 대한 스케줄링 기능을 제공한다. 둘쨰, 스파크 앱에서 여러개의 잡을 다른 스레드가 제출한 경우 동시에 실행할 수 있다.동적 할당 하나의 클러스터에서 여러 스파크 앱을 실행하려면 워크로드에 따라 앱이 점유하는 자원을 동적으로 조정해야 한다. 동적 할당은 사용자 앱이 사용하지 않는 자원을 클러스터에 반환하고 필요할 때 다시 요청하는 방식을 의미한다.17.3 기타 고려사항 애플리케이션 개수와 유형을 고려해야 한다. YARN은 HDFS를 사용하는 앱을 실행할 때 가장 적합하지만 그외의 경우에는 잘 사용하지 않는다. YARN은 HDFS의 정보를 사용하도록 설계되어 있으므로 클라우드 환경을 제대로 지원할 수 없다. 또 연산용 클러스터와 저장소 클러스터가 강하게 결합되어 있다.(클러스터 확장시 동시 확장만 가능) 메소스는 YARN이 가진 개념을 조금 더 개선하였으며 다양한 앱 유형을 지원한다. 다만, 큰 규모의 클러스터에 적합하다. 스파크 앱을 실행한다고 해서 메소스 클러스터를 꼭 구축해야 하는것은 안디ㅏ. 다양한 스파크 버전을 관리하는 것도 상당히 어려운 문제이다. 다양한 스파크 버전으로 된 앱을 실행하려면 버전별 설정 스크립트를 관리하는 데 많은 시간을 할애해야 한다. 클러스터 매니저와 관계 없이 앱 디버깅이나 이슈 추적을 위한 로그를 기록하는 방식도 적절히 결정해야 한다. 데이터 카탈로그같은 저장된 데이터셋의 메타데이터 관리를 위한 메타스토어 사용을 고려해야 한다. 워크로드 특성에 맞춰 외부 셔플 서비스를 사용해야 할수도 있다. 클러스터에서 실행되는 스파크 잡을 디버깅하려면 최소한 기본적인 모니터링 솔루션 도입은 필수적이다.Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 16. 스파크 애플리케이션 개발하기", "url": "/posts/spark-guide-16/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2023-02-04 20:10:00 +0900", "snippet": "[스파크 완벽 가이드] 16. 스파크 애플리케이션 개발하기16.1 스파크 애플리케이션 작성하기 스파크 애플리케이션은 스파크 클러스터와 사용자 코드 2가지 조합으로 구성된다.16.1.1 간단한 스칼라 기반 앱 스칼라는 스파크의 기본 언어이기 때문에 이를 개발하는 가장 적합한 방법이라 볼 수 있다. 다만 실무에서 airflow를 이용해...", "content": "[스파크 완벽 가이드] 16. 스파크 애플리케이션 개발하기16.1 스파크 애플리케이션 작성하기 스파크 애플리케이션은 스파크 클러스터와 사용자 코드 2가지 조합으로 구성된다.16.1.1 간단한 스칼라 기반 앱 스칼라는 스파크의 기본 언어이기 때문에 이를 개발하는 가장 적합한 방법이라 볼 수 있다. 다만 실무에서 airflow를 이용해 스케줄링이 가능한 파이프라인 구축시에는 python을 가장 많이 사용하는듯 하다. object DataFrameExample extends Serializable { def main(args: Array[String]) = { val spark = SparkSession .builder() .appName(\"Databricks Spark Example\") .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") .getOrCreate() import spark.implicits._ spark.udf.register(\"pointlessUDF\", DFUtils.pointlessUDF(_:String):String) val authors = Seq(\"bill,databricks\", \"matei,databricks\") val authorsDF = spark .sparkContext .parallelize(authors) .toDF(\"raw\") .selectExpr(\"split(raw, ',') as values\") .selectExpr(\"pointlessUDF(values[0]) as name\", \"values[1] as company\") .show() }} 위와 같은 예제 코드가 있다고했을떄 이를 실행하기 위해 빌드를 우선해야 한다. JVM 언어를 빌드하기 위한 도구는 gradle, maven, sbt 등 다양한 방법이 있으나 여기서는 생략하도록 한다.애플리케이션 실행하기 빌드된 jar파일을 spark-submit에 파라미터로 지정하여 앱을 실행할 수 있다.$SPARK_HOME/bin/spark-submit --class com.databricks.example.DataFrameExample --master local target/scala-2.11/example_2.11-0.1-SNAPSHOT.jar \"hello\"16.1.2 파이썬 애플리케이션 작성하기 PySpark 애플리케이션을 작성하는 방법은 일반 파이썬 애플리케이션이나 패키지를 작성하는 방법과 거의 비슷하다. 파이썬에서는 빌드 개념이 없고, 일반 스크립트를 작성하는것에 지나지 않는다.from __future__ import print_functionif __name__ == '__main__': from pyspark.sql import SparkSession spark = SparkSession.builder \\ .master(\"local\") \\ .appName(\"Word Count\") \\ .config(\"spark.some.config.option\", \"some-value\") \\ .getOrCreate()print(spark.range(5000).where(\"id &gt; 500\").selectExpr(\"sum(id)\").collect()) 위 코드가 실행되면 애플리케이션에서 활용할 수 있는 SparkSession 객체가 생성된다. 권장 방법은 모든 파이썬 클래스에서 SparkSession 객체를 생성하는 것보다 런타임 환경에서는 변수를 생성해 파이썬 클래스에 전달하는 방식을 사용하는것이 더 좋다. 애플리케이션 실행하기$SPARK_HOME/bin/spark-submit -master local pyspark/template/amin.py16.1.3 자바 애플리케이션 작성하기 스파크와 거의 유사하다.16.2 스파크 애플리케이션 테스트 약간 지루하지만 아주 중요한 주제인 테스트 방법을 알아보겠습니다. 스파크 앱을 테스트하려면 작성시 몇가지 핵심 원칙과 구성 전략을 고려해야 한다.16.2.1 전략적 원칙 데이터 파이프라인과 앱에 대한 테스트 코드 개발은 실제 애플리케이션 개발만큼이나 중요하다. 테스트 코드는 미래에 발생할 수 있는 데이터, 로직 그리고 결과 변환에 유연하게 대처할 수 있게 도와준다.입력 데이터에 대한 유연성 데이터 파이프라인은 다양한 유형의 입력 데이터에 유연하게 대처할 수 있어야 한다. 비즈니스 요구사항이 변하면 데이터도 변한다. 스파크 앱과 파이프라인은 입력 데이터 중 일부가 변하더라도 유연하게 대처할 수 있어야 한다. 입력 데이터 변화로 발생할 수 있는 다양한 예외 상항을 테스트하는 코드를 작성해야 한다.비즈니스 로직 변경에 대한 유연성 예상했던 원형 데이터의 형태가 실제 원형 데이터와 같은지 확인하고 싶은 경우, 원하는 결과를 얻을 수 있도록 실제와 유사한 데이터를 사용해 비즈니스 로직을 꼼꼼하게 테스트해야 한다. 이 유형의 테스트에서는 스파크의 고유 기능에 대한 단위 테스트를 작성하지 말고, 비즈니스 로직을 테스트해서 복잡한 비즈니스 파이프라인이 의도한 대로 동작하는지를 검증해야 한다.결과의 유연성과 원자성 입력 데이터 및 비즈니스 로직의 테스트가 완료되었다면 결과가 의도한 대로 반환되느지 확인해야 한다. 데이터가 얼마나 자주 갱신되는지, 데이터가 완벽한지(늦게 들어온 데이터가 없는지), 마지막 순간에 데이터가 변경되지 않았는지 등을 이해할 수 있도록 만들어야 한다.16.2.2 테스트 코드 작성시 고려사항 적절한 단위 테스트를 작성해 입력 데이터나 구조가 변경되어도 비즈니스 로직이 정상적으로 동작하는지 확인 해야 한다. 단위 테스트를 하면 스키마 변경되는 상황에 쉽게 대응할 수 있다.SparkSession 관리하기 스파크 로컬 모드 덕분에 단위 테스트용 프레임워크로 비교적 쉽게 스파크 코드를 테스트할 수 있다. 단위 테스트에서는 의존성 주입 방식으로 SparkSession이 관리하도록 만들어주면 된다. 런타임에 적절히 SparkSession을 전달해주는 방식으로 코드 작성 테스트 코드용 스파크 API 선정하기 SQL, DataFrame, Dataset 다양한 API 중 유지보수성, 테스트 용이성 측면에서 적절히 검토한 후 팀의 니즈에 맞게 선정하여야 한다. API 유형에 관계없이 함수의 입/출력(함수 시그니처)을 정의하면 추후 이런 변경에 유연하게 대응할 수 있다.16.2.3 단위 테스트 프레임워크에 연결하기 테스트마다 SparkSession을 생성하고 제거하도록 설정하는 것이 좋다. JUnit의 @Before, @After 애노테이션과 같은 것을 활용 16.2.4 데이터소스 연결하기 테스트 코드에서는 운영 환경의 데이터소스에 접속하지 않도록 해야 한다. 로컬 환경에 Docker를 이용해 별도 데이터 소스를 연결해서 처리할수 있도록 하거나 h2 in-memory db 같은것을 활용할 수 있음 16.3 개발 프로세스 스파크 애플리케이션의 개발에서도 기존에 삳용하던 개발 흐름과 유사하다. 대화형 노트북이나 유사한 환경에 치고화된 작업 공간을 마련한다.(Zeppline) 그리고 핵심 컴포넌트와 알고리즘을 만든 후에 영구적인 영역으로 코드를 옮긴다. 로컬 머신에서 spark-shell과 같은 것을 사용할 수도 있다.16.4 애플리케이션 시작하기 spark-submit 명령을 실행하고, 잡 제출시 클라이언트 모드 혹은 클러스터 모드를 선택해서 실행할 수 있다.spark-submit 도움말배포 환경별 설정16.4.1 애플리케이션 시작 예제/bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://207.184.161.138:7077 --executor-memory 206 --total-executor-cores 100 replace/with/path/to/examples.jar 1000/bin/spark-submit --master spark://207.184.161.138:7077 examples/src/main/python/pi.py master 옵션의 값을 local이나 local[*]로 변경하면 애플리케이션 로컬 모드를 실행할 수 있다.16.5 애플리케이션 환경 설정하기 목적에 따라 다양한 환경을 설정할 수 있다. 애플리케이션 속성런타임 환경셔플 동작 방식스파크 UI압축과 직렬화메모리 관리처리 방식네트워크 설정스케줄링동적할당보안암호화스파크 SQL스파크 스트리밍SparkR설정 방식 스파크 속성은 대부분 애플리케이션 파라미터를 제어하며, SparkConf 객체를 사용해 스파크 속성을 설정할 수 있다. 자바 시스템 Property 하드코딩된 환경 설정 파일16.5.1 SparkConfimport org.apache.spark.SparkConfval conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"DefinitiveGuide\") .set(\"some.conf\", \"to.some.value\") SparkConf 객체는 개별 스파크 애플리케이션에 대한 스파크 속성값을 구성하는 용도로 사용한다. 애플리케이션의 동작 방식과 클러스터 구성 방식을 제어한다. 명령행 파라미터를 통해 런타임에 구성할 수 있다./bin/spark.submit --name \"DefinitiveGuide\" --master local[4]시간 주기 형태 속성값(포맷) 25ms, 5s, 10m or 10min, 3h, 5d, 1y16.5.2 애플리케이션 속성16.5.3 런타임 속성 드물지만 애플리케이션의 런타임 환경을 설정해야 할수도 있다. 관련 속성을 사용해 드라이버와 익스큐터를 위한 추가 클래스트패스와 파이썬패스, 파이썬 워커 설정, 다양한 로그 관련 속성을 정의할 수 있다. 자세한 내용은 공식 문서 참고16.5.4 실행 속성 실행 속성과 관련된 설정값은 실제 처리를 더욱 세밀하게 제어할 수 있기 때문에 자주 사용된다. spark.executor.cores(익스큐터의 코어수) spark.files.maxPartitionsBytes (파일 읽기 시 파티션의 최대크기)16.5.5 메로리 관리 설정 앱 최적화를 위해 메모리 옵션을 수동으로 관리해야 하는 경우도 있다. 대다수의 메모리 설정은 메모리 자동 관리 기능의 추가로 인해 스파크 2.x 버전에서는 제거된 예전 개념이기는 하다. 세밀한 제어를 위한 설정이기 때문에 실제로 사용자는 이를 신경쓰지 않아도 된다.16.5.6 셔플 동작방식 설정 셔플 동작 방식을 제어하기 위한 고급 설정이 존재한다는 점까지만 참고하라.16.5.7 환경변수 스파크가 설치된 디렉터리의 conf/spark-env.sh 파일에서 읽은 환경변수로 특정 스파크 설정을 구성할 수 있다. 위 목록 외에도 각 머신이 사용할 코어 수나 최대 메모리 크기 같은 스파크 스탠드얼론 클러스터 설정과 관련된 옵션 등이 있다.16.5.8 애플리케이션에서 잡 스케줄링 별도의 스레드를 사용해 여러 잡을 동시에 실행할 수 있다. 스파크의 스케줄러는 스레드 안정성을 보장하고, 여러 요청을 동시에 처리할 수 있는 애플리케이션으로 만들어 줄수 있다. 여러 스파크 잡이 자원을 공평하게 나눠 쓰도록 구성할 수도 있다.페어 스케줄러 SparkContext 설정시 spark.scheduler.mode 속성을 FAIR로 지정해서 사용할 수 있다. 여러 개의 잡을 풀로 그룹화하는 방식도 지원한다. 개별 풀에 다른 스케줄링 옵션이나 가중치를 설정할 수 있다. 더 중요한 스파크 잡을 할당할 수 있도록 우선순위가 높은 풀을 만들수 있음. 하둡의 페어 스케줄러 모델을 본떠서 만들었음.Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 15. 클러스터에서 스파크 실행하기", "url": "/posts/spark-guide-15/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2023-02-04 19:21:00 +0900", "snippet": "[스파크 완벽 가이드] 15. 클러스터에서 스파크 실행하기 스파크 애플리케이션의 아키텍처와 컴포넌트스파크 내/외부에서 실행되는 스파크 애플리케이션 생애주기파이프라이닝과 같은 중요한 저수준 실행 속성스파크 애플리케이션을 실행하는 데 필요한 사항15.1 스파크 애플리케이션의 아키텍쳐스파크 드라이버 스파크 애플리케이션의 운전자 역할 프로세스 스파크 애...", "content": "[스파크 완벽 가이드] 15. 클러스터에서 스파크 실행하기 스파크 애플리케이션의 아키텍처와 컴포넌트스파크 내/외부에서 실행되는 스파크 애플리케이션 생애주기파이프라이닝과 같은 중요한 저수준 실행 속성스파크 애플리케이션을 실행하는 데 필요한 사항15.1 스파크 애플리케이션의 아키텍쳐스파크 드라이버 스파크 애플리케이션의 운전자 역할 프로세스 스파크 애플리케이션의 실행을 제어하고, 클러스터의 모든 상태 정보를 유지한다. 클러스터 내 물리적 컴퓨팅 자원 확보와 익스큐터 실행을 위해 클러스터 매니저와 통신할 수 있어야 한다. 실제 정의한 스파크 태스크 코드를 실행하는 주체라고 이해 하면 될듯 하다.리소스가 사용되는것은 스파크 클러스터의 익스큐터이겠지만, 이 명령을 요청하는 클라이언트 애플리케이션이라고 이해하면 됨.예를 들면 제플린과 같은 BI툴이 될수도 있고, 사용자가 작성한 애플리케이션 코드(web application일수도, batch application일수도 있음)라고 생각하면 됨.스파크 익스큐터 스파크 드라이버가 할당한 태스크를 수행하는 프로세서 드라이버가 할당한 태스크를 받아 실행하고 태스크의 상태와 결과를 드라이버에 보고한다.클러스터 매니저 스파크 드라이버와 익스큐터를 관리하는 클러스터 매니저가 필요하다, 클러스터 매니저를 드라이버 혹은 마스터라고 부르기도 한다. 스파크 애플리케이션이 실제로 실행할 때가 되면 클러스터 매니저에 자원 할당 요청을 한다.스파크가 지원하는 클러스터 매니저 종류 스탠드얼론 클러스터 매니저 아파치 메소스 하둡 YARN15.1.1 실행 모드 실행 모드는 애플리케이션을 실행할 때 요청한 자원의 물리적인 위치를 결정한다.클러스터 모드 가장 흔하게 사용되는 실행 방식 컴파일된 JAR 파일이나 파이썬 스크립트를 클러스터 매니저에게 전달하고, 매니저는 이 파일을 받아 워커 노드에 드라이버와 익스큐터 프로세스를 실행하는 방식이다.클라이언트 모드 애플리케이션을 제출한 클라이언트 머신에 스파크 드라이버가 위치한다는것을 제외하면 클러스터 모드와 비슷하다. 클라이언트 내에 스파크 드라이버 프로세스를 유지하며 매니저는 익스큐터 프로세스를 유지한다. 스파크 앱이 클러스터와는 무관한 머신에서 동작할 수 있음. 이런 머신을 게이트웨이 머신 또는 에지 노드라고 부른다. 로컬 모드 모든 스파크 애플리케이션이 단일 머신에서 실행되는 구조 병렬 처리는 머신의 스레드를 활용하여 처리된다. 개발 테스트 및 학습용에서 쓰이는 방식15. 2 스파크 애플리케이션의 생명 주기(스파크 외부)15.2.1 클라이언트 요청 첫 단계는 스파크 애플리케이션을 제출하는 것. 컴파일된 JAR나 라이브러리 파일 등 15.2.2 시작 드라이버 프로세스가 클러스터에 배치되면 사용자 코드를 실제로 실행할 차례이다. 사용자 코드에는 반드시 스파크 클러스터를 초기화하는 SparkSession이 포함되어야 한다. 클러스터 매니저는 익스큐터 프로세스를 시작하고 결과를 응답받아 익스큐터 위치와 관련 정보를 드라이버 프로세스로 전송한다.15.2.3 실행 클러스터 처리를 위한 준비가 끝나고 이제 실제 코드가 실행된다. 드라이버와 워커는 코드를 실행하고, 데이터를 이동하는 과정에서 서로 통신한다. 드라이버는 각 워커에 태스크를 할당하고, 워커는 태스크의 상태와 성공/실패 여부를 드라이버로 전송한다.15.2.4 완료 실행이 완료되면 드라이버 프로세스가 성공/실패 중 하나의 상태로 종료된다. 그 이후 클러스터 매니저는 드라이버가 속한 클러스터의 모든 익스큐터를 종료시킨다.15.3 스파크 애플리케이션의 생애주기(스파크 내부) 스파크 앱은 하나 이상의 스파크 잡으로 구성된다. 스레드를 사용해 여러 액션을 병렬로 수행하는 경우가 아니라면 스파크 잡은 차례대로 실행된다.15.3.1 SparkSession 모든 스파크 앱은 가장 먼저 SparkSession을 생성한다. 대화형 모드에서는 자동으로 생성됨. 개발시 빌더 메소드를 사용해 생성하는 것을 추천함. import org.apache.spark.sql.SparkSessionval spark = SparkSession.builder().appName(\"Databricks Spark Example\") .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") .getOrCreate()SparkContext SparkSession의 SparkContext는 스파크 클러스터에 대한 연결을 나타낸다. 이를 이용해 RDD 같은 스파크 저수준 API를 사용할 수 있다. RDD, 어큐뮬레이터, 브로드캐스트 변수를 생성하고 코드를 실행할 수 있음. SparkSession을 통해 SparkContext에 접근할 수 있으므로 명시적으로 초기화하지 않고, getOrCrete()을 사용한다.import org.apache.spark.SparkContextval sc = SparkContext.getOrCreate()15.3.2 논리적 명령 사용자는 SQL, 저수준 RDD처리, 머신러닝 알고리즘 등을 사용해 트랜스포메이션과 액션을 마음대로 구성할 수 있다. 하지만, 스파크 코드는 트랜스포메이션과 액션으로만 구성된다는 점에서 실제 내부동작은 동일하다. 논리적 명령이 물리적 실행계획으로 어떻게 변환되는지 이해하는 것이 중요하다.논리적 명령을 물리적 실행 계획으로 변환하기 스파크가 사용자 코드를 어떻게 받아들이고, 클러스터에 어떻게 명령을 전달하는지 다시한번 짚어보자.df1 = spark.range(2, 10000000, 2)df2 = spark.range(2, 10000000, 4)step1 = df1.repartition(5)step12 = df2.repartition(6)step2 = step1.selectExpr(\"id * 5 as id\")step3 = step2.join(step12, [\"id\"])step4 = step3.selectExpr(\"sum(id)\")step4.collect() # 2500000000000step4.explain() collect 같은 액션을 호출하면 개별 스테이지와 태스크로 이루어진 스파크 잡이 실행된다.15.3.3 스파크 잡 액션 하나당 하나의 스파크 잡이 생성되며, 액션은 항상 결과를 반환한다. 스파크 잡은 일련의 스테이지로 나뉘며 스테이지 수는 셔플 작업이 얼마나 많이 발생하는지에 따라 달라진다.15.3.4 스테이지 다수의 머신에서 동일한 연산을 수행하는 태스크의 그룹을 나타낸다. 스파크는 가능한 한 많은 태스크(잡의 트랜스포메이션)을 동일한 스테이지로 묶으려 노력한다. 셔플 작업이 일어난 다음에는 반드시 새로운 스테이지를 시작한다. 셔플은 데이터의 물리적 재분배 과정 조인(셔플) 수행시 스테이지 태스크 수가 증가할 수 있는 부분에 대해 유의하여야 한다. spark.sql.shuffle.partitions 기본값이 200인데, 잡 실행 도중 셔플이 수행되면 태스크 수가 200까지 늘어날 수 있는 것이다. 클러스터의 익스큐터 수보다 파티션 수를 더 크게 지정하는 것이 일반적으로 좋음 최종 스테이지 태스크 1개는 드라이버로 결과를 전송하기 전에 파티션마다 개별적으로 수행된 결과를 단일 파티션으로 모으는 작업을 수행하는 부분이다.15.3.5 태스크 스파크의 스테이지는 태스크로 구성된다. 각 태스크는 단일 익스큐터에서 실행할 데이터의 블록과 다수의 트랜스포메이션의 조합으로 볼수 있다. 데이터셋이 거대한 하나의 파티션인 경우 하나의 태스크만 생성된다. 1,000개의 작은 파티션으로 구성되어 있다면 1,000개의 태스크를 만들어 병렬로 실행할 수 있다. 태스크는 데이터 단위(파티션)에 적용되는 연산 단위를 의미한다. 파티션 수를 늘리면 높은 병렬성을 얻을수 있다. 최적화를 위한 가장 쉬운 방법 중 하나 15.4 세부 실행 과정 스파크의 스테이지와 태스크는 알아두면 좋을만한 중요한 특성을 가지고 있다. 첫째, 스파크는 map 연산 후 다른 map 연산이 이어진다면 함께 실행할 수 있도록 스테이지와 태스크를 자동으로 연결. 둘쨰, 스파카는 모든 셔플을 작업할 때 데이터를 안정적인 저장소(디스크)에 저장하므로 여러 잡에서 재사용할 수 있다.15.4.1 파이프라이닝 스파크를 인메모리 컴퓨팅 도구로 만들어주는 핵심 요소 중 하나는 메모리나 디스크에 데이터를 쓰기 전에 최대한 많은 단계를 수행한다는 점이다.(맵리듀스와 비교되는 차이점) 스파크가 수행하는 주요 최적화 기법 중 하나는 RDD나 더 RDD보다 더 아래에서 발생하는 파이프라이닝 기법이다. 노드 간의 데이터 이동 없이 각 노드가 데이터를 직접 가공할 수 있는 연산만 모아 태스크의 단일 스테이지를 만들어 준다.15.4.2 셔플 결과 저장 두번쨰 특성은 셔플 결과를 저장하는 것이다. reduceByKey 연산과 같이 노드간 복제를 유발하는 연산을 실행하면 엔진에서 파이프라이닝을 수행하지 못하므로 네트워크 셔플이 발생한다. 노드간 복제를 유발하는 연산은 각 키에 대한 입력데이터를 먼저 여러 노드로부터 복사하고, 셔플 파일을 로컬 디스크에 기록한다. 다만, 위와 같은 처리를 할때 소스와 관련된 셔플이 다시 실행되지 않고 재사용되기 때문에 사이드이펙트가 발생할 수 있음을 알고 있어야 한다. 이러한 자동 최적화 기능을 통해 동일한 데이터를 사용해 여러 잡을 실행하는 워크로드의 시간을 절약할 수 있다. 더 나은 성능을 얻기 위해 직접 DataFrame이나 RDD의 cache 메소드를 사용할 수도 있다.Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 14. 분산형 공유 변수", "url": "/posts/spark-guide-14/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2023-02-04 18:39:00 +0900", "snippet": "[스파크 완벽 가이드] 14. 분산형 공유 변수 브로드캐스트 변수어큐뮬레이터14.1 브로드캐스트 변수 브로드캐스트 변수는 변하지 않는 값(불변성 값)을 클로저 함수의 변수로 캡슐화하지 않고, 클러스터에서 효율적으로 공유하는 방법을 제공한다. 모든 태스크마다 직렬화하지 않고 클러스터의 모든 머신에 캐시하는 불변성 공유 변수 익스큐터 메모리 크기에...", "content": "[스파크 완벽 가이드] 14. 분산형 공유 변수 브로드캐스트 변수어큐뮬레이터14.1 브로드캐스트 변수 브로드캐스트 변수는 변하지 않는 값(불변성 값)을 클로저 함수의 변수로 캡슐화하지 않고, 클러스터에서 효율적으로 공유하는 방법을 제공한다. 모든 태스크마다 직렬화하지 않고 클러스터의 모든 머신에 캐시하는 불변성 공유 변수 익스큐터 메모리 크기에 맞는 조회용 테이블을 전달하고 함수에서 사용하는 것이 대표적인 예이다.val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\" .split(\" \")val supplementalData = Map(\"Spark\" -&gt; 1000, \"Definitive\" -&gt; 200, \"Big\" -&gt; -300, \"Simple\" -&gt; 100)// 클러스터 모든 노드에 지연 처리방식으로 복제 처리val suppBroadcast = spark.sparkContext.broadcast(supplementalData)// 브로드캐스트된 변수 참조suppBroadcast.value// 브로드캐스트된 데이터를 사용해 RDD 반환words.map(word =&gt; (word, suppBroadcast.value.getOrElse(word, 0))) .sortBy(wordPair =&gt; wordPair._2) .collect() 브로드캐스트 변수를 사용한 방식과 클로저에 담아 전달하는 방식의 차이점은 브로드캐스트 변수를 사용하는게 훨씬 더 효율적이라는 것이다. 데이터 총량과 익스큐터 수에 따라 다를수 있으나 브로드캐스트 변수에 아주 큰 데이터를 사용하는 경우 효율적이다. 전체 데이터를 직렬화하는 데 발생하는 부하가 커질수 있으므로 14.2 어큐뮬레이터 어큐뮬레이터는 스파크의 두 번째 공유 변수 타입이다. 트랜스포메이션 내부의 다양한 값을 갱신하는데 사용됩니다. 내고장성을 보장하면서 효율적인 방식으로 드라이버에 값을 전달할 수 있습니다. 결합성과 가환성을 가진 연산을 통해서만 더할 수 있는 변수이므로, 병렬 처리 과정에서 효율적으로 사용할 수 있다. 카운터나 합계를 구하는 용도로 사용 가능 액션을 처리하는 과정에서만 갱신됨에 유의해야 한다. 각 태스크에서 어큐뮬레이터 한번만 갱신하도록 제어한다. 재시작한 태스크는 값을 갱신할 수 없음. 스파크 지연 연산 모델에 영향을 주지않는것에 유의해야 한다. RDD 처리 중에 갱신되면 RDD 연산이 실제로 수행된 시점에 딱 한번만 값을 갱신한다. map 함수 같은 지연 처리 형태의 트랜스포메이션에서 어큐뮬레이터 갱신 작업을 수행하는 경우 실제 실행 전까지는 값이 갱신되지 않는다. 14.2.1 기본 예제import org.apache.spark.util.LongAccumulatorcase class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)val flights = spark.read .parquet(\"/data/flight-data/parquet/2010-summary.parquet\") .as[Flight]val accUnnamed = new LongAccumulatorval acc = spark.sparkContext.register(accUnnamed)val accChina = new LongAccumulatorval accChina2 = spark.sparkContext.longAccumulator(\"China\")spark.sparkContext.register(accChina, \"China\") 출발지나 도착지가 중국인 항공편의 수를 구하는 어큐뮬레이터를 생성.(SQL로도 처리할수 있음) 어큐뮬레이터를 사용하면 프로그래밍 방식으로 처리가 가능한다. sparkContext을 통해 간단히 생성할수 있고, 이름을 붙여서 사용할 수 있다. 이름을 붙인 경우 Spark UI에서 확인 가능(그 반대는 불가능) def accChinaFunc(flight_row: Flight) = { val destination = flight_row.DEST_COUNTRY_NAME val origin = flight_row.ORIGIN_COUNTRY_NAME if (destination == \"China\") { accChina.add(flight_row.count.toLong) } if (origin == \"China\") { accChina.add(flight_row.count.toLong) }}flights.foreach(flight_row =&gt; accChinaFunc(flight_row))// 수행 후 결과 확인accChina.value // 953 어큐뮬레이터는 액션에서만 실행을 보장하므로 foreach 메서드(액션)을 수행시킨다.14.2.2 사용자 정의 어큐뮬레이터 기본적으로 스파크에서는 수치와 관련된 유용한 어큐뮬레이터를 제공한다. 떄에 따라 사용자 정의 어큐뮬레이터가 필요할 수 있다. ACcumulatorV2클래스를 상속받아서 구현할 수 있다.(실제 사용시 최신버전 문서 확인 필요함, 책이 오래전이라..) 파이썬에서는 AccumulatorParam을 상속받아 구현 import scala.collection.mutable.ArrayBufferimport org.apache.spark.util.AccumulatorV2val arr = ArrayBuffer[BigInt]()class EvenAccumulator extends AccumulatorV2[BigInt, BigInt] { private var num:BigInt = 0 def reset(): Unit = { this.num = 0 } def add(intValue: BigInt): Unit = { if (intValue % 2 == 0) { this.num += intValue } } def merge(other: AccumulatorV2[BigInt,BigInt]): Unit = { this.num += other.value } def value():BigInt = { this.num } def copy(): AccumulatorV2[BigInt,BigInt] = { new EvenAccumulator } def isZero():Boolean = { this.num == 0 }}val acc = new EvenAccumulatorval newAcc = sc.register(acc, \"evenAcc\")acc.value // 0flights.foreach(flight_row =&gt; acc.add(flight_row.count))acc.value // 31390Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 13. RDD 고급 개념", "url": "/posts/spark-guide-13/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2022-12-31 19:43:00 +0900", "snippet": "[스파크 완벽 가이드] 13. RDD 고급 개념핵심 주제 집계와 키-값 형태의 RDD사용자 정의 파티셔닝RDD조인13.1 키-값 형태의 기초(키-값 형태의 RDD) 데이터를 키-값 형태로 다룰수 있는 메소드 ~byKey 류는 PairRDD 만 사용 가능 (ex. Pair(“s”, “Scala”))words.map(word =&gt; (word.t...", "content": "[스파크 완벽 가이드] 13. RDD 고급 개념핵심 주제 집계와 키-값 형태의 RDD사용자 정의 파티셔닝RDD조인13.1 키-값 형태의 기초(키-값 형태의 RDD) 데이터를 키-값 형태로 다룰수 있는 메소드 ~byKey 류는 PairRDD 만 사용 가능 (ex. Pair(“s”, “Scala”))words.map(word =&gt; (word.toLowerCase, 1))13.1.1 keyBy 현재 값으로부터 키 생성val keyword = words.keyBy(word =&gt; word.toLowerCase.toSeq(0).toString)13.1.2 값 매핑하기mapValues 스파크에서는 튜플인 경우 첫번째 요소를 키, 두번쨰 요소를 값으로 추정 튜플에서 값만 추출keyword.mapValues(word =&gt; word.toUpperCase).collect( )[\t('s', 'SPARK'),\t('t', 'THE'),\t('d', 'DEFINITIVE'),\t('g', ,'GUIDE'),\t(':' ':'),\t('b', 'BIG'),\t('d', 'DATA'),\t('p', 'PROCESSING'),\t('m', 'MADE'),\t('s', 'SIMPLE')]flatMapValueskeyword.flatMapValues(word =&gt;word.toUpperCase).collect()13.1.3 키와 값 추출하기 keys , valueskeyword.keys.collect()keyWord.values.collect()13.1.4 lookup 특정 키에 관한 결과 검색keyword.lookup(\"s\")// 키가 \"s\"인 Spark와 Simpe 반환13.1.5 sampleByKey 근사치나 정확도를 이용해서 키 기반 RDD 샘플 생성 간단한 무작위 샘플링을 사용import scala.util.Randomval distinctchars = words.flatMap(word =&gt; word.toLowerCase.toSeq).distinct.collect()// sampleByKeyval sampleMap = distinctChars.map(c =&gt; (c, new Random().nextDouble())).toMapwords\t.map(word =&gt; (word.toLowerCase.toSeq(0), word))\t.sampleByKey(true, sampleMap, 6L)\t.collect()// sampleByKeyExactval sampleByKeyExact = words\t.map(word =&gt; (word.toLowerCase.toSeq(0), word))\t.sampleByKeyExact(true, sampleMap, 6L)\t.collect() 99.99% 신뢰도를 가진 모든 키 값에 대해서 RDD를 추가로 처리하므로 math.ceil (numltems * samplingRate)의 합과 완전히 동일한 크기의 샘플 데이터를 생성13. 2 집계 RDD 또는 PairRDD를 사용해 집계 수행val chars = words.flatMap(word =&gt; word.toLowerCase.toSeq)val KVcharacters = chars.map(letter =&gt; (letter, 1))def maxFunc(left:Int, right:Int) = math.max(left, right)def addFunc(left:Int, right:Int) = left + rightval nums = sc.parallelize(1 to 30, 5)13.2.1 countByKey 각 키의 아이템 수를 구하고 맵으로 결과 반환 Scala 또는 java에서는 timeout과 신뢰도를 인수로 지정해 근사치를 구할 수 있음. val timeout = 1000L // 일리세컨드 단위val confidence = 0.95KVcharacters.countByKey()KVcharacters.countByKeyApprox(timeout, confidence)13.2.2 집계 연산 구현 방식 이해 키-값 형태의 PairRDD 생성하는 몇가지 방식groupByKeyKVcharacters\t.groupByKey()\t.map(row =&gt; (row._1, row._2.reduce(addFunc)))\t.count() 사용시 주의사항 모든 익스큐터에서 함수를 적용하기 전에 해당 키와 관련된 모든 값을 메모리에 올려야하는 문제 심각하게 치우쳐진 키가 있다면 일부 파티션에서 OOM이 발생할 수 있음. 각 키에 대한 값의 크기가 일정하고, 익스큐터에 할당된 메모리에서 처리 가능할 정도의 크기 인 경우에만 groupByKey 사용 reduceByKey 각 파티션에서 리듀스 작업을 수행하기 떄문에 훨씬 안정적이고, 모든 값을 메모리에 유지할 필요가 없음. 최종 리듀스 과정을 제외한 모든 작업은 개별 워커에서 처리하므로 연산 수행 속도도 향상될 수 있음.KVcharacters\t.reduceByKey(addFunc)\t.collect()// key별 그룹 RDD Array 반환// Array((d,4), (p,3), (t,3), (b,1), (h,1), (n,2), (a,4), (i,7), (k,1), (u,1), (o,1), (g,3), (m,2), (c,1)) 정렬되어 있지 않기 떄문에 작업 부하를 줄일수는 있으나, 순서가 중요한 경우 적합하지 않음.13.2.3 기타 집계 메서드 대부분은 구조적 API를 사용하면 간단히 집계를 수행할수 있음. 고급 집계 함수를 사용해 클러스터 노드에서 수행하는 집계를 구체적이고 세밀하게 제어 가능aggregatenums.aggregate(0)(maxFunc, addFunc)// nulll 또는 집계의 시작값 필요// 첫번쨰 함수 : 파티션 내에서 수행// 두번째 함수 : 모든 파티션에 걸쳐 수행 드라이버에서 최종 집계를 수행하므로 성능에 영향이 있음. executor의 결과가 너무 크면 OOM 발생할수 있음. treeAggreateval depth = 3nums.treeAggregate(0)(maxFunc, addFunc, depth) treeAggregate 를 이용하면 처리과정은 다르지만 같은 결과를 얻을 수 있음. executor끼리 트리를 형성해 집계 처리의 일부 하위 과정을 push down 방식으로 먼저 수행 aggregateByKey aggregate 함수와 동일하지만 파티션 대신 키를 기준으로 연산 수행KVcharacters\t.aggregateByKey(0)(addFunc, maxFunC)\t.collect()combineByKey 집계 함수 대신 comniner를 사용 key를 기준으로 연산을 수행하고, 파라미터로 사용된 함수에 따라 값을 병합 val valToCombiner = (value:Int) =&gt; List(value)val mergeValuesFunc = (vals: List[Int], valToAppend: Int) =&gt; valToAppend :: valsval mergeCombinerFunc = (vals1: List[Int], vals2:List[Int]) =&gt; vals1 ::: vals2// 함수형 변수 정의도 가능val outputPartitions = 6val result = KVcharacters\t.combineByKey(\t\tvalToCombiner,\t\tmergeValuesFunc,\t\tmergeCombinerFunc,\t\toutputPartitions\t)\t.collect()foldByKey 결합 함수와 항등원(neutral)인 제로값을 이용해 각 키의 값을 병합KVcharacters\t.foldByKey(0)(addFunc)\t.collect()13.3 cogroup RDD에 대한 그룹 기반의 조인을 수행 스칼라 : 3개, 파이썬 2개의 키-값 형태의 RDD를 그룹화할 수 있음. 출력 파티션 수나 클러스터에 데이터 분산 방식 제어를 위한 사용자 정의 파라미터 제공 import scala.util.Randomval distinctChars = words\t.flatMap(word =&gt; word.toLowerCase.toSeq)\t.distinctval charRDD = distinctChars.map(c =&gt; (c, new Random().nextDouble()))val charRDD2 = distinctChars.map(c =&gt; (c, new Random().nextDouble()))val charRDD3 = distinctChars.map(c =&gt; (c, new Random().nextDouble()))charRDD\t.cogroup(charRDD2, charRDD3)\t.take(5)13.4 JOIN 구조적 API와 동일한 조인 방식을 가지고 있지만, RDD를 사용하면 사용자가 많은 부분을 관여해야 함. 출력 파티션 수나 사용자 정의 파티션 함수 파라미터 사용 13.4.1 INNER JOINval keyedChars = distinctChars.map(C =&gt; (C, new Random().nextDouble()))val outputPartitions = 10KVcharacters.join(keyedChars).count()KVcharacters.join(keyedChars, outputPartitions).count() 이 외의 조인들도 동일한 기본 형식을 따름13.4.2 zip 실제로 JOIN은 아니고 두 개의 RDD를 결합하는 방식 동일한 길이의 2개의 RDD를 zipper를 잠그듯이 연결하여 PairRDD 생성 요소와 파티션수가 일치해야 함. val numRange = sc.parallelize(0 to 9, 2)words.zip(numRange).collect()[\t('Spark', 0),\t('The', 1),\t('Definitive', 2),\t('Guide', 3),\t(':', 4),\t('Big', 5),\t('Data', 6),\t('ProceSsing', 7),\t('Made', 8),\t('Simple', 9)]13.5 파티션 제어하기 RDD를 사용하면 데이터가 클러스터 전체에 물리적으로 정확히 분산되는 방식으로 정의 할 수 있음. 기본적으로 구조적 API와 동일구조적 API와 차이점 파티션 함수를 파라미터로 사용할 수 있음. 사용자 지정 Partitioner 13.5.1 coalesce 파티션을 재분배할 떄 발생하는 데이터 셔플을 방지하기 위해 동일한 워커에 존재하는 파티션을 합치는 메서드 2개의 파티션으로 구성된 words RDD를 셔플링 없이 하나의 파티션으로 합친다. words.coalesce(1).getNumPartitions // 값은113.5.2 repartition 파티션 수를 늘리거나 줄일 때 사용(노드간 셔플 발생할 수 있음) 파티션 수를 늘리면 필터 map or filter 타입의 연산을 수행할때 병렬 처리 수준을 높일 수 있음 words.repartition(10) // 10개의 파티션이 생성됩니다13.5.3 repartitionAndSortWithinPartitions 파티션을 재분배할 수 있고, 재분배된 결과 파티션의 정렬 방식을 지정 partition연산을 할때 key에대해서 sorting https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html#repartitionAndSortWithinPartitions-org.apache.spark.Partitioner 문서는 보기 좋지 않아보임. 셔플링 단계에서 정렬 작업을 함께 수행하기 때문에 repartition을 호출한 후 직접 정렬하는 것보다 성능이 더 좋음13.5.4 사용자 정의 파티셔닝 RDD를 사용하는 가장 큰 이유 중 하나 구조적 API에서는 사용자 정의 파티셔너를 사용할 수 없음. 저수준 API의 세부적인 구현 방식 데이터 치우짐 문제를 피하고자 클러스터 전체에 데이터를 균등하게 분배하는 목적으로 사용될 수 있음. 구조적 API로 RDD를 얻고, 사용자 정의 파티셔너를 적용한 뒤 다시 DataFrame 또는 Dataset으로 변환하여 사용 구조적 API와 RDD 장점을 모두 활용 가능 Partitioner를 확장한 클래스를 직접 구현해야 하므로 문제에 대한 업무 지식을 충분히 가진 경우에만 사용 단일 값이나 다수 값(다수 컬럼)을 파티셔닝해야 한다면 DataFrame API를 사용하는 것이 좋음. val df = spark.read\t.option(\"header\", \"true\")\t.option(\"inferSchema\", \"true\")\t.csv(\"/data/retail-data/all/\")val rdd = df.coalesce(10).rddHashPartitioner, RangePartitoner RDD API에서만 사용할 수 있는 내장형 이산형과 연속형 값을 다룰 때 사용import org.apache.spark.HashPartitionerrdd\t.map(r =&gt; r(6))\t.take(5)\t.foreach(println)val keyedRDD = rdd\t.keyBy(row =&gt; row(6).asInstanceOf[Int].toDouble)keyedRDD\t.partitionBy(newHashPartitioner(10))\t.take(10) 이는 유용하지만 매우 기초적인 기능을 제공. 매우 큰 데이터나 심각하게 치우친 키를 다뤄야 한다면 고급 파티셔닝 기능을 사용해야 함. 키 치우침 현상 특정 키가 다른 키들에 비해 아주 많은 데이터를 가지는 현상 병렬성을 개선하고 실행과정에서 OOM을 방지하기 위해서는 키를 최대한 분할해야 한다.Class DomainPartitioner extends Partitioner {\tdef numPartitions = 3\tdef getPartition(key: Any): Int = {\t\tval customerld = key.aslnstanceOf[Double].tolnt\t\tif (customerld == 17850.0 || customerld == 12583.0) {\t\t\treturn 0\t\t} else {\t\t\treturn new java.util.Random().nextlnt(2) + 1\t\t}\t}}keyedRDD\t.partitionBy(new DomainPartitioner)\t.map(_._1)\t.glom()\t.map(_.toSet.toSeq.length)\t.take(5) 사용자 정의 키 분배 로직은 RDD 수준에서만 사용 가능. 임의의 로직을 사용해 물리적인 방식으로 클러스터에 데이터를 분배하는 강력한 방법 13.6 사용자 정의 직렬화Kryo 직렬화 병렬화 대상인 모든 객체나 함수는 직렬화할 수 있어야 한다.class SomeClass extends Serializable {\tvar someValue = 0\tdef setSomeValue(i:Int) = {\t\tsomevalue= i\t\tthis\t}}sc.parallelize(1 to 10)\t.map(num =&gt; new SomeClass().setSomeValue(num)) 기본 직렬화는 매우 느리다. Kryo를 사용해 빠르게 객체를 직렬화 할 수 있다. Java 직렬화보다 10배 이상 성능이 좋음. Job 초기화시 spark.serializer=org.apache.Spark.Serializer.Kryo5erializer 를 설정 Spark 2.0.0부터 단순 데이터타입, 배열, 문자열 데이터 타입의 RDD 셔플링시 내부적으로 Kryo Serializer 사용 val conf = new SparkConf()\t.setMaster(...)\t.setAppName(...)conf.registerKryoClasses(Array(\tclassOf[MyClass1],\tclassOf[MyCIass2]))val sc = new SparkContext(conf)Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 12. RDD", "url": "/posts/spark-guide-12/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2022-12-31 19:34:00 +0900", "snippet": "[스파크 완벽 가이드] 12. RDD 대부분의 상황에서는 구조적 API를 사용하는것이 좋음. 하지만 모든 비즈니스나 기술적 문제를 고수준 API(구조적 API)를 사용해 해결할수 있지는 않기 때문에는 저수준 API인 RDD를 제공한다.12.1 저수준 API란? 분산 데이터 처리를 위한 RDD, 브로드캐스트 변수와 어큐뮬레이터처럼 분산형 공유 변수...", "content": "[스파크 완벽 가이드] 12. RDD 대부분의 상황에서는 구조적 API를 사용하는것이 좋음. 하지만 모든 비즈니스나 기술적 문제를 고수준 API(구조적 API)를 사용해 해결할수 있지는 않기 때문에는 저수준 API인 RDD를 제공한다.12.1 저수준 API란? 분산 데이터 처리를 위한 RDD, 브로드캐스트 변수와 어큐뮬레이터처럼 분산형 공유 변수를 배포하고 다루는 API 이렇게 2가지가 있다.12.1.1 저수준 API는 언제 사용할까? 고수준 API에서 제공하지 않는 기능이 필요한 경우(클러스터의 물리적 데이터 배치를 세밀하게 제어해야 하는 경우) RDD를 사용해 갭라된 기존 코드를 유지보수하는 경우 사용자가 정의한 공유 변수를 다뤄야 하는 경우(14장) 위와 같은 특정 상황이 아니라면 저수준 API는 사용하지 말도록 하자.스파크 모든 워크로드가 저수준 기능을 사용하는 기초 형태로 컴파일되므로 이를 이해하는것은 중요하지만 이걸로 개발하는게 항상 좋은것을 의미하지 않는다.12.1.2 저수준 API는 어떻게 사용할까? SparkContext는 저수준 API 기능을 사용하기 위한 진입점이다. SparkSession을 이용해 SparkContext에 접근할 수 있다. spark.sparkContext12.2 RDD 개요 스파크 1.x 버전의 핵심 API 2.x에서도 사용할 수 있지만 잘 사용핮 ㅣ않는다. 스파크에서 실행한 모든 DataFrame이나 Dataset 코드는 RDD로 컴파일 된다. RDD는 불변성을 가지며 병렬로 처리할 수 있는 파티셔닝된 레코드의 모음 개발자가 강력한 제어권을 가질 수 있으나, 모든 값을 다루거나 값 사이의 상호작용 과정을 반드시 수동으로 정의해야 한다는 단점이 있다. 구조적 API에서 자동으로 데이터를 최적화하고 압축된 바이너리 포맷으로 저장하는걸 저수준 API에서는 동일한 공간 효율/성능을 얻기 위해 직접 포맷을 구현해 모든 연산과정에 사용해야 한다. 재정렬/집계 같은 최적화 기법도 직접 구현해야 함. 스파크의 구조적 API를 사용할것을 강력히 권고12.2.1 RDD 유형 RDD에 수많은 하위 클래스들이 존재함. RDD는 DataFrame API에서 최적화된 물리적 실행계획을 만드는데 대부분 사용됨. 사용자는 젠리기 RDD 타입, 키 기반의 집계가 가능한 키-값 RDD를 만들수 있다.RDD 정의 파티션의 목록 각 조각을 연산하는 함수 다른 RDD와의 의존성 목록 부가적으로 키-값 RDD를 위한 Partitioner 부가적으로 각 조각을 연산하기 위한 기본 위치 목록(hdfs 파일 블록 위치)자바/스칼라가 아닌 언어에서 RDD를 다루는 경우 주사항 파이썬에서 RDD를 다루는 경우 상당한 성능 저하가 발생할수 있음. 파이썬으로 RDD를 실행하는 것은 파이썬으로 만들어진 사용자 정의 함수를 사용해 로우마다 적용하는 것과 동일 12.2.2 RDD는 언제 사용할까? 정말 필요한 경우가 아니라면 수동으로 RDD를 생성해서는 안된다. 구조적 API가 제공하는 여러 최적화 기법을 사용할 수 없다. DataFrame은 RDD보다 더 효율적이고 안정적이며 표현력이 좋다. 물리적으로 분산된 데이터(자체적으로 구성한 데이터 파티셔닝)에 세부적인 제어가 필요한 경우에 RDD를 사용하는것이 적합하다.12.2.3 Dataset과 RDD의 케이스 클래스 Dataset과 케이스 클래스를 사용해서 만들어진 RDD의 차이점은 구조적 API가 제공하는 풍부한 기능과 최적화 기법을 제공하느냐 아니냐가 가장 큰 차이점이다. Dataset을 사용하면 JVM 데이터 타입과 스파크 데이터 타입 중 어떤것을 쓸지 고민하지 않아도 된다. 어떤것을 사용하더라도 동일한 성능을 보장한다. 12.3 RDD 생성하기12.3.1 DataFrame, Dataset으로 RDD 생성// 스칼라 버전: converts a Dataset[Long] to RDD[Long]spark.range(500).rddspark.range(10).toDF().rdd.map(rowObject =&gt; rowObject.getLong(0))spark.range(10).rdd.toDF() rdd 메서드는 Row 타입을 가진 RDD를 생성한다. 이를 이용해 상황에 따라 구조적 API / 저수준 API를 오고가게 만들수 있다.12.3.2 로컬 컬렉션으로 RDD 생성하기 SparkSession안에 있는 sparkContext의 parallelize 메서드를 호출해서 컬렉션 객체를 RDD로 만들수 있다. 단일 노드에 있는 컬렉션을 병렬 컬렉션으로 전환하고, 이때 파티션 수를 명시적으로 지정할 수 있다. val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\" .split(\" \")val words = spark.sparkContext.parallelize(myCollection, 2)// RDD에 이름 지정words.setName(\"myWords\")words.name // myWords12.3.3 데이터소스로 RDD 생성하기 데이터소스나 텍스트 파일을 이용해 RDD를 직접 생성할 수 있다. DataSource API를 사용하는것이 바람직하나, RDD에는 이런 개념이 없다. spark.sparkContext.textFile(\"/some/path/withTextFiles\")spark.sparkContext.wholeTextFiles(\"/some/path/withTextFiles\")12.4 RDD 다루기 RDD는 스파크 데이터 타입 대신 자바나 스칼라의 객체를 다룬다는 사실이 가장 큰 차이점이다. 연산을 단순화하는 헬퍼 메서드나 함수가 구조적 API에 비해 많이 부족하다.(사용자가 직접 정의해야 함)12.5 트랜스포메이션 대부분의 RDD 트랜스포메이션은 구조적 API에서 사용할 수 있는 기능을 가지고 있다.12.5.1 distincdt RDD에서 중복된 데이터 제거words.distinct().count()12.5.2 filter SQL의 where 조건절을 생성하는것과 비슷하다.def startsWithS(individual:String) = { individual.startsWith(\"S\")}words.filter(word =&gt; startsWithS(word)).collect()12.5.3 map과 flatMapmap 주어진 입력을 원하는 값으로 변환하는 함수를 명시하고, 레코드별로 적용할 수 있다.val words2 = words.map(word =&gt; (word, word(0), word.startsWith(\"S\")))// 3번째 반환값으로 필터링words2.filter(record =&gt; record._3).take(5)flatMap map의 확장 버전 단일 로우를 여러 로우로 변환해야 하는 경우// word를 character 집합으로 반환words.flatMap(word =&gt; word.toSeq).take(5)12.5.4 sortBy RDD 정렬을 위해 sortBy 를 사용해야 한다.words.sortBy(word =&gt; word.length() * -1).take(2)12.5.5 randomSplit RDD를 임의로 분할하여 RDD 배열을 만들때 사용val fiftyFiftySplit = words.randomSplit(Array[Double](0.5, 0.5))12.6 액션 DataFrame과 Dataset에서 했던 것처럼 지정된 트랜스포메이션 연산을 시작하려면 액션을 사용해야 한다. 액션은 데이터를 드라이버로 모으거나 외부 데이터소스로 내보내는것을 의미12.6.1 reduce RDD의 모든 값을 하나의 값으로 만들려면 reduce를 사용해야 항다.spark.sparkContext.parallelize(1 to 20).reduce(_ + _) // 210// 단어 집합에서 가장 긴 단어를 찾는 functiondef wordLengthReducer(leftWord:String, rightWord:String): String = { if (leftWord.length &gt; rightWord.length) return leftWord else return rightWord}words.reduce(wordLengthReducer) 파티션에 대한 리듀스 연산은 비결정적 특성을 가진다. 위 예제에서 길이가 10인 단어가 중복될 경우 실행할때마다 결과가 다를 수 있음. 길이가 10인 definitive 나 processing 중 하나가 leftWord 변수에 할당될 수 있음. 12.6.2 count RDD의 전체 로우 수 반환words.count()countApprox 반환 결과는 조금 이상해보일 수 있지만 꽤 정교한 편 count 함수의 근사치를 제한시간 내에 계산할 수 있음.(불완전한 결과를 반환할 수 있다.) 신뢰도(confidence)는 실제로 연산한 결과와의 오차율val confidence = 0.95val timeoutMilliseconds = 400words.countApprox(timeoutMilliseconds, confidence)countApproxDistinct// 정확도 파라미터를 기준으로 카운팅을 추정함.(0.000017보다 커야함)words.countApproxDistinct(0.05)// 정밀도와 희소 정밀도를 받아서 카운팅 추정words.countApproxDistinct(4, 10)countByValue RDD의 개수를 구한다. 결과 데이터셋을 드라이버의 메모리로 읽어들여 처리함. 익스ㅠ터의 연산 결과가 드라이버 메모리에 모두 적재되므로 결과가 작은 경우에만 사용해야 한다.words.countByValue()countByValueApprox count 함수와 동일한 연산을 수행하지만 근사치를 계산한다.// 신뢰도 95%words.countByValueApprox(1000, 0.95)12.6.3 first 데이터셋의 첫번째 값을 반환words.first()12.6.4 max / min 최댓값과 최솟값 반환spark.sparkContext.parallelize(1 to 20).max()spark.sparkContext.parallelize(1 to 20).min()12.6.5 take RDD에서 가져올 값의 개수를 지정하여 조회 고정 크기의 임의 표본 데이터를 얻기 위한 메소드들도 제공함(takeSample)words.take(5)words.takeOrdered(5)words.top(5)val withReplacement = trueval numberToTake = 6val randomSeed = 100Lwords.takeSample(withReplacement, numberToTake, randomSeed)12.7 파일 저장하기 데이터 처리 결과를 일반 텍스트 파일로 쓰는것을 의미 RDD를 사용하면 일반적인 의미의 데이터소스에 저장할 수 없다.12.7.1 saveAsTextFilewords.saveAsTextFile(\"file:/tmp/bookTitle\")// 압축 코덱 지정import org.apache.hadoop.io.compress.BZip2Codecwords.saveAsTextFile(\"file:/tmp/bookTitleCompressed\", classOf[BZip2Codec])12.7.2 시퀀스 파일 시퀀스 파일은 바이너리 키-값 쌍으로 구성된 플랫 파일이며, 맵리듀스의 입출력 포맷으로 널리 사용되는 형태 saveAsObjectFile 메서드나 명시적인 키-값 쌍 데이터 저장방식을 이용해 시퀀스 파일을 만들수 있다.words.saveAsObjectFile(\"/tmp/my/sequenceFilePath\")12.7.3 하둡 파일 하둡 파일 포맷을 사용하면 클래스, 출력 포맷, 하둡 설정 그리고 압축 방식을 지정할 수 있다. 하둡 에코시스템이나 기존의 맵리듀스 잡을 깊이 있게 다루는 경우가 아니라면 상관 없음.12.8 캐싱 RDD 캐싱에도 DataFrame이나 Dataset의 캐싱과 동일한 원칙이 적용된다. 기본적으로 캐시와 저장은 메모리에 있는 데이터만을 대상으로 하고, setName 함수를 통해 캐시된 RDD에 이름을 지정할 수 있다.words.cache()// 저장소 수준 조회words.getStorageLevel12.9 체크포인팅 DataFrame API에서 사용할 수 없는 기능 중 하나인 체크포인팅 개념을 제공한다. 체크포인팅이란 RDD를 디스크에 저장하는 방식이다. 메모리 대신 디스크에 저장한다는 점만 다르고 캐싱과 유사하다. 반복적인 연산 수행시 유용하게 사용할 수 있다.spark.sparkContext.setCheckpointDir(\"/some/path/for/checkpointing\")words.checkpoint()12.10 RDD를 시스템 명령으로 전송하기 pipe 메소드를 사용하면 파이핑 요소로 생성된 RDD를 외부 프로세스로 전달할 수 있다. 외부 프로세스는 파티션마다 한 번씩 처리해 결과 RDD를 생성words.pipe(\"wc -l\").collect()12.10.1 mapPartitons 스파크는 코드를 실행할때 파티션 단위로 동작한다. map 함수에서 반환하는 RDD의 진짜 형태가 MapPartionsRDD이다. words.mapPartitions(part =&gt; Iterator[Int](1)).sum() // 2 파티션 그룹 전체 값을 단일 파티션으로 모으고 임의의 함수를 적용하고 제어할 수 있다.mapPartitonsWithIndex mapPartions와 유사하지만, 인덱스(파티션 범위의 인덱스)와 파티션의 모든 아이템을 순회하는 이터레이터를 가진 함수를 인수로 지정하여 처리 가능.// 인덱스 함수를 넣어 각 레코드가 속한 데이터셋이 어디인지 알아내는 디버깅용 함수 추가def indexedFunc(partitionIndex:Int, withinPartIterator: Iterator[String]) = { withinPartIterator.toList.map( value =&gt; s\"Partition: $partitionIndex =&gt; $value\").iterator}words.mapPartitionsWithIndex(indexedFunc).collect()12.10.2 foreachPartiton map과 달리 파티션의 모든 데이터를 순회만 하는 함수words.foreachPartition { iter =&gt; import java.io._ import scala.util.Random val randomFileName = new Random().nextInt() val pw = new PrintWriter(new File(s\"/tmp/random-file-${randomFileName}.txt\")) while (iter.hasNext) { pw.write(iter.next()) } pw.close()}12.10.3 glom glom 함수는 데이터셋의 모든 파티션을 배열로 변환하는 함수 데이터를 드라이버로 모으고 데이터가 존재하는 파티션의 배열이 필요한 경우 유용하다. 파티션이 크거나 파티션 수가 많다면 드라이버가 비정상 종료될 수 있기 때문에 사용시 주의 필요// 입력된 단어를 2개에 파티션에 개별적으로 할당spark.sparkContext.parallelize(Seq(\"Hello\", \"World\"), 2).glom().collect()// Array(Array(Hello), Array(World))Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 11. Dataset", "url": "/posts/spark-guide-11/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2022-12-31 18:34:00 +0900", "snippet": "[스파크 완벽 가이드] 11. Dataset Dataset은 구조적 API의 기본 데이터 타입이다. DataFrame은 Row 타입의 Dataset ( DataFrame == Dataset[Row] ) Dataset을 사용하면 Row 타입 대신 사용자가 정의한 데이터 타입을 분산 방식으로 다룰 수 있다.11.1 Dataset을 사용할 시기 Da...", "content": "[스파크 완벽 가이드] 11. Dataset Dataset은 구조적 API의 기본 데이터 타입이다. DataFrame은 Row 타입의 Dataset ( DataFrame == Dataset[Row] ) Dataset을 사용하면 Row 타입 대신 사용자가 정의한 데이터 타입을 분산 방식으로 다룰 수 있다.11.1 Dataset을 사용할 시기 Dataset을 사용하면 성능이 떨어진다는데 사용할 필요가 있을까?Dataset을 사용해야 하는 두가지 이유 DataFrame 기능만으로 수행할 연산을 표현할 수 없는 경우 성능 저하를 감수하더라도 타입 안정성(type-safe)을 가진 데이터 타입을 사용하고 싶은 경우구조적 API를 사용해 표현할수 없는 몇가지 작업 비즈니스 로직을 SQL이나 DataFrame 대신 단일 함수로 인코딩해야 하는 경우 두 문자열을 사용해 뺄셈 연산을 하는 것처럼 타입이 유효하지 않을떄 런타임 에러가 아닌 컴파일 단계에서 에러를 탐지하기 좋은 케이스 단일 노드의 워크로드와 스파크 워크로드에서 전체 로우에 대한 다양한 트랜스포메이션을 재사용하려는 경우 로컬과 분산환경의 워크로드 재사용 가능 경우에 따라서 더 적합한 워크로드를 만들기 위해 DataFrame과 Dataset을 동시에 사용해야 하는 경우가 있음. 성능과 타입 안정성 중 하나는 반드시 희생되어야 하는 트레이드오프 관계 11.2 Dataset 생성 정의할 스키마를 미리 알고 있어야 한다.11.2.1 자바: Encoders 데이터 타입 클래스를 정으한 다음에 DataFrame(Dataset 타입)에 지정해 인코딩 import org.apache.spark.Sql.Encoders;public class FIight implements Serializable{ String DEST COUNTRY=NAME; String 0RIGIN COUNTRY=NAME; Long DEST COUNTRY-NAME;}Dataset&lt;FIight&gt; flights = spark.read .parquet(\"/data/flight-data/parquet/2010-summary.parquet/\") .as(Encoders.bean(FIight.class));11.2.2 슴칼라: 케이스 클래스 스칼라에서 Dataset을 생성하려면 스칼라 case class 구문을 사용해 데이터 타입을 정의해야 한다.케이스 클래스의 특징 불변성 패턴 매칭으로 분해 가능 참조값 대신 클래스 구조를 기반으로 비교 사용하기 쉽고 다루기 편함.케이스 클래스의 장점 불변성이므로 객체들이 언제 어디서 변경되었는지 추적할 필요가 없다. 값으로 비교하면 인스턴스를 마치 원시(primitive) 데이터 타입의 값처럼 비교할 수 있다. 그러므로 클래스 인스턴스가 값으로 비교되는지, 참조로 비교되는지 불확실해하지 않아도 됨. 패턴 매칭은 로직 분기를 단수화해 버그를 줄이고 가독성을 좋게 한다.case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)val flightsDF = spark.read .parquet(\"/data/flight-data/parquet/2010-summary.parquet/\")val flights = flightsDF.as[Flight]11.3 액션 Dataset과 DataFrame에 collect, take, count와 같은 액션을 적용할 수 있다는 사실이 중요하다.flights.show(2)11.4 트랜스포메이션 Dataset의 트랜스포메이션은 DataFrame과 동일하다. Dataset을 사용하면 원형의 JVM 데이터 타입을 다루기 때문에 DataFrame만 사용해서 트랜스포메이션을 수행하는 것보다 좀 더 복잡하고 강력한 데이터 타입으로 트랜스포메이션을 사용할 수 있다.11.4.1 필터링// Flight 클래스를 파라미터로 사용해 Bollean 값을 반환하는 함수def originIsDestination(flight_row: Flight): Boolean = { return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME}flights.filter(flight_row =&gt; originIsDestination(flight_row)).first()flights.collect().filter(flight_row =&gt; originIsDestination(flight_row))11.4.2 맵핑 특정 값을 다른 값으로 맵핑 JVM 데이터 타입을 알고 있기 때문에 컴파일 타임에 데이터 타입 유효성 검사를 할수 있다.val destinations = flights.map(f =&gt; f.DEST_COUNTRY_NAME)val localDestinations = destinations.take(5)11.5 조인 DataFrame에서와 마찬가지로 Dataset에도 동일하게 적용된다. Dataset에서는 joinWith와 같은 정교한 메소드를 제공함.case class FlightMetadata(count: BigInt, randomData: BigInt)val flightsMeta = spark.range(500).map(x =&gt; (x, scala.util.Random.nextLong)) .withColumnRenamed(\"_1\", \"count\").withColumnRenamed(\"_2\", \"randomData\") .as[FlightMetadata]val flights2 = flights .joinWith(flightsMeta, flights.col(\"count\") === flightsMeta.col(\"count\"))flights2.selectExpr(\"_1.DEST_COUNTRY_NAME\")flights2.take(2)val flights2 = flights.join(flightsMeta, Seq(\"count\"))val flights2 = flights.join(flightsMeta.toDF(), Seq(\"count\")) 일반 조인도 아주 잘 동작하나, DataFrame을 반환하므로 JVM 데이터 타입 정보를 잃게 된다. 이 정보를 다시 얻으려면 Dataset을 정의해야 한다. DataFrame과 Dataset을 조인하는것에는 아무런 문제가 되지 않음.11.6 그룹화와 집계 Dataset을 가지고도 groupBy, rollup, cube 메서드를 모두 사용할 수 있다. Dataset 대신 DataFrame을 반환하므로 데이터 타입 정보를 잃게 됨.flights.groupBy(\"DEST_COUNTRY_NAME\").count()flights.groupByKey(x =&gt; x.DEST_COUNTRY_NAME).count()flights.groupByKey(x =&gt; x.DEST_COUNTRY_NAME).count().explain Dataset의 키를 이용해 그룹화를 수행한 다음 결과를 키-값 형태로 전달해 원시 객체 형태로 그룹화된 데이터를 다룰수 있다.def grpSum(countryName:String, values: Iterator[Flight]) = { values.dropWhile(_.count &lt; 5).map(x =&gt; (countryName, x))}flights.groupByKey(x =&gt; x.DEST_COUNTRY_NAME).flatMapGroups(grpSum).show(5)def grpSum2(f:Flight):Integer = { 1}flights.groupByKey(x =&gt; x.DEST_COUNTRY_NAME).mapValues(grpSum2).count().take(5)// 새로운 처리방법을 생성해 그룹을 축소(reduce)하는 방법 정의 가능def sum2(left:Flight, right:Flight) = { Flight(left.DEST_COUNTRY_NAME, null, left.count + right.count)}flights.groupByKey(x =&gt; x.DEST_COUNTRY_NAME).reduceGroups((l, r) =&gt; sum2(l, r)) .take(5) gorupByKey 메서드는 동일한 결과를 반환하지만, 데이터 스캔 직후에 집계를 수행하는 groupBy에 비해 더 비싼 처리를 한다.flights.groupBy(\"DEST_COUNTRY_NAME\").count().explain 사용자가 정의한 인코딩으로 세밀한 처리가 필요한 경우에만 Dataset의 groupByKey 메서드를 사용하는것이 좋음. Dataset은 빅데이터 처리 파이프라인의 처음과 끝 작업에서 주로 사용하는것이 좋다.Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 10. 스파크 SQL", "url": "/posts/spark-guide-10/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2022-12-31 17:02:00 +0900", "snippet": "[스파크 완벽 가이드] 10. 스파크 SQL10.1 SQL이란 SQL 또는 구조적 질의 언어(Structured Query Langauge)는 데이터에 대한 관계형 연산을 표현하기 위한 도메인 특화언어 스파크에서는 NoSQL DB에서도 쉽게 사용할 수 있는 변형된 자체 SQL을 제공합니다. 스파크에서는 ANSI SQL 2003의 일부도 구현함.1...", "content": "[스파크 완벽 가이드] 10. 스파크 SQL10.1 SQL이란 SQL 또는 구조적 질의 언어(Structured Query Langauge)는 데이터에 대한 관계형 연산을 표현하기 위한 도메인 특화언어 스파크에서는 NoSQL DB에서도 쉽게 사용할 수 있는 변형된 자체 SQL을 제공합니다. 스파크에서는 ANSI SQL 2003의 일부도 구현함.10.2 빅데이터와 SQL : 아파치 하이브 스파크가 등장하기 전에는 Hive가 빅데이터 SQL 접근 계층에서 사실상 표준이었음. 현재는 많은 사용자가 스파크 SQL을 사용함.10.3 빅데이터와 SQL : 스파크 SQL 스파크 2.0 버전에는 하이브를 지원할 수 있는 상위 호환 기능으로 ANSI-SQL과 HiveSL을 모두 지원하는 자체 개발된 SQL 파서가 포함되어 있음. 기존 하이브 기반 파이프라인에 비해 엄청난 성능 개선(4.5~6배 CPU 성능 개선, 3~4배의 자원 예약 개선, 최대 5배의 지연 시간 감소)를 만들어 냈음.10.3.1 스파크와 하이브의 관계 스파크 SQL은 하이브 메타스토어를 사용하므로 하이브와 잘 연동할 수 있다. 하이브 메타스토어는 여러 세션에서 사용할 테이블 정보를 보관. 스파크 SQL은 메타스토어에 접속하여 조회할 파일 수를 최소화 하기 위해 참조함. 하이브 메타스토어 스파크에서 spark.sql.hive.metastore.version 설정을 통해 접근 가능. HiveMetastoreClient 초기화 방식 변경을 위해서는 spark.sql.hive.metastore.jars을 변경해서 처리 가능10.4 스파크 SQL 쿼리 실행 방법10.4.1 스파크 SQL CLI 로컬 환경의 CLI에서 스파크 SQL 쿼리를 해볼수 있는 편리한 도구./bin/spark-sql10.4.2 스파크의 프로그래밍 SQL 인터페이스 서버를 설정해 SQl을 사용할수 있지만, 스파크에서 지원하는 언어 APi로 비정형 SQL을 실행할 수 있음. SparkSession 객체의 sql 메서드를 사용.spark.sql(\"SELECT 1 + 1\").show()spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, sum(count)FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\"\"\") .where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` &gt; 10\") .count() // SQL =&gt; DF 쿼리 명령을 수행하면 프로그래밍 방식으로 평가할 수 있는 DataFrame을 반환함. 스파크SQL도 마찬가지로 즉시 실행되는것이 아니라 지연처리됨. SQL과 DataFrame은 완벽히 연동 가능함. DataFrame을 생성하고 SQl을 사용해 처리하고 그 결과를 다시 DataFrame으로 받을수 있음. 10.4.3 스파크 SQL, 쓰리프트 JDBC/ODBC 서버 스파크는 자바 DB 연결(JDBC) 인터페이스를 제공함. 비즈니스 분석가가 태블로 같은 BI 소프트웨어를 이용해 스파크에 접속하는 혀애가 대표적인 케이스 쓰리프트 JDBC/oDBC 서버는 하이브 1.2.1 버전의 HiveServer2에 맞추어 구현되어있음.10.5 카탈로그 스파크 SQL에서 가장 높은 추상화 단계는 카탈로그입니다. 테이블에 저장된 데이터의 메타데이터 뿐만 아니라 DB, Table, Function, View에 대한 정보를 추상화 org.apache.spark.sql.catelog.Catalog 패키지로 사용10.6 테이블 스파크 SQL을 사용해 작업을 수행하려면 먼저 테이블을 정의해야함. DataFrame과 논리적으로 동일함. 조인, 필터링, 집계 등 여러 데이터 변환 작업을 수행할 수 있다. DatFrame은 프로그래밍 언어 레벨에서 정의할수 있으나, 테이블은 데이터베이스상에 정의해야 함.10.6.1 스파크 관리형 테이블 관리형 테이블과 외부 테이블 개념을 기억해야 함.외부 테이블 디스크에 저장된 파일을 용해서 테이블을 정의하는 경우 해당됨.관리형 테이블 DataFrame의 saveAsTable 메서드를 이용해서 스파크가 관련된 모든 정보를 추적할 수 있는 관리형 테이블을 만들수 있음.10.6.2 테이블 생성하기 다양한 데이터 소스를 사용해 테이블을 생성할 수 있음.CREATE TABLE flights ( DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)USING JSON OPTIONS (path '/data/flight-data/json/2015-summary.json')USING과 STORED AS 구문 USING 구문은 매우 중요함. 포맷을 지정하지 않으면 기본적으로 하이브 SerDe설정을 사용한다. 하이브의 SerDe는 스파크의 자체 직렬화보다 훨씬 느리므로 테이블을 사용하는 Reader, Writer성능에 영향을 미칠 수 있음. 하이브 사용자는 STORED AS 구문을 통해 하이브 테이블을 생성할 수 있음.COMMENTCREATE TABLE flights_csv ( DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING COMMENT \"remember, the US will be most prevalent\", count LONG)USING csv OPTIONS (header true, path '/data/flight-data/csv/2015-summary.csv')CREATE TABLE FROM SELECTCREATE TABLE flights_from_select USING parquet AS SELECT * FROM flights테이블이 없는 경우에만 생성CREATE TABLE IF NOT EXISTS flights_from_select AS SELECT * FROM flights파티셔닝된 데이터셋을 저장해 데이터 레이아웃을 제어CREATE TABLE partitioned_flights USING parquet PARTITIONED BY (DEST_COUNTRY_NAME)AS SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 510.6.3 외부 테이블 생성하기 스파크 SQL은 초기 빅데이터 SQL시스템 중 하나인 하이브 SQl과 완벽하게 호환됨. 기존 하이브 쿼리문을 스파크 SQL로 변환해야 하는 경우 대부분 문제없이 바로 사용할수 있음.CREATE EXTERNAL TABLE hive_flights ( DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/data/flight-data-hive/'SELECT로부터 생성CREATE EXTERNAL TABLE hive_flights_2ROW FORMAT DELIMITED FIELDS TERMINATED BY ','LOCATION '/data/flight-data-hive/' AS SELECT * FROM flights10.6.4 테이블에 데이터 삽입하기 표준 SQL 문법을 따름.INSERT INTO flights_from_select SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 20특정 파티션에만 저장하고 싶은 경우 파티션 명세 추가하여 가능. 쓰기 연산은 파티셔닝 스키마에 맞게 데이터를 저장INSERT INTO partitioned_flights PARTITION (DEST_COUNTRY_NAME=\"UNITED STATES\") SELECT count, ORIGIN_COUNTRY_NAME FROM flights WHERE DEST_COUNTRY_NAME='UNITED STATES' LIMIT 1210.6.5 테이블 메타데이터 확인DESCRIBE TABLE flights_csv// 파티셔닝 스키마 정보 확인SHOW PARTITIONS partitioned_flights10.6.6 테이블 메타데이터 갱신하기 테이블 메타데이터를 유지하는 것은 가장 최신의 데이터셋을 읽고 있다는 것을 보장할 수 있는 중요한 작업. 테이블 메타데이터를 갱신할 수 있는 2가지 명령이 있다.-- 테이블과 관련된 모든 캐싱된 항목(기본적으로 파일) 갱신REFRESH table partitioned_flights-- 카탈로그에서 관리하는 테이블의 파티션 정보를 새로고침.MSCK REPAIR TABLE partitioned_flights10.6.7 테이블 제거하기 테이블은 삭제(DELETE)할 수 없음. 오로지 제거(DROP)맘ㄴ 가능. 관리형 테이블의 경우 제거하면 데이터와 테이블 정의 모두 제거 됨.DROP TABLE flights_csv;-- 존재하는 경우에만 제거DROP TABLE IF EXISTS flights_csv;외부 테이블 제거 외부 테이블을 제거하면 데이터는 삭제되지 않지만, 외부 테이블명을 통해 데이터를 조회할 수 없음.10.6.8 테이블 캐싱하기 DataFrame에서처럼 테이블을 캐시하거나 캐시에서 제거할 수 있다.-- 캐시CACHE TABLE flights-- 캐시 제거UNCACHE TABLE FLIGHTS10.7 뷰 뷰는 기준 테이블에서 여러 트랜스포메이션 작업을 지정하여 정의 기본적으로 뷰는 단순 쿼리 실행 계획일 뿐이다. 뷰를 사용하면 쿼리 로직을 체계화하거나 재사용하기 편하게 만들수 있다.10.7.1 뷰 생성하기-- 테이블처럼 데이터베이스에 등록한 뷰 생성CREATE VIEW just_usa_view AS SELECT * FROM flights WHERE dest_country_name = 'United States'-- 데이터베이스에 등록되지 않고, 현재 세션에서만 사용할 수 있는 임시 뷰 생성CREATE TEMP VIEW just_usa_view_temp AS SELECT * FROM flights WHERE dest_country_name = 'United States'-- 전역적 임시 뷰(데이터베이스에 관계 없이 상관없이 사용 가능, 전체 스파크 애플리케이션에서 볼수 있으나, 세션이 종료되면 사라짐)CREATE GLOBAL TEMP VIEW just_usa_global_view_temp AS SELECT * FROM flights WHERE dest_country_name = 'United States'-- 생성된 뷰를 덮어쓸수 있음CREATE OR REPLACE TEMP VIEW just_usa_view_temp AS SELECT * FROM flights WHERE dest_country_name = 'United States' 뷰는 실질적으로 트랜스포메이션이기때문에 쿼리가 실행될 때만 뷰에 정의된 트랜스포메이션이 수행됨. 테이블의 데이터를 실제로 조회하는 경우에만 필터를 적용. 새로운 DataFrame을 만드는것과 동일 val flights = spark.read.format(\"json\") .load(\"/data/flight-data/json/2015-summary.json\")val just_usa_df = flights.where(\"dest_country_name = 'United States'\")just_usa_df.selectExpr(\"*\").explain 내부적으로는 동일한 매커니즘으로 동작하므로 DataFrame이나 SQL 중 편한 방법을 선택해서 사용하면 됨.###10.7.2 뷰 제거하기 테이블을 제거하는 것과 동일한 방식으로 뷰 제거 가능.DROP VIEW IF EXISTS just_usa_view;10.8 데이터 베이스 데이터베이스는 여러 테이블을 조직화하기 위한 도구이다. 스파크에서 실행하는 모든 SQL 명령은 사용중인 데이터베이스 범위 내에서 실행.-- 전체 데이터베이스 목록 조회SHOW DATABASES10.8.1 데이터베이스 생성하기CREATE DATABASE some_db10.8.2 데이터베이스 설정하기-- 현재 사용하는 데이터베이스 변경USE some_db-- 현재 사용중인 데이터베이스 확인SELECT current_database()10.8.3 데이터베이스 제거하기DROP DATABASE IF EXISTS some_db;10.9 select 구문 스파크 쿼리는 ANSI SQL 요건을 충족한다.SELECT [ALL|DISTINCT] named_expression[, named_expression, ...] FROM relation[, relation, ...] [lateral_view[, lateral_view, ...]] [WHERE boolean_expression] [aggregation [HAVING boolean_expression]] [ORDER BY sort_expressions] [CLUSTER BY expressions] [DISTRIBUTE BY expressions] [SORT BY sort_expressions] [WINDOW named_window[, WINDOW named_window, ...]] [LIMIT num_rows]named_expression: : expression [AS alias]relation: | join_relation | (table_name|query|relation) [sample] [AS alias] : VALUES (expressions)[, (expressions), ...] [AS (column_name[, column_name, ...])]expressions: : expression[, expression, ...]sort_expressions: : expression [ASC|DESC][, expression [ASC|DESC], ...]10.9.1 case..when..then 구문 SQL 쿼리으 값을 조건에 맞게 변경해야 하는 경우 위 구문을 사용해 조건에 맞는 처리를 할수 있다.SELECT CASE WHEN DEST_COUNTRY_NAME = 'UNITED STATES' THEN 1 WHEN DEST_COUNTRY_NAME = 'Egypt' THEN 0 ELSE -1 ENDFROM partitioned_flights10.10 고급 주제 SQL 구문은 조작, 정의, 제어와 관련된 명령을 정의할 수 있다.10.10.1 복합 데이터 타입 복합 데이터 타입은 표준 SQL과는 거리가 있는 스파크의 매우 강력한 기능이다. 이를 SQL에서 어떻게 적절하게 처리하는지 이해해야 한다. 스파크 SQL에는 구조체, 리스트, 맵 3가지 핵심 복합 데이터 타입이 존재함구조체 구조체는 맵에 더 가까우며 스파크에서 중첩 데이터를 생성하거나 쿼리하는 방법을 제공함. 여러 컬럼이나 표현식으로 괄호로 묶기만 하면 된다.CREATE VIEW IF NOT EXISTS nested_data AS SELECT (DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME) as country, count FROM flightsSELECT * FROM nested_dataSELECT country.DEST_COUNTRY_NAME, count FROM nested_dataSELECT country.*, count FROM nested_data 구조체의 이름과 모든 하위 컬럼을 지정해 모든 값을 조회할 수 있다.리스트 일반적인 프로그래밍 언어의 리스트와 유사함. 값의 배열이나 리스트는 여러가지 방법으로 생성 가능.SELECT DEST_COUNTRY_NAME as new_name, collect_list(count) as flight_counts, collect_set(ORIGIN_COUNTRY_NAME) as origin_setFROM flights GROUP BY DEST_COUNTRY_NAMESELECT DEST_COUNTRY_NAME, ARRAY(1, 2, 3) FROM flightsSELECT DEST_COUNTRY_NAME as new_name, collect_list(count)[0]FROM flights GROUP BY DEST_COUNTRY_NAME explode 함수를 사용해 배열을 다시 여러 로우로 변환할 수 있다.CREATE OR REPLACE TEMP VIEW flights_agg AS SELECT DEST_COUNTRY_NAME, collect_list(count) as collected_counts FROM flights GROUP BY DEST_COUNTRY_NAMESELECT explode(collected_counts), DEST_COUNTRY_NAME FROM flights_agg10.10.2 함수 스파크 SQL은 복합 데이터 타입 외에도 다양한 고급 함수를 제공한다. DataFrame 함수 문서에서 모든 함수를 찾아볼수 있다. SQL로도 제공하는 전체 함수 목록 확인 가능.-- 전체SHOW FUNCTIONS-- 내장 시스템 함수SHOW SYSTEM FUNCTIONS-- 누군가가 스파크 환경에 공개한 함수SHOW USER FUNCTIONS-- s로 시작하는 모든 함수SHOW FUNCTIONS \"s*\";-- LIKE 키워드를 사용해 함수 검색SHOW FUNCTIONS LIKE \"collect*\";사용자 정의 함수 스파크는 사용자 정의 함수를 정의하고 분산 환경에서 사용할수 있는 기능을 제공함. 특정 언어를 사용해 함수를 개발하고 등록하여 사용할 수 있다.def power3(number:Double):Double = number * number * numberspark.udf.register(\"power3\", power3(_:Double):Double)10.10.3 서브쿼리 서브쿼리르 사용하면 쿼리 안에 쿼리를 지정할 수 있다. SQL에서 정교하 로직을 명시. 상호연관 서브쿼리(correlated subquery) : 서브쿼리의 정보를 보완하기 위해 쿼리의 외부 범위에 있는 일부 정보를 사용 비상호연관 서브쿼리(uncorrelated subquery) : 외부 범위에 있는 정보를 사용하지 않는 서브쿼리 조건절 서브쿼리(predicate subquery) : 값에 따라 필터링할 수 있는 서브 쿼리비상호연관 서브쿼리(uncorrelated subquery)SELECT dest_country_name FROM flightsGROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5SELECT * FROM flightsWHERE origin_country_name IN ( SELECT dest_country_name FROM flights GROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5)상호연관 서브쿼리(correlated subquery)SELECT * FROM flights f1WHERE EXISTS ( SELECT 1 FROM flights f2 WHERE f1.dest_country_name = f2.origin_country_name)AND EXISTS ( SELECT 1 FROM flights f2 WHERE f2.dest_country_name = f1.origin_country_name)비상호연관 스칼라 쿼리 기존에 없던 일부 부가 정보를 가져올 수 있다.SELECT *, (SELECT max(count) FROM flights) AS maximumFROM flights10.11 다양한 기능 SQL 코드의 성능 최적화나 디버깅이 필요한 경우 관련될 수 있는 부분들10.11.1 설정 애플리케이션과 관련된 몇가지 환경 설정값들이 있다. 셔플 파티션 수를 조정하는 것처럼 애플리케이션 초기화 시점 혹은 애플리케이션 실행 시점에 설정 가능.10.11.2 SQL에 설정값 지정하기 SQL을 사용해 환경을 설정하는 경우 스파크 SQL과 관련된 설정만 가능하다는 것을 참고해야 한다.-- 셔플 파티션 수를 지정하는 방법SET spark.sql.shuffle.partitions=20Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 9. 데이터소스", "url": "/posts/spark-guide-9/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2022-11-25 22:52:00 +0900", "snippet": "[스파크 완벽 가이드] 9. 데이터소스9.1 데이터소스 API 구조 특정 포맷을 읽고 쓰는 방법을 알아보기 전에 데이터소스 API 전체적인 구조를 알아보자.9.1.1 읽기 API 구조dataframe.write.format(\"csv\") .option(\"mode\", \"OVERWRITE\") .option(\"dateFormat\", \"yyyy-MM-dd...", "content": "[스파크 완벽 가이드] 9. 데이터소스9.1 데이터소스 API 구조 특정 포맷을 읽고 쓰는 방법을 알아보기 전에 데이터소스 API 전체적인 구조를 알아보자.9.1.1 읽기 API 구조dataframe.write.format(\"csv\") .option(\"mode\", \"OVERWRITE\") .option(\"dateFormat\", \"yyyy-MM-dd\") .option(\"path\", \"path/to/file(s)\") .save() 스파크에서는 모든 데이터소스를 읽을때 위와 같은 포맷을 사용한다. format을 선택적으로 사용할수 있고, default는 파케이 포맷을 사용 한다.9.1.2 데이터 읽기 기초 스파크에서 데이터를 읽을때는 기본적으로 DataFrameReader를 사용하여 SparkSession.read 소성으로 접근한다. 전바적인 코드 구성은 아래 포맷을 참고하면 된다.spark.read.format(\"csv\") .option(\"header\", \"true\") .option(\"mode\", \"FAILFAST\") .option(\"inferSchema\", \"true\") .load(\"some/path/to/file.csv\")읽기 모드 외부 데이터소스에서 데이터를 읽다보면 자연스럽게 형식에 맞지 않는 데이터를 만날 수 있다.- 읽기 모드란 형식에 맞지 않는 데이터를 만났을때의 동작방식을 지정하는 옵션이다. 기본값은 permissive이다.9.1.3 쓰기 API 구조 format은 읽기와 마찬가지로 default. 파케이 포맷입니다. partitionBy, bucketBy, sortBy는 파일 기반 데이터소스에서만 동작한다.DataFrameWriter .format(...) .option(...) .partitionBy(...) .bucketBy(...) .sortBy(...) .save()9.1.4 데이터 쓰기의 기초 읽기 API와 매우 유사하고, DataFrameReader 대신 DataFrameWriter를 사용하면 된다.csvFile.write .format(\"csv\") .mode(\"overwrite\") .option(\"sep\", \"\\t\") .save(\"/tmp/my-tsv-file.tsv\")저장 모드 저장 모드란 스파크가 지정된 위치에서 동일한 파일이 발견되었을ㄷ 떄의 동작방식을 지정하는 옵션입니다. 기본값은 errorIfExists9.2 CSV 파일 , 구분된 데이터 포맷9.2.1 CSV 옵션 CSV reader에서 사용할 수 있는 옵션9.2.2 CSV 파일 읽기import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}val myManualSchema = new StructType(Array( new StructField(\"DEST_COUNTRY_NAME\", StringType, true), new StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true), new StructField(\"count\", LongType, false)))spark.read.format(\"csv\") .option(\"header\", \"true\") .option(\"mode\", \"FAILFAST\") .schema(myManualSchema) .load(\"/data/flight-data/csv/2010-summary.csv\") .show(5) 위와 같이 schema 정의하고 모드를 FAILFAST로 지정했으므로, 스키마와 데이터가 일치하지 않는 경우 오류가 발생합니다. 컴파일 단계에서는 알수 없고, 스파크가 실행되는 RunTime에 오류가 발생함. 9.2.3 CSV 파일 쓰기 maxColumns, inferSchema 옵션 같이 데이터 쓰기에는 적용되지 않는 옵션을 제외하면 동일한 옵션을 제공한다.csv 파일을 읽어서 tsv 파일로 내보내기val csvFile = spark.read.format(\"csv\") .option(\"header\", \"true\").option(\"mode\", \"FAILFAST\").schema(myManualSchema) .load(\"/data/flight-data/csv/2010-summary.csv\")csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\") .save(\"/tmp/my-tsv-file.tsv\")9.3 JSON 파일JSON파일을 Spark에서 다룰때 중요한 부분 JSON 파일 내부는 개행으로 구분된 것을 기본으로 합니다. JSON 객체나 배열을 하나씩 가지고 있는 파일을 다루는 것에 차이를 두고 처리해야 함. multiLine 옵션을 통해 줄로 구분된 방식과 여러줄로 구성된 방식을 선택적으로 사용할 수 있음.9.3.1 JSON 옵션9.3.2 JSON 파일 읽기spark.read .format(\"json\") .option(\"mode\", \"FAILFAST\") .schema(myManualSchema) .load(\"/data/flight-data/json/2010-summary.json\") .show(5)9.3.3 JSON 파일 쓰기 input 데이터소스에 관계 없이 JSON 파일에 저장할 수 있습니다.csvFile.write .format(\"json\") .mode(\"overwrite\") .save(\"/tmp/my-json-file.json\")9.4 파케이(Parquet) 파일 다양한 스토리지 최적화 기술을 제공하는 오픈소스로 만들어진 컬럼 기반 데이터 저장 방식 분석 워크로드에 최적화되어있고, 저장소 공간을 절약할 수 있음. 또한, 전체 파일을 읽는 대신 개별 컬럼을 읽을수있으며, 컬럼 기반의 압축 기능을 제공한다. 스파크와 잘 호환되기 떄문에 기본 파일 포맷이기도 함.9.4.1 파케이 파일 읽기 데이터를 저장할 때 자체 스키마를 사용해 데이터를 저장하기 때문에 옵션이 거의 없다. 따라서 포맷을 설정하는 것만으로도 충분하다.spark.read .format(\"parquet\") .load(\"/data/flight-data/parquet/2010-summary.parquet\") .show(5)파케이 옵션 옵션이 거의 없지만, 간혹 호환되지 않은 파케이 파일이 존재할 수 있는데, 이는 다른 버전(특히 오래된 버전)의 스파크를 사용해 만든 파케이 파일의 경우 조심해야 한다는 점 외에 특이사항은 없다.9.4.2 파케이 파일 쓰기csvFile.write .format(\"parquet\") .mode(\"overwrite\") .save(\"/tmp/my-parquet-file.parquet\")9.5 ORC 파일 ORC는 하둡 워크로드를 위해 설계된 자기 기술적(self-describing)이고, 데이터 타입을 인식할 수 있는 컬럼 기반의 파일 포맷 파케이와 매우 유사한하나 근본적인 차이점은 스파크에 최적화되어있느냐 하이브에 최적화되어있느냐 차이가 있다.9.5.1 ORC 파일 읽기spark.read .format(\"orc\") .load(\"/data/flight-data/orc/2010-summary.orc\") .show(5)9.5.2 ORC 파일 쓰기csvFile.write .format(\"orc\") .mode(\"overwrite\") .save(\"/tmp/my-orc-file.orc\")9.6 SQL 데이터베이스 SQL 데이터베이스는 매우 강력한 커넥터 중 하나. 사용자는 SQL을 지원하는 다양한 시스템에 SQL 게이터소스를 연결할 수 있습니다.(MySQL, ProstgreSQL, Oracle 등)데이터베이스 연결 스파크 classpath에 데이터베이스 JDBC 드라이버를 추가하고, 적ㅈ러한 JDBC 드라이버 jar파일을 제공해야 함../bin/5park-Shell \\--driver-class-path postgresql-9.4.1207.jar \\--jars postgresql-9.4.1207.jarJDBC 데이터소스 옵션9.6.1 SQL 데이터베이스 읽기val driver = \"org.sqlite.JDBC\"val path = \"/data/flight-data/jdbc/my-sqlite.db\"val url = s\"jdbc:sqlite:/${path}\"val tablename = \"flight_info\"import java.sql.DriverManagerval connection = DriverManager.getConnection(url)connection.isClosed()connection.close()val dbDataFrame = spark.read .format(\"jdbc\") .option(\"url\", url) .option(\"dbtable\", tablename) .option(\"driver\", driver).load() 위와 같은 방식으로 생성된 DataFrame은 기존에 생성된 DataFrame과 전혀다르지 않음.9.6.2 쿼리 푸시다운 스파크에서는 DataFrame을 만들기 전에 데이터베이스 자체에서 데이터를 필터링하도록 만들 수 있습니다. 쿼리 실행계획을 보면 테이블의 컬럼 중 관련 있는 컬럼만 선택한다는것을 알수 있습니다.dbDataFrame .select(\"DEST_COUNTRY_NAME\") .distinct() .explainSQL 쿼리 명시 모든 스파크 함수를 SQL 데이터베이스에 맞게 변환하지는 못하기 때문에 SQL 쿼리를 직접 명시해서 처리하는 경우도 필요할 수 있습니다.val pushdownQuery = \"\"\"(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) AS flight_info\"\"\"val dbDataFrame = spark.read.format(\"jdbc\") .option(\"url\", url).option(\"dbtable\", pushdownQuery).option(\"driver\", driver) .load()데이터베이스 병렬로 읽기 스파크는 파일 크기, 유형, 압축 방식에 따른 분할 가능성에 따라 여러 파일을 읽어 하나의 파티션으로 만들거나 여러 파티션을 하나의 파일로 만드는 기본 알고리즘을 가지고 있습니다. SQL 데이터베이스에서는 numPartitons 옵션을 사용해 읽기 및 쓰기용 동시작업 수를 제한 할 수 있습니다.val dbDataFrame = spark.read .format(\"jdbc\") .option(\"url\", url) .option(\"dbtable\", tablename) .option(\"driver\", driver) .option(\"numPartitions\", 10) .load()슬라이딩 윈도우 기반의 파티셔닝 조건절을 기반으로 분할할수 있는 방법을 제공합니다. lowerBound(min), upperBound(max) 값과 numPartitions을 기준으로 쿼리를 N번으로 쪼개서 병렬처리 가능.val colName = \"count\"val lowerBound = 0Lval upperBound = 348113L // this is the max count in our databaseval numPartitions = 10spark.read .jdbc(url,tablename,colName,lowerBound,upperBound,numPartitions,props) .count() // 2559.6.3 SQL 데이터베이스 쓰기 데이터 쓰는것은 읽기만큼 쉬움.val newPath = \"jdbc:sqlite://tmp/my-sqlite.db\"csvFile.write .mode(\"overwrite\") .jdbc(newPath, tablename, props)9.7 텍스트 파일 스파크에서는 일반 텍스트 파일도 읽을 수 있습니다. 파일의 각 줄은 DataFrame의 레코드로 맵핑됨. 9.7.1 텍스트 파일 읽기 textFile 메소드에 텍스트 파일을 지정하기만 하면 됨. 파티션 수행 결과로 만들어진 디렉터리명을 무시(파티션된 텍스트 파일을 읽거나 쓰려면 text 메소드 사용 spark.read.textFile(\"/data/flight-data/csv/2010-summary.csv\") .selectExpr(\"split(value, ',') as rows\").show()9.7.2 텍스트 파일 쓰기 텍스트 파일을 쓸때는 문자열 컬럼이 하나만 존재해야 함.(아닌 경우 작업 실패) 텍스트 파일에 데이터를 저장할때 파티셔닝을 수행하면 더 많은 컬럼을 저장할 수 있음. 디렉터리에 컬럼별로 별도 저장됨 csvFile.select(\"DEST_COUNTRY_NAME\").write.text(\"/tmp/simple-text-file.txt\")csvFile.limit(10).select(\"DEST_COUNTRY_NAME\", \"count\") .write.partitionBy(\"count\").text(\"/tmp/five-csv-files2.csv\")9.8 고급 I/O 개념 쓰기 작업 전 파티션 수를 조절함으로써 병렬 처리할 파일 수를 제어할 수 있음. 버텟팅과 파티셔닝의 조절 9.8.1 분할 가능한 파일 타입과 압축 방식 특정 파일 포맷은 기본적으로 분할을 지원함. 따라서 스파크에서 전체 파일이 아닌 쿼리에 필요한 부분만 읽을수 있어 성능 향상됨. 스파크에서는 파케이 파일 포맷과 GZIP 압축 방식을 권장함.9.8.2 병렬로 데이터 읽기 여러 익스큐터가 같은 파일을 동시에 읽을 수는 없지만, 여러 파일을 동시에 읽을 수는 있음.(?) 다수의 파일이 존재하는 폴더를 읽을ㄷ 때 폴더의 개별 파일은 Dataframe의 파티션이 되는데 이걸 기준으로 병렬로 읽기가 가능 9.8.3 병렬로 데이터 쓰기 파일이나 데이터 수는 쓰는 시점에 DataFrame이 가진 파티션 수에 따라 달라질 수 있음. 기본적으로 데이터 파티션당 하나의 파일이 작성됨. 실제로 옵션에 지정된 파일명은 다수의 파일을 가진 디렉터리 // 디렉토리 안에 5개의 파일이 생성csvFile.repartiton(5).write.format(\"csv\").save(\"/tmp/multiple.csv\")파티셔닝 어떤 데이터를 어디에 저장할 것인지 제어할 수 있는 기능 파티셔닝된 디렉터리 또는 테이블에 파일을 쓸때 디렉터리별로 컬럼 데이터를 인코딩해 저장함. 데이터를 읽을 때 전체 데이터셋을 스캔하지 않고 필요한 컬럼의 데이터만 읽을수 있는 이유. csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\") .save(\"/tmp/partitioned-files.parquet\") 각 폴더는 조건절을 폴더명으로 사용하며, 만족한 데이터가 저장된 파케이 파일을 가지고 있다. 파티셔닝은 필터링을 자주 사용하는 테이블을 가진 경우에 사용할 수 있는 가장 손쉬운 최적화 방법.버켓팅 각 파일에 저장된 데이터를 제어할 수 있는 또 다른 파일 조직화 기법 동일한 버킷 ID를 가진 데이터가 하나의 물리적 파티션에 모두 모여 있게 하므로 데이터를 읽을때 셔플을 피할 수 있다. 특정 컬럼을 파티셔닝했을때 수억개의 디렉터리가 만들어질수도 있는데, 이런 경우 버켓팅 방법을 찾아야 한다.// 버켓 단위로 데이터를 모아 일정 수의 파일로 저장하는 예제val numberBuckets = 10val columnToBucketBy = \"count\"csvFile.write.format(\"parquet\").mode(\"overwrite\") .bucketBy(numberBuckets, columnToBucketBy).saveAsTable(\"bucketedFiles\")9.8.4 복합 데이터 유형 쓰기 스파크는 다양한 자체 데이터 타입을 제공하는데, 이는 스파크에서는 잘 동작하지만, 모든 데이터 파일 포맷에 적합하지는 않음. CSV파일은 복합 데이터 타입을 지원하지 않음 파케이나 ORC에서는 지원함. 9.8.5 파일 크기 관리 파일크기는 데이터를 저장할 때는 중요한 요소는 아니다. 하지만, 데이터를 읽을때는 중요한 소요적은 크기의 파일 문제(작은 크기의 파일이 많은 경우) 메타데이터에 엄청난 관리 부하가 발생할 수 있음. HDFS 등 많은 파일 시스템에서 작은 크기의 많은 파일을 잘 다루지 못한다.(스파크도 포함)파일크기가 큰 파일을 다루는 경우 몇개의 로우가 필요하더라도 전체 데이터 블록을 읽어야 하기 때문에 비효율적이므로 지양해야 함.maxRecordsPerFile(파일당 레코드 수) 스파크 2.2 부터 해당 기능을 사용해서 각 파일에 기록될 레코드 수를 조절할 수 있으므로 파일 크기를 효과적으로 제어할 수 있다.df.write.option(\"maxRecordsPerFile\", 5000)Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 8. 조인", "url": "/posts/spark-guide-8/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2022-11-19 18:36:00 +0900", "snippet": "[스파크 완벽 가이드] 8. 조인8.1 조인 표현식 스파크는 왼쪽과 오른쪽 데이터셋에 있는 하나 이상의 키값을 비교하고 왼쪽 데이터셋과 오른쪽 데이터셋의 결합 여부를 결정하는 조인 표현식의 평가 결과에 따라 2개의 데이터셋을 조인합니다. 일반적인 RDB의 JOIN과 유사한 방식8.2 조인 타입 내부 조인(inner join) : 왼쪽과 오른쪽 데...", "content": "[스파크 완벽 가이드] 8. 조인8.1 조인 표현식 스파크는 왼쪽과 오른쪽 데이터셋에 있는 하나 이상의 키값을 비교하고 왼쪽 데이터셋과 오른쪽 데이터셋의 결합 여부를 결정하는 조인 표현식의 평가 결과에 따라 2개의 데이터셋을 조인합니다. 일반적인 RDB의 JOIN과 유사한 방식8.2 조인 타입 내부 조인(inner join) : 왼쪽과 오른쪽 데이터셋에 키가 있는 로우를 유지 외부 조인(outer join) : 왼쪽이나 오른쪽 데이터셋에 키가 있는 로우를 유지 왼쪽 외부 조인(left outer join) : 왼쪽 데이터셋에 키가 있는 로우를 유지 오른쪽 외부 조인(right outer join) : 오른쪽 데이터셋에 키가 있는 로우를 유지 왼쪽 세미 조인(left semi join) : 왼쪽 데이터셋의 키가 오른쪽 데이터셋에 있는 경우 키가 일치하는 왼쪽 데이터셋만 유지 왼쪽 안티 조인(left anti join) : 왼쪽 데이터셋의 키가 오른쪽 데이터셋에 없는 경우에는 키가 일치하지 않은 왼쪽 데이터셋만 유지 자연 조인(natural join) : 두 데이터셋에서 동일한 이름을 가진 컬럼을 암시적(implicit)으로 결합하는 조인을 수행 교차 조인(cross join) 또는 카테시안 조인(cartesian join) : 왼쪽 데이터셋의 모든 로우와 오른쪽 데이터셋의 모든 로우 조합을 표시base dataval person = Seq( (0, \"Bill Chambers\", 0, Seq(100)), (1, \"Matei Zaharia\", 1, Seq(500, 250, 100)), (2, \"Michael Armbrust\", 1, Seq(250, 100))) .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")val graduateProgram = Seq( (0, \"Masters\", \"School of Information\", \"UC Berkeley\"), (2, \"Masters\", \"EECS\", \"UC Berkeley\"), (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")) .toDF(\"id\", \"degree\", \"department\", \"school\")val sparkStatus = Seq( (500, \"Vice President\"), (250, \"PMC Member\"), (100, \"Contributor\")) .toDF(\"id\", \"status\")person.createOrReplaceTempView(\"person\")graduateProgram.createOrReplaceTempView(\"graduateProgram\")sparkStatus.createOrReplaceTempView(\"sparkStatus\")8.3 내부 조인(INNER JOIN) 테이블에 존재하는 키를 평가하고, 결과가 참(true)인 로우만 결합val joinExpression = person.col(\"graduate_program\") === graduateProgram.col(\"id\")val wrongJoinExpression = person.col(\"name\") === graduateProgram.col(\"school\")person.join(graduateProgram, joinExpression).show()SELECT *FROM personJOIN graduateProgram ON person.graduate_program = graduateProgram.idjoinType 지정var joinType = \"inner\"// 3번쨰 파라미터로 joinType을 명시해줄수 있다.person.join(graduateProgram, joinExpression, joinType).show()8.4 외부 조인(FULL OUTER JOIN) Outer Join은 DataFrame이나 테이블에 존재하는 키를 평가하여 참이나 거짓으로 평가한 로우를 조인하고, 일치하는 로우가 없다면 해당 위치를 null로 채워주는 조인 방식joinType = \"outer\"person.join(graduateProgram, joinExpression, joinType).show()SELECT *FROM personFULL OUTER JOIN graduateProgram ON graduate_program = graduateProgram.id8.5 왼쪽 외부 조인(LEFT OUTER JOIN) 왼쪽 DataFrame의 모든 로우를 표시하고, 이와 일치하는 오른쪽 DataFrame 로우를 함꼐 표시해주는 조인 방식 오른쪽 DataFrame에 일치하는 로우가 없다면 해당 위치는 null로 채워짐 joinType = \"left_outer\"graduateProgram.join(person, joinExpression, joinType).show()SELECT *FROM graduateProgramLEFT OUTER JOIN person ON person.graduate_program = graduateProgram.id8.6 오른쪽 외부 조인(RIGHT OUTER JOIN) LEFT OUTER JOIN과 드라이빙 테이블이 왼쪽이냐 오른쪽이냐만 차이가 있고 동일.8.7 왼쪽 세미 조인 왼쪾 DataFrame의 어떤 값도 포함하지 않기 떄문에 조금 다르지만, 오른쪽 DataFrame의 존재여부에 따라 결과가 달라질수 있는 조인 타입 결과 데이터에는 왼쪽 DataFrame만 표시됨.joinType = \"left_semi\"graduateProgram.join(person, joinExpression, joinType).show()val gradProgram2 = graduateProgram.union(Seq( (0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")).toDF())gradProgram2.createOrReplaceTempView(\"gradProgram2\")gradProgram2.join(person, joinExpression, joinType).show()SELECT *FROM gradProgram2LEFT SEMI JOIN person ON gradProgram2.id = person.graduate_program8.8 왼쪽 안티 조인(LEFT ANTI JOIN) 세미 조인의 반대 개념 오른쪽 DataFrame의 어떤 값과도 일치되지 않은 왼쪽 DataFrame만 표시joinType = \"left_anti\"graduateProgram.join(person, joinExpression, joinType).show()SELECT *FROM graduateProgramLEFT ANTI JOIN person ON graduateProgram.id = person.graduate_program8.9 자연 조인(NATURAL JOIN) 조인하려는 컬럼을 암시적으로 추정하여 JOIN하는 방식 별도로 조인 키를 지정해주지 않고 동일한 컬럼명으로 참조되기 때문에 의도치 않은 결과를 반환할수 있어서 사용시 주의 필요.SELECT *FROM graduateProgramNATURAL JOIN person8.10 교차 조인(CROSS JOIN) 또는 카테시안 조인(CARTESIAN JOIN) 교차 조인은 조건절을 기술하지 않은 내부 조인 DataFrame의 모든 로우를 오른쪽 모든 로우와 결합 교차 조인을 하게 되면 엄청나게 많은 수의 로우가 생성될 수 있음. 1,000 X 1,000의 교차 조인 결과는 1,000,000개의 로우가 생성됨. joinType = \"cross\"graduateProgram.join(person, joinExpression, joinType).show()person.crossJoin(graduateProgram).show()SELECT *FROM graduateProgramCROSS JOIN person ON graduateProgram.id = person.graduate_program8.11 조인 사용시 문제점8.11.1 복합 데이터 타입의 조인 복합 데이터 타입의 조인이 어려워보일수 있지만, 실제로 블리언을 반환하는 모든 표현식을 이용해서 조인을 할수 있다.import org.apache.spark.sql.functions.exprperson.withColumnRenamed(\"id\", \"personId\") .join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()SELECT *FROM ( SELECT id as personId, name, graduate_program, spark_status FROM person)INNER JOIN sparkStatus ON array_contains(spark_status, id)8.11.2 중복 컬럼명 처리 JOIN을 수행할때 가장 까다로운 것중 하나는 결과 DataFrame에서 중복된 컬럼명을 다루는 것이다. 각 컬럼은 스파크 SQL엔진인 카탈리스트 내에 고유 ID를 가지고 있으나, 이는 카탈리스트 내부에서만 관리되고, 직접 참조할 수 없다. 따라서 중복된 컬럼명이 존재하는 DataFrame을 사용할 떄는 특정 컬럼을 참조할 수 없어서 별도 처리가 필요하다. 해결방법1 : 다른 조인 표현식 사용 불리언 형태의 조인 표현식을 문자열이나 시퀀스 형태로 변경한다. 조인할때 두 컬럼 중 하나가 자동으로 제거 됨.val gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")person.join(gradProgramDupe,\"graduate_program\").select(\"graduate_program\").show()해결방법2 : 조인 후 컬럼 제거person.join(gradProgramDupe, joinExpr).drop(person.col(\"graduate_program\")) .select(\"graduate_program\").show()val joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")person.join(graduateProgram, joinExpr).drop(graduateProgram.col(\"id\")).show()해결 방법3 : 조인 전 컬럼명 변경(가장 좋은 방법이라고 생각됨)val gradProgram3 = graduateProgram.withColumnRenamed(\"id\", \"grad_id\")val joinExpr = person.col(\"graduate_program\") === gradProgram3.col(\"grad_id\")person.join(gradProgram3, joinExpr).show()8.12 스파크의 조인 수행 방식 스파크의 조인 수행 방식을 이해하기 위해서는 두 가지 핵심 전략을 이해해야 한다.핵심 전략 노드간 네트워크 통신 전략 노드별 연산 전략8.12.1 네트워크 통신 전략 스파크는 조인시 2가지 클러스터 통신 방식을 활용한다. 셔플 조인(shuffle join), 브로드캐스트 조인(broadcast join) 큰 테이블과 큰 테이블 조인 하나의 큰 테이블을 다른 큰 테이블과 조인하면 아래와 같은 셔플 조인이 발생합니다. 셔플 조인은 전체 노드 간 통신이 발생하는데, 사용된 특정 키나 키 집합이 어떤 노드에 있느냐에 따라 해당 노드와 데이터를 공유해야 합니다. 이런 방식 떄문에 네트워크가 복잡해지고, 많은 자원을 사용해야 합니다. 이런 데이터가 자주 사용된다면 네트워크 비용을 줄이기 위해 1차적으로 비정규화된 데이터셋을 하나 만들어서 프로세싱하는것도 방법이 될수 있다.큰 테이블과 작은 테이블 조인 작은 테이블이 단일 워커 노드의 메모리 크기에 적합할 정도로 충분히 작다면 조인 연산을 최적화 할수 있습니다. 이 경우 작은 DataFrame을 클러스터 전체 워커노드에 복제하는 브로드캐스트 조인을 수행한다면 효율적으로 처리가 간으합니다. 최초 전체 워커노드에 데이터를 복제하는 과정에서는 I/O가 발생하겠지만 그이후로는 추가적인 네트워크 비용이 들기 않기 때문에 작업 전체적인 리소스 효율을 증대시킬수 있음.broadcast join 수행하지 않는 경우val joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")person.join(graduateProgram, joinExpr).explain()broadcast join hint 아래와 같이 힌트를 주어 broadcast join을 수행할 수 있습니다. 다만, 강제성이 있는것은 아니라서 옵티마이저 판단에 의해 무시될 수 있습니다. import org.apache.spark.sql.functions.broadcastval joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")person.join(broadcast(graduateProgram), joinExpr).explain()아주 작은 테이블 사이의 조인 이 경우는 스파크 옵티마이저가 조인 방식을 결정하도록 내버려 두는게 가장 좋은 방법Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 7. 집계 연산", "url": "/posts/spark-guide-7/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2022-11-19 17:20:00 +0900", "snippet": "[스파크 완벽 가이드] 7. 집계 연산7.1 집계 함수 ` org.apache.Spark.sql.functions` 패키지에서 제공되는 함수를 찾아볼수 있음.7.1.1 count count는 액션이 아닌 트랜스포메이션으로 동작 지연 연산이 아니라 즉각 결과를 처리한다는 의미 import org.apache.spark.sql...", "content": "[스파크 완벽 가이드] 7. 집계 연산7.1 집계 함수 ` org.apache.Spark.sql.functions` 패키지에서 제공되는 함수를 찾아볼수 있음.7.1.1 count count는 액션이 아닌 트랜스포메이션으로 동작 지연 연산이 아니라 즉각 결과를 처리한다는 의미 import org.apache.spark.sql.functions.countdf.select(count(\"StockCode\")).show() // 541909SELECT COUNT(*) FROM dfTable null 값이 포함된 데이터 레코드의 count를 구할때는 유의사항이 있음.count(*)을 사용하면 null 값을 가진 로우도 포함해서 카운트하고, 특정 컬럼을 지정하면 null 값은 카운트 하지 않음.7.1.2 countDistinct 전체 레코드 수가 아니라 고유 레코드 수를 구해야 하는 경우 사용import org.apache.spark.sql.functions.countDistinctdf.select(countDistinct(\"StockCode\")).show() // 4070SELECT COUNT(DISTINCT *)FROM DFTABLE7.1.3 approx_count_distinct 정확한 고유 개수가 무의미한 경우 어느 정도 수준의 정확도를 가지는 근사치만으로 유의미할 경우 사용할수 있는 집계함수 최대 추정 오류율(maximum estimation error)이라는 한가지 파라미터를 더 사용함. 대규모 데이터셋을 처리할때 오차가 어느정도 발생할수 있지만 countDistinct함수보다 더 빠르게 결과를 반환받을수 있음. import org.apache.spark.sql.functions.approx_count_distinctdf.select(approx_count_distinct(\"StockCode\", 0.1)).show() // 3364SELECT approx_count_distinct(StockCode, 0.1)FROM DFTABLE7.1.4 first와 last DataFrame의 첫번째 값이나 마지막 값을 조회import org.apache.spark.sql.functions.{first, last}df.select(first(\"StockCode\"), last(\"StockCode\")).show()SELECT first(StockCode), last(StockCode)FROM dfTable7.1.5 min 과 max 최솟값과 최댓값 추출import org.apache.spark.sql.functions.{min, max}df.select(min(\"Quantity\"), max(\"Quantity\")).show()SELECT min(Quantity), max(Quantity)FROM dfTable7.1.6 sum DataFrame에서 특정 컬럼의 모든 값을 합산할때 사용import org.apache.spark.sql.functions.sumdf.select(sum(\"Quantity\")).show() // 5176450SELECT sum(Quantity)FROM dfTable7.1.7 sumDistinct 특정 컬럼의 고윳값을 합산import org.apache.spark.sql.functions.sumDistinctdf.select(sumDistinct(\"Quantity\")).show() // 29310SELECT SUM(DISTINCT Quantity)FROM dfTable -- 293107.1.8 avg sum() / count() 대신 스파크의 avg, mean 함수를 사용하면 평균값을 더 쉽게 구할 수 있음.import org.apache.spark.sql.functions.{sum, count, avg, expr}df.select( count(\"Quantity\").alias(\"total_transactions\"), sum(\"Quantity\").alias(\"total_purchases\"), avg(\"Quantity\").alias(\"avg_purchases\"), expr(\"mean(Quantity)\").alias(\"mean_purchases\")) .selectExpr( \"total_purchases/total_transactions\", \"avg_purchases\", \"mean_purchases\").show()7.1.9 분산과 표준편차 평균을 구하다 보면 자연스럽게 분산과 표준편차가 궁금할 수 있다. 스파크에서 표본표준편차, 모표준편차과 같은 처리도 지원함.import org.apache.spark.sql.functions.{var_pop, stddev_pop}import org.apache.spark.sql.functions.{var_samp, stddev_samp}df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"), stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()SELECT var_pop(Quantity), var_samp(Quantity), stddev_pop(Quantity), stddev_samp(Quantity)FROM dfTable7.1.10 비대칭도와 첨도 비대칭도(skewness), 첨도(kurtosis)와 같은 데이터의 변곡점(extreme point)을 측정하는 방법도 제공.import org.apache.spark.sql.functions.{skewness, kurtosis}df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()SELECT skewness(Quantity), kurtosis(Quantity)FROM dfTable7.1.11 공분산과 상관관계 두 컬럼값 사이의 영향도 비교하는 함수들도 제공. cov() : 공분산(covariance) 계산 corr() : 상관관계(correlation) 계산 표본공분산(sample covariance) 방식이나 모공분산(population covariance) 방식으로 공분산 계산도 가능.import org.apache.spark.sql.functions.{corr, covar_pop, covar_samp}df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"), covar_pop(\"InvoiceNo\", \"Quantity\")).show()SELECT corr(InvoiceNo, Quantity), covar_samp(InvoiceNo, Quantity), covar_pop(InvoiceNo, Quantity)FROM dfTable7.1.12 복합 데이터 타입의 집계 복합 데이터 타입을 사용해 집계를 수행할 수 있다. 특정 컬럼의 값을 리스트로 수집, 셋 데이터 타입으로 고윳값만 수집 import org.apache.spark.sql.functions.{collect_set, collect_list}df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()SELECT collect_set(Country), collect_set(Country)FROM dfTable7.2 그룹화 실제로는 DataFrame 수준을 넘어서서 그룹 기반 집계를 수행하는 경우가 많이 있음. 단일 컬럼의 데이터를 그룹화하고, 해당 그룹의 다른 여러 컬럼을 사용해 계산하기 위해 카테고맇여 데이터를 사용한다. 그룹화 과정 RelationalGroupedDataset -&gt; DataFrame 반환df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()SELECT count(*) FROM dfTable GROUP BY InvoiceNo, CustomerId7.2.1 표현식을 이용한 그룹화 count 함수를 select 구문에 표현식으로 지정하는것보다 agg 메소드를 사용하는 것이 좋음. agg 메서드는 여러 집계 ㄹ처리를 한번에 지정할 수 있고, 집계에 표현식을 사용할 수 있음. 트랜스포메이션이 완료된 컬럼에서 alias 메서드를 사용할 수 있음.df.groupBy(\"InvoiceNo\").agg( count(\"Quantity\").alias(\"quan\"), expr(\"count(Quantity)\")).show()7.2.2 맵을 이용한 그룹화 컬럼을 키로, 수행할 집계 함수의 문자열을 값으로 하는 맵 타입을 사용해서 트랜스포메이션을 정의할 수 있음.df.groupBy(\"InvoiceNo\").agg(\"Quantity\"-&gt;\"avg\", \"Quantity\"-&gt;\"stddev_pop\").show()SELECT avg(Quantity), stddev_pop(Quantity), InvoiceNoFROM dfTableGROUP BY InvoiceNo7.3. 윈도우 함수 윈도우 함수를 집계에 사용할 수 있다. 윈도우 함수는 데이터의 특정 윈도우(window)를 대상으로 고유의 집계 연산을 수행하는 것을 말함. 윈도우는 현재 데이터에 대한 참조를 사용해서 정의 group-by와의 차이점 group-by 를 사용하면 모든 로우 레코드가 단일 그룹으로만 이동 윈도우 함수는 frame에 입력되는 모든 로우에 대해 결과값을 계싼. frame은 로우 그룹 단위 테이블을 의미하고, 각 로우는 하나 이상의 프레임에 할당될 수 있다. 지원되는 윈도우 함수 랭크 함수(raking function) 분석 함수(analytic function) 집계 함수(aggregate function)윈도우 함수 시각화윈도우 함수 스펙 정의 및 사용import org.apache.spark.sql.expressions.Windowimport org.apache.spark.sql.functions.colval windowSpec = Window .partitionBy(\"CustomerId\", \"date\") // 파티셔닝 스키마와 개념적으로 관련 없는 Window 명세 .orderBy(col(\"Quantity\").desc) // 파티션의 정렬 방식 지정 .rowsBetween(Window.unboundedPreceding, Window.currentRow) // 입력된 로우의 참조를 기반으로 프레임에 로우가 포함될 수 있는지 결정 윈도우의 그룹을 어떻게 나눌지 결정하는것과 유사한 개념.import org.apache.spark.sql.functions.maximport org.apache.spark.sql.functions.{dense_rank, rank}import org.apache.spark.sql.functions.col// 최대 구매 개수 구하기val maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)// 구매량 순위 구하기val purchaseDenseRank = dense_rank().over(windowSpec)val purchaseRank = rank().over(windowSpec)// 시간대별 최대 구매 개수 구하기dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\") .select( col(\"CustomerId\"), col(\"date\"), col(\"Quantity\"), purchaseRank.alias(\"quantityRank\"), purchaseDenseRank.alias(\"quantityDenseRank\"), maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()SELECT CustomerId, date, Quantity, rank(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as rank, dense_rank(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as dRank, max(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as maxPurchaseFROM dfWithDateWHERE CustomerId IS NOT NULLORDER BY CustomerId7.4 그룹화 셋 때로는 여러 그룹에 걸쳐 집계할 수 있는 무언가가 필요할수 있는데 이때 그룹화 셋을 사용할 수 있다. 그룹화 셋은 여러 집계를 결합하는 저수준 기능입니다.그룹화셋을 사용하지 않은 방식val dfNoNull = dfWithDate.drop()dfNoNull.createOrReplaceTempView(\"dfNoNull\")SELECT CustomerId, stockCode, sum(Quantity)FROM dfNoNullGROUP BY customerId, stockCodeORDER BY CustomerId DESC, stockCode DESC그룹화셋 사용방식SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNullGROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode))ORDER BY CustomerId DESC, stockCode DESC 책 내용을 보면 무슨차이인지 모르겠음 그룹화셋을 사용할떄는 dfNoNull 대신 dfWithDate을 사용할수 있다는것인지 추후에 스파크 개발을 해보면서 업데이트 하겠다. null과 그룹화 셋 null 값에 따라 집계 수준이 달라지는데, null값이 제거되지 않으면 부정확한 결과값을 얻을수 있음. CustomerId, stockCode과 관계 없이 총 수량의 합산 결과를 추가하려고 하는경우 group-by를 이용해서는 불가능하지만 그룹화셋을 사용하면 가능함.SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNullGROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode),())ORDER BY CustomerId DESC, stockCode DESCGROUPING SETS 제약사항 이 구문은 SQL에서만 사용 가능하다. DataFrame에서 동일한 연산을 수행하려면 rollup, cube 메소드를 사용해야 한다.7.4.1 롤업(rollup) 다양한 컬럼을 그룹화 키로 설정하면 그룹화 키로 설정된 조합 뿐만 아니라 데이터셋에서 볼수 있는 실제 조합을 모두 살펴볼 수 있다. 롤업은 group-by 스타일의 다양한 연산을 수행할 수 있는 다차원 집계 기능이다.val rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\")) .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\") .orderBy(\"Date\")rolledUpDF.show() null 값을 가진 로우에서 전체 날짜의 합계를 확인할 수 있다. 두 개의 컬럼값이 모두 null인 로우는 두 컬럼에 속한 레코드의 전체 합계 rolledUpDF.where(\"Country IS NULL\").show()rolledUpDF.where(\"Date IS NULL\").show()7.4.2 큐브(cube) 롤업을 고차원적으로 사용할수 있도록 해주는 기능. 요소들을 계층적으로 다루는 대신 모든 차원에 대해 동일한 작업을 수행한다.dfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\"))) .select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show() 큐브를 사용해 테이블에 있는 모든 정보를 빠르고 쉽게 조회할 수 있는 요약 정보 테이블을 만들수 있습니다.7.4.3 그룹화 메타데이터 큐브와 롤업을 사용하다보면 수준에 따라 쉽게 필터링하기 위해 집계 수준을 조회하는 경우가 발생한다. grouping_id을 사용해서 결과 데이터셋의 집계 수준을 명시하는 컬럼을 제공할 수 있음.그룹화 ID의 의미그룹화 ID 사용 예시import org.apache.spark.sql.functions.{grouping_id, sum, expr}dfNoNull.cube(\"customerId\", \"stockCode\").agg(grouping_id(), sum(\"Quantity\")).orderBy(col(\"grouping_id()\").desc).show()7.4.4 피벗 피벗(pivot)을 사용해 로우를 컬럼으로 변환할 수 있다.val pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()pivoted.where(\"date &gt; '2011-12-05'\").select(\"date\" ,\"`USA_sum(Quantity)`\").show()// data | USA_sum(Quantity)// 2011-12-06 | null// 2011-12-09 | null// 2011-12-08 | -196// 2011-12-07 | null 컬럼의 모든 값을 단일 그룹화해서 계산할 수 있음.7.5 사용자 정의 집계 함수 UDAF(user-defined aggregation function)은 직접 제작한 함수나 비즈니스 규칙에 기반을 둔 자쳬 집계 함수를 정의하는 방법. 입력 데이터 그룹에 직접 개발한 연산을 수행할 수 있다. 스파크는 입력 데이터의 모든 그룹의 중간 결과를 단일 AggregationBuffer에 저장해 관리UDAF 정의 UserDefinedAggregateFunction을 상속하고 아래 메서드를 정의import org.apache.spark.sql.expressions.MutableAggregationBufferimport org.apache.spark.sql.expressions.UserDefinedAggregateFunctionimport org.apache.spark.sql.Rowimport org.apache.spark.sql.types._class BoolAnd extends UserDefinedAggregateFunction { def inputSchema: org.apache.spark.sql.types.StructType = StructType(StructField(\"value\", BooleanType) :: Nil) def bufferSchema: StructType = StructType( StructField(\"result\", BooleanType) :: Nil ) def dataType: DataType = BooleanType def deterministic: Boolean = true def initialize(buffer: MutableAggregationBuffer): Unit = { buffer(0) = true } def update(buffer: MutableAggregationBuffer, input: Row): Unit = { buffer(0) = buffer.getAs[Boolean](0) &amp;&amp; input.getAs[Boolean](0) } def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = { buffer1(0) = buffer1.getAs[Boolean](0) &amp;&amp; buffer2.getAs[Boolean](0) } def evaluate(buffer: Row): Any = { buffer(0) }}UDAF 등록import org.apache.spark.sql.functions._val ba = new BoolAndspark.udf.register(\"booland\", ba)spark.range(1) .selectExpr(\"explode(array(TRUE, TRUE, TRUE)) as t\") .selectExpr(\"explode(array(TRUE, FALSE, TRUE)) as f\", \"t\") .select(ba(col(\"t\")), expr(\"booland(f)\")) .show()UDAF 유의사항 스파크 완벽 가이드 기준으로 스칼라와 자바로만 사용할 수 있음.Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 6. 다양한 데이터 타입 다루기", "url": "/posts/spark-guide-6/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2022-11-19 14:36:00 +0900", "snippet": "[스파크 완벽 가이드] 6. 다양한 데이터 타입 다루기데이터 타입들 불리언 타입 수치 타입 문자열 타입 date와 timestamp 타입 null 값 다루기 복합 데이터 타입 사용자 정의 함수6.1 API는 어디서 찾을까?DataFrame(DataSet) 메서드 DataFrame은 Row 타입을 가진 Dataset이므로 Dataset 메...", "content": "[스파크 완벽 가이드] 6. 다양한 데이터 타입 다루기데이터 타입들 불리언 타입 수치 타입 문자열 타입 date와 timestamp 타입 null 값 다루기 복합 데이터 타입 사용자 정의 함수6.1 API는 어디서 찾을까?DataFrame(DataSet) 메서드 DataFrame은 Row 타입을 가진 Dataset이므로 Dataset 메서드를 보면 됨. Column 메서드 alias나 contains 같은 컬럼 관련된 여러 메소드를 제공하고, Column API 스파크 문서를 참고하자.val df = spark.read.format(\"csv\") .option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .load(\"/data/retail-data/by-day/2010-12-01.csv\")df.printSchema()df.createOrReplaceTempView(\"dfTable\")6.2 스파크 데이터 타입으로 변환하기 프로그래밍 언어 고유 데이터 타입을 스파크 데이터 타입으로 변환해보자. lit() 은 다른 언어의 데이터 타입을 스파크 데이터 타입에 맞게 변환합니다.import org.apache.spark.sql.functions.litdf.select(lit(5), lit(\"five\"), lit(5.0))6.3 불리언 데이터 타입 다루기 불리언 구문은 and, or, true, false 로 구성됨.import org.apache.spark.sql.functions.coldf.where(col(\"InvoiceNo\").equalTo(536365)) .select(\"InvoiceNo\", \"Description\") .show(5, false)df.where(col(\"InvoiceNo\") === 536365) .select(\"InvoiceNo\", \"Description\") .show(5, false) 스칼라에서는 not, equalsTo 를 사용하거나 ===, =!=을 통해 동등성 비교를 할 수 있음.// 일치df.where(\"InvoiceNo = 536365\") .show(5, false)// 불일치df.where(\"InvoiceNo &lt;&gt; 536365\") .show(5, false) 위와 같이 문자열 표현에 조건절을 명시하는 방법이 있음.( 가장 명확)and, or을 사용해 여러 조건 표현식val priceFilter = col(\"UnitPrice\") &gt; 600val descripFilter = col(\"Description\").contains(\"POSTAGE\")df.where(col(\"StockCode\").isin(\"DOT\")).where(priceFilter.or(descripFilter)) .show() and는 별도로 정의하지 않더라도 스파크 내부적으로 하나의 문장으로 변환됨. or 구문을 사용할 때는 반드시 동일한 구문에 조건을 정의해주어야 함.불리언 표현식을 이용해 DataFrame 필터링 조회 필터링 조건 외에 DataFrame 데이터를 필터링하는데에도 이용할 수 있음.val DOTCodeFilter = col(\"StockCode\") === \"DOT\"val priceFilter = col(\"UnitPrice\") &gt; 600val descripFilter = col(\"Description\").contains(\"POSTAGE\")df.withColumn(\"isExpensive\", DOTCodeFilter.and(priceFilter.or(descripFilter))) .where(\"isExpensive\") .select(\"unitPrice\", \"isExpensive\").show(5)SELECT UnitPrice, (StockCode = 'DOT' AND (UnitPrice &gt; 600 OR instr(Description, \"POSTAGE\") &gt;= 1)) as isExpensiveFROM dfTableWHERE (StockCode = 'DOT' AND (UnitPrice &gt; 600 OR instr(Description, \"POSTAGE\") &gt;= 1))컬럼명을 사용해 필터를 정의할 수 있음.import org.apache.spark.sql.functions.{expr, not, col}df.withColumn(\"isExpensive\", not(col(\"UnitPrice\").leq(250))) .filter(\"isExpensive\") .select(\"Description\", \"UnitPrice\").show(5)df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice &lt;= 250\")) .filter(\"isExpensive\") .select(\"Description\", \"UnitPrice\").show(5)6.4 수치형 데이터 타입 다루기 count는 빅데이터 처리애서 필터링 다음으로 많이 수행하는 작업이다. 수치형 데이터 타입을 사용해 연산 방식을 정의하기만 하면 된다.import org.apache.spark.sql.functions.{expr, pow}// 실제수량 = (현재 수량 * 단위가격)^2 + 5val fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)df.selectExpr( \"CustomerId\", \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)SELECT customerId, (POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantityFROM dfTableround(반올림), bround(내림)import org.apache.spark.sql.functions.{round, bround}import org.apache.spark.sql.functions.litdf.select(round(col(\"UnitPrice\"), 1).alias(\"rounded\"), col(\"UnitPrice\")).show(5)df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)SELECT round(2.5), bround(2.5)컬럼 사이의 상관관계 계산 피어슨 상관 계수를 계산해보고자 할 경우 내부적으로 제공해주는 함수와 메서드를 사용해 계산할 수 있음.import org.apache.spark.sql.functions.{corr}df.stat.corr(\"Quantity\", \"UnitPrice\")df.select(corr(\"Quantity\", \"UnitPrice\")).show()SELECT corr(Quantity, UnitPrice)FROM dfTable하나 이상의 컬럼에 대한 요약 통계df.describe().show()// 아래 import를 통해 정확한 수치 집계 가능import org.apache.spark.sql.functions.{count, mean, stddev_pop, min, max}StatFunctions stat 속성을 사용해 다양한 통계값을 계산할 수 있음(approxQuantile 을 통한 데이터 백분위수 계산, 근사치 계산)val colName = \"UnitPrice\"val quantileProbs = Array(0.5)val relError = 0.05df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError) // 2.51// 교차표 조회df.stat.crosstab(\"StockCode\", \"Quantity\").show()// 자주 사용하는 항목쌍 조회df.stat.freqItems(Seq(\"StockCode\", \"Quantity\")).show()// 모든 로우에 고유 ID값 추가df.select(monotonically_increasing_id()).show(2)6.5 문자열 데이터 타입 다루기 거의 모든 데이터 처리 과정에서 발생. 정규 표현식, 데이터 치환, 문자열 존재 여부, 대/소문자 변환 처리 등 작업이 가능하다.대/소문자 변환 initcap() : 주어진 문자열에서 공백으로 나뉘는 모든 단어의 첫글자를 대문자로 변경 lowwer() : 전체를 소문자로 변경 upper() : 전체를 대문자로 변경import org.apache.spark.sql.functions.{initcap}df.select(initcap(col(\"Description\"))).show(2, false)df.select(col(\"Description\"), lower(col(\"Description\")), upper(lower(col(\"Description\")))).show(2)공백 제거/추가 lpad(), ltrim(), rpad(), rtrim(), trim()import org.apache.spark.sql.functions.{lit, ltrim, rtrim, rpad, lpad, trim}df.select( ltrim(lit(\" HELLO \")).as(\"ltrim\"), rtrim(lit(\" HELLO \")).as(\"rtrim\"), trim(lit(\" HELLO \")).as(\"trim\"), lpad(lit(\"HELLO\"), 3, \" \").as(\"lp\"), rpad(lit(\"HELLO\"), 10, \" \").as(\"rp\")).show(2)SELECT ltrim(' HELLLOOOO '), rtrim(' HELLLOOOO '), trim(' HELLLOOOO '), lpad('HELLOOOO ', 3, ' '), rpad('HELLOOOO ', 10, ' ')FROM dfTable6.5.1 정규 표현식 스파크에서는 regexp_extract, regexp_replace 함수를 제공합니다. 자바 정규 표현식 문법이 일반적인 문법과 약간 다르므로 사용 전 검토 필요정규 표현식을 이용한 문자열 치환import org.apache.spark.sql.functions.regexp_replaceval simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")val regexString = simpleColors.map(_.toUpperCase).mkString(\"|\")// the | signifies `OR` in regular expression syntaxdf.select( regexp_replace(col(\"Description\"), regexString, \"COLOR\").alias(\"color_clean\"), col(\"Description\")).show(2)SELECT regexp_replace(Description, 'BLACK|WHITE|RED|GREEN|BLUE', 'COLOR') as color_clean, DescriptionFROM dfTabletranslate 를 이용한 문자열 치환import org.apache.spark.sql.functions.translatedf.select(translate(col(\"Description\"), \"LEET\", \"1337\"), col(\"Description\")) .show(2)값의 존재여부 확인val containsBlack = col(\"Description\").contains(\"BLACK\")val containsWhite = col(\"DESCRIPTION\").contains(\"WHITE\")df.withColumn(\"hasSimpleColor\", containsBlack.or(containsWhite)) .where(\"hasSimpleColor\") .select(\"Description\").show(3, false)-- sql에서는 instr을 이용해 존재 여부 확인SELECT DescriptionFROM dfTableWHERE instr(Description, 'BLACK') &gt;= 1OR instr(Description, 'WHITE') &gt;= 16.6 날짜와 타임스탬프 데이터 타입 다루기 날짜와 시간은 프로그래밍 언어와 DB 분야의 변함없는 과제여서 스파크에서는 복잡함을 피하고자 시간 관련 정보만 집중적으로 관리합니다. 달력 형태의 date, 날짜와 시간 정보를 모두 가지는 timestamp입니다. 스파크의 TimestampType 클래스는 초 단위 정밀도까지만 지원함. 그 아래 단위까지 다뤄야 한다면 Long 데이터 타입으로 변환해 처리해야 한다. import org.apache.spark.sql.functions.{current_date, current_timestamp}import org.apache.spark.sql.functions.{date_add, date_sub}// 현재 날짜, 시간 구하기val dateDF = spark.range(10) .withColumn(\"today\", current_date()) .withColumn(\"now\", current_timestamp())dateDF.createOrReplaceTempView(\"dateTable\")dateDF.printSchema()// 현재 기준으로부터 5일 전후 날짜 구하기dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)두 날짜의 차이를 구하기 datediff 함수를 사용import org.apache.spark.sql.functions.{datediff, months_between, to_date}dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7)) .select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)dateDF.select( to_date(lit(\"2016-01-01\")).alias(\"start\"), to_date(lit(\"2017-05-22\")).alias(\"end\")) .select(months_between(col(\"start\"), col(\"end\"))).show(1) to_date 함수는 문자열을 날짜로 변환할 수 있는 함수로 날짜 포맷은 Java의 SimpleDateFormat 클래스가 지원하는 포맷을 사용해야 한다.import org.apache.spark.sql.functions.{to_date, lit}spark.range(5).withColumn(\"date\", lit(\"2017-01-01\")) .select(to_date(col(\"date\"))).show(1) 날짜를 파싱할 수 없는 경우 null을 반환함.6.7 null 값 다루기 DataFrame에서 빠져 있거나 비어있는 데이터를 표현할 때는 항상 null 값을 사용하는 것이 좋음. 사용자가 정의하는 대체 문자열이나 이런것을 사용하지 않아야 최적화 가능. DataFrame에서 .na 를 사용하는 것이 null을 다루는 기본적인 방식이다.6.7.1 coalesce coalesce 함수는 인자로 지정한 여러 컬럼 중 null이 아닌 첫번째 값을 반환import org.apache.spark.sql.functions.coalescedf.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()6.7.2 ifnull, nullIf, nvl, nvl2 coalesce 함수와 유사한 결과를 얻을 수 있는 몇가지 SQL 함수 ifnull : 첫번째 값이 null이면 두번쨰 값 반환하고, null이 아니면 첫번째 값을 반환. nullif : 두 값이 같으면 null 반환, 다르면 첫번째 값 반환 nvl : 첫번째 값이 null이면 두번쨰 값 반환 nvl2 : 첫번쨰 값이 null이 아니면 2번째 값을 반화하고, null이면 세번쨰 인수로 지정된 값을 반환(else_value)SELECT ifnull(null, 'return_value'), nullif('value', 'value'), nvl(null, 'return_value'), nvl2('not_null', 'return_value', \"else_value\")FROM dfTable LIMIT 16.7.3 drop drop 메서드는 null 값을 가진 로우를 제거하는 가장 간단한 함수 null 값을 가진 모든 로우를 제거df.na.drop()// 로우 컬럼 값 중 하나라도 nul인경우df.na.drop(\"any\")// 모든 컬럼 값이 null 또는 NaN인 경우df.na.drop(\"all\")SELECT *FROM dfTableWHERE Description IS NOT NULL6.7.4 fill fill 함수를 사용해 하나 이상의 컬럼을 특정 값으로 채울 수 있습니다.// String 데이터 타입의 컬럼이 존재하는 null 값을 5명으로 채워 넣는 방법df.na.fill(\"All Null values become this string\")// 다수의 컬럼에 적용하고 싶다면 다음으로 적용df.na.fill(5, Seq(\"StockCode\", \"InvoiceNo\"))// Map을 이용해서 다수의 컬럼에 fill 메소드 적용val fillColValues = Map(\"StockCode\" -&gt; 5, \"Description\" -&gt; \"No Value\")df.na.fill(fillColValues)6.7.5 replace replace 메소드를 이용해서 다른값으로 대체할 수 있습니다.df.na.replace(\"Description\", Map(\"\" -&gt; \"UNKNOWN\"))6.8 정렬하기 asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 함수를 사용해 DataFrame을 정렬할때 null 값이 표시되는 기준을 지정할 수 있습니다.6.9 복합 데이터 타입 다루기6.9.1 구조체 DataFrame 내부의 DataFrame을 구조체라 생각할 수 있습니다.df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\")df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\")import org.apache.spark.sql.functions.structval complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))complexDF.createOrReplaceTempView(\"complexDF\")complexDF.select(\"complex.Description\")complexDF.select(col(\"complex\").getField(\"Description\"))complexDF.select(\"complex.*\") 복합 데이터 타입을 가진 DataFrame, 유일한 차이점은 문법에 .을 사용하거나 getField 메소드를 사용한다는 점만 차이가 있음. * 문자를 사용해 모든 값을 조회할 수 있고, 모든 컬럼을 DataFrame의 최상위 수준으로 끌어 올릴 수 있음.6.9.2 배열split 구분자를 인수로 전달해 배열로 변환import org.apache.spark.sql.functions.splitdf.select(split(col(\"Description\"), \" \")).show(2)df.select(split(col(\"Description\"), \" \").alias(\"array_col\")) .selectExpr(\"array_col[0]\").show(2)배열의 길이import org.apache.spark.sql.functions.sizedf.select(size(split(col(\"Description\"), \" \"))).show(2) // shows 5 and 3array_contatins 값의 존재 유무 확인import org.apache.spark.sql.functions.array_containsdf.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)explode 배열 타입의 컬럼을 받아 포함된 모든 로우를 반환import org.apache.spark.sql.functions.{split, explode}df.withColumn(\"splitted\", split(col(\"Description\"), \" \")) .withColumn(\"exploded\", explode(col(\"splitted\"))) .select(\"Description\", \"InvoiceNo\", \"exploded\").show(2)6.9.3 Map key-value 쌍을 이용해 생성import org.apache.spark.sql.functions.mapdf.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(2)df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")) .selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)6.10 JSON 다루기 스파크에서 문자열 형태의 JSON을 직접 조작하거나 파싱하여 JSON 객체로 만들 수 있습니다.val jsonDF = spark.range(1).selectExpr(\"\"\" '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")import org.apache.spark.sql.functions.{get_json_object, json_tuple}jsonDF.select( get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\") as \"column\", json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2)jsonDF.selectExpr( \"get_json_object(jsonString, '$.myJSONKey.myJSONValue[1]') as column\", \"json_tuple(jsonString, 'myJSONKey')\").show(2)to_json StructType을 JSON 문자열로 변환import org.apache.spark.sql.functions.to_jsondf.selectExpr(\"(InvoiceNo, Description) as myStruct\") .select(to_json(col(\"myStruct\")))import org.apache.spark.sql.functions.from_jsonimport org.apache.spark.sql.types._val parseSchema = new StructType(Array( new StructField(\"InvoiceNo\",StringType,true), new StructField(\"Description\",StringType,true)))df.selectExpr(\"(InvoiceNo, Description) as myStruct\") .select(to_json(col(\"myStruct\")).alias(\"newJSON\")) .select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2)6.11 사용자 정의 함수 스파크의 가장 강력한 기능 중 하나는 사용 자 정의 함수(User Defined Function, UDF)를 사용할 수 있다. 파이썬이나 스칼라 등 외부 라이브러리를 사용해 사용자가 원하는 형태로 프랜스포메이션을 만들수 있게 한다. SparkSession이나 Context에서 사용할수 있도록 임시 함수 형태로 등록된다.val udfExampleDF = spark.range(5).toDF(\"num\")def power3(number:Double):Double = number * number * numberpower3(2.0) 이렇게 만들어진 함수는 모든 워커 노드에서 사용하려면 등록해야 한다. 함수를 개발한 언어에 따라 근본적으로 동작방식이 달라질 수 있는데, 스칼라나 자바로 함수를 작성했다면 JVM 환경에서만 사용할 수 있습니다.파이썬으로 UDF를 작성하는 경우 파이썬으로 작성한 함수라면 스파크는 워커 노드에 파이썬 프로세스를 실행하고 파이썬이 이해할 수 있는 포맷으로 모든 데이터를 직렬화해야 합니다. 이 과정에서 파이썬 프로세스에 대한 부하도 있고, 데이터 직렬화 문제가 있을수 있습니다. 가급적이면 자바나 스칼라로 사용자 정의 함수를 작성하는 것이 좋음.6.12 Hive UDF 하이브 문법을 사용해서 만든 UDF / UDAF도 사용할 수 있음. SparkSession 생성시 .enableHiveSupport() 명시해야함. 이렇게하면 SQL로 UDF를 정의등록할 수 있음. TEMPORARY 키워드 여부에 따라 하이브 메타스토어에 영구 함수로 등록할수도 있음. CREATE TEMPORARY FUNCTION myFunc AS `com.organization.hive.udf.FunctionName`Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 5. 구조적 API 기본 연산", "url": "/posts/spark-guide-5/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2022-10-29 20:21:00 +0900", "snippet": "[스파크 완벽 가이드] 5. 구조적 API 기본 연산5.1 스키마 스키마는 DataFrame의 컬럼명과 데이터 타입을 정의(데이터소스로부터 얻거나 직접 정의) 스키마는 여러 개의 StructField 타입 필드로 구성된 StructType 객체 이름, 데이터 타입, 컬럼의 nullable 여부를 가짐.import org.apache.spark.s...", "content": "[스파크 완벽 가이드] 5. 구조적 API 기본 연산5.1 스키마 스키마는 DataFrame의 컬럼명과 데이터 타입을 정의(데이터소스로부터 얻거나 직접 정의) 스키마는 여러 개의 StructField 타입 필드로 구성된 StructType 객체 이름, 데이터 타입, 컬럼의 nullable 여부를 가짐.import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}import org.apache.spark.sql.types.Metadataval myManualSchema = StructType(Array( StructField(\"DEST_COUNTRY_NAME\", StringType, true), StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true), StructField(\"count\", LongType, false, Metadata.fromJson(\"{\\\"hello\\\":\\\"world\\\"}\"))))val df = spark.read.format(\"json\").schema(myManualSchema) .load(\"/data/flight-data/json/2015-summary.json\") 프로그래밍 언어의 데이터 타입을 스파크 데이터 타입으로 설정할 수 없다는점에 유의5.2 컬럼과 표현식 사용자는 표현식으로 DataFrame의 컬럼을 선택, 조작, 제거할 수 있음. DataFrame을 통해 컬럼에 접근하여야 하고, 수정하려면 DataFrame의 트랜스포메이션을 사용해야 함.5.2.1 컬럼 col(), colum()을 사용하는것이 가장 간단import org.apache.spark.sql.functions.{col, column}col(\"someColumnName\")column(\"someColumnName\")명시적 컬럼 참조 DataFrame의 컬럼은 col 메서드로 참조하고, 이는 데이터 JOIN시 유용하게 사용할 수 있음.5.2.2 표현식 표현식은 DataFrame 레코드의 여러 값에 대한 트랜스포메이션의 집합을 의미. expr(\"someCol\"), col(\"someCol\")표현식으로 컬럼 표현 트랜스포메이션을 수행하려면 반드시 컬럼 참조를 사용해야 함.(((col(\"someCol\") + 5) * 200) - 6) &lt; col(\"otherCol\")DataFrame 컬럼에 접근하기spark.read.format(\"json\").load(\"/data/flight-data/json/2015-summary.json\") .columns5.3 레코드와 로우 DataFrame의 각 로우는 하나의 레코드이며, 스파크에서는 Row 객체로 표현된다.df.first()5.3.1 로우 생성 Row 객체는 스키마 정보를 가지고 있지 않음. Row 객체를 직접 생성하려면 DataFrame의 스키마와 같은 순서로 값을 명시해야 함.import org.apache.spark.sql.Rowval myRow = Row(\"Hello\", null, 1, false)// 스칼라 버전myRow(0) // type AnymyRow(0).asInstanceOf[String] // StringmyRow.getString(0) // StringmyRow.getInt(2) // Int5.4 DataFrame의 트랜스포메이션 로우나 컬럼 추가/제거, 로우-&gt;컬럼 변환, 로우 순서 변경 등 처리5.4.1 DataFrame 생성 원시 데이터소스에서 DataFrame을 생성할 수 있음.val df = spark.read.format(\"json\") .load(\"/data/flight-data/json/2015-summary.json\")df.createOrReplaceTempView(\"dfTable\")import org.apache.spark.sql.Rowimport org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}val myManualSchema = new StructType(Array( new StructField(\"some\", StringType, true), new StructField(\"col\", StringType, true), new StructField(\"names\", LongType, false)))val myRows = Seq(Row(\"Hello\", null, 1L))val myRDD = spark.sparkContext.parallelize(myRows)val myDf = spark.createDataFrame(myRDD, myManualSchema)myDf.show()5.4.2 select, selectExpr 데이터 테이블에 SQL을 사용한것처럼 DataFrame에서도 사용할 수 있음.df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)SELECT DEST_COUNTRY_NAME, 0RIGIN_COUNTRY_NAMEFROM dfTableLIMIT 2selectExpr 새로운 DataFrame을 생성하는 복잡한 표현식으로 간단하게 만들어주는 도구 모든 유효한 비집계형 SQL 구문을 지정할 수 있음.df.selectExpr( \"*\", // include all original columns \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\") .show(2)SELECT *, (DEST=COUNTRY NAME = 0RIGIN COUNTRY_NAME) as withinCountryFROM dfTableLIMIT25.4.3 스파크 데이터 타입으로 변환하기 때로는 새로운 컬럼이 아닌 명시적인 값을 스파크에 전달해야 함.(ex. 상수) 리터럴을 사용할 수 있음.import org.apache.spark.sql.functions.litdf.select(expr(\"*\"), lit(1).as(\"One\")).show(2)SELECT *, 1 as OneFROM dfTableLIMIT 25.4.4 컬럼 추가하기 DataFrame의 withColumn 메서드롤 사용해서 신규 컬럼을 추가할 수 있음.df.withColumn(\"numberOne\", lit(1)).show(2)SELECT *, 1 as numberOneFROM dfTableLIMIT 25.4.5 컬럼명 변경하기 withColumn 대신 withColumnRenamed 메서드로 컬럼명을 변경할 수 있음.df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns// ... dest, ORIGIN-COUNTRY_NAME, count5.4.6 예약 문자와 키워드 공백이나 하이픈(-) 같은 예약 문자는 컬럼명에 사용할 수 없음. 이를 사용하기 위해서는 백틱(`) 문자을 이용해 escaping해야 함.dfWithLongColName.selectExpr( \"`This Long Column-Name`\", \"`This Long Column-Name` as `new col`\") .show(2)SELECT `This Long Column-Name`, `This Long CoIumn-Name` as `new-col`FROM dfTableLongLIMIT 2 표현식 대신 문자열을 사용해서 명시적으로 컬럼을 참조하면 리터럴로 해석되기 떄문에 예약 문자가 포함된 컬럼을 참조할 수 있음.5.4.7 대소문자 구분 기본적으로 스파크에서는 대소문자를 구별하지 않으나, 설정을 통해 구분하게 할 수 있음.set spark.sql.caseSensitive true5.4.8 컬럼 제거하기df.drop(\"ORIGIN_COUNTRY_NAME\").columns// 다수의 컬럼 한번에 제거dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\")5.4.9 컬럼의 데이터 타입 변경df.withColumn(\"count2\", col(\"count\").cast(\"long\"))5ELECT *, cast(count as string) AS count2FROM dfTable5.4.10 로우 필터링하기 true / false을 판별하는 표현식을 만들어 필터링을 할 수 있음. where 또는 filter 메소드롤 이용해서 필터링(같은 동작)df.filter(col(\"count\") &lt; 2).show(2)df.where(\"count &lt; 2\").show(2)SELECT *FROM dfTableWHERE count &lt; 2LIMIT 2여러 필터 적용df .where(col(\"count\") &lt; 2) .where(col(\"ORIGIN_COUNTRY_NAME\") =!= \"Croatia\") .show(2)SELECT *FROM dfTableWHERE count &lt; 2AND 0RIGIN_COUNTRY_NAME != 'Croatia'LIMIT 25.4.11 고유한 로우 얻기df .select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\") .distinct() .count()SELECT COUNT(DISTINCT(0RIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME))FROM dfTable5.4.12 무작위 샘플 만들기 sample 메서드를 이용해서 무작위 샘플 데이터를 얻을수 있음. 표본 데이터 추출 비율, 복원 추출 또는 비복원 추출의 사용 여부를 지정할 수 있음.val seed = 5val withReplacement = falseval fraction = 0.5df.sample(withReplacement, fraction, seed).count()5.4.13 임의 분할하기 원본 DataFrame의 임의 크기로 분할할때 유용하다.val dataFrames = df.randomSplit(Array(0.25, 0.75), seed)dataFrames(0).count() &gt; dataFrames(1).count() // False5.4.14 로우 합치기와 추가하기 DataFrame은 불변성이기 때문에 레코드 추가하는 작업은 구조적으로 불가능. 레코드를 추가하려면 원본 DF에 새로운 DF를 통합해야 함. 반드시 동일한 스키마와 컬럼수를 가져야 함.import org.apache.spark.sql.Rowval schema = df.schemaval newRows = Seq( Row(\"New Country\", \"Other Country\", 5L), Row(\"New Country 2\", \"Other Country 3\", 1L))val parallelizedRows = spark.sparkContext.parallelize(newRows)val newDF = spark.createDataFrame(parallelizedRows, schema)df.union(newDF) .where(\"count = 1\") .where($\"ORIGIN_COUNTRY_NAME\" =!= \"United States\") .show() // get all of them and we'll see our new rows at the end 컬럼 표현식과 문자열을 비교할떄 =!= 연산자를 사용하면 컬럼의 실제 값을 비교 대상 문자열과 비교함. =!=, === 5.4.15 로우 정렬하기 sort와 orderBy 메소드를 사용해 최대값 혹은 최솟값이 상단에 위치하도록 정렬할 수 있음.import org.apache.5park.Sql.functions.{desc, asc}df.sort(\"count\").show(5)df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)df.orderBy(desc(\"count\"), asc(\"DEST_COUNTRY_NAME\")).show(2) asc_nulls_first, desc_null_first와 같은 메소드를 사용하여 렬된 DataFrame에서 null 값 표시 기준을 지정할 수 있음.5.4.16 로우 수 제한하기 추출할 로우수 제한을 하는 경우 사용df.limit(5).show()5.4.17 repartition와 coalesce 자주 필터링하는 컬럼을 기준으로 데디터를 분할하여 최적화할 수 있다. 파티셔닝 스키마와 파티션 수를 포함한 물리적인 데이터 구성 제어 repartition을 하면 무조건 전체 데이터 셔플이 발생함. 파티션 수가 현재보다 많거나 컬럼을 기준으로 파티션을 만들어야 하는 경우에만 사용df.rdd.getNumPartitions // 1df.repartition(5)// 특정 컬럼을 기준으로 자주 필터링한다면 이를 기준으로 파티션을 재분배하는것이 좋음.df.repartition(col(\"DEST_COUNTRY_NAME\"))// 선택적으로 파티션 수를 지정할 수 있음.df.repartition(5, col(\"DEST_COUNTRY_NAME\"))coalesec 전체 데이터를 셔플하지 않고 파티션을 병합하는 경우에 사용df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)5.4.18 드라이버로 로우 데이터 수집하기 로컬환경에서 데이터를 다루려면 드라이버로 데이터를 수집해야 함.val collectDF = df.limit(10)collectDF.take(5) // take works with an Integer countcollectDF.show() // this prints it out nicelycollectDF.show(5, false)collectDF.collect()전체 데이터셋에 대한 반복 처리를 위해 드라이버로 로우를 모으는 다른 방법collectDF.toLocalIterator() 드라이버로 모든 데이터 컬렉션을 수집하는 경우 매우 큰 비용(CPU, 메모리, 네트워크 등)이 발생하므로 데이터와 필요에 따라 적절히 선택되어야 한다.Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 4. 구조적 API 개요", "url": "/posts/spark-guide-4/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2022-10-29 19:22:00 +0900", "snippet": "[스파크 완벽 가이드] 4. 구조적 API 개요4.1 DataFrame과 Dataset 2가지 모두 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션. 각 컬럼은 다른 컬럼과 동일한 수의 로우를 가져야 함.(값이 없는 경우 null일수는 있음)4.2 스미카 DataFrame의 컬럼명과 데이터 타입을 정의 스키마는 데이터 소스로부터 얻거...", "content": "[스파크 완벽 가이드] 4. 구조적 API 개요4.1 DataFrame과 Dataset 2가지 모두 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션. 각 컬럼은 다른 컬럼과 동일한 수의 로우를 가져야 함.(값이 없는 경우 null일수는 있음)4.2 스미카 DataFrame의 컬럼명과 데이터 타입을 정의 스키마는 데이터 소스로부터 얻거나 직접 정의할 수 있음.4.3 스파크의 구조적 데이터 타입 개요 스파크는 하나의 프로그래밍 언어로 봐도 좋음. 실행계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 카탈리스트 엔진을 사용함.val df = spark.range(500).toDF(\"number\")df.select(df.col(\"number\") + 10) 이런 덧셈이 가능한 이유는 카탈리스트 엔진에서 스파크의 데이터 타입으로 변환해 명령을 처리하기 때문이다.4.3.1 DataFrame과 Dataset 비교 DataFrame : 비타입형 / Dataset : 타입형 실제 코드레벨에서 DataFrame에도 데이터 타입이 있으나, 스키마에 명시된 타입의 일치 여부는 런타임이 되어야 확인함. 반면, Dataset은 스키마에 명시된 데이터 타입의 일치 여부를 컴파일 타임에 확인함.(Java / Scala에서만 지원) 중요한 점은 DataFrame을 사용하면 스파크의 최적화된 내부 포맷을 사용할수 있다는 사실이고, 이를 사용하면 스파크가 지원하는 어떤 언어의 API를 사용하더라도 동일한 효과와 효율성을 얻을 수 있음.4.3.2 컬럼 정수형, 문자열 같은 단순 데이터 타입과 배열이나 맵 같은 복합 데이터 타입, null 값 표현4.3.3 로우 데이터 레코드, DataFrame의 레코드는 Row 타입으로 구성. 로우는 SQL, RDD, 데이터 소스에서 얻거나 직접 만들수 있음.spark.range(2).toDF().collect()4.3.4 스파크 데이터 타입 스파크는 내부 데이터 타입을 가지고 있음. 실제 langauge 데이터 타입과의 맵핑을 통해 처리할 수 있음.4.4 구조적 API의 실행 과정 스파크 코드가 클러스터에서 실제 처리되는 과정을 설명처리 과정 DataFrame/DataSet/SQL을 이용해 코드 작성 정상적인 코드라면 스파크가 논리적 실행 계획 변환 논리적 실행 계획을 물리적 실행 계획으로 변환, 추가적인 최적화 확인 클러스터에서 물리적 실행계획(RDD 처리) 실행카탈리스트 옵티마이저 코드를 넘겨 받아 실행 계획을 생성하는 역할4.4.1 논리적 실행 계획 추상적 트랜스포메이션만 포함되는 단계 이 단계에서는 드라이버/익스큐터 정보를 고려하지 않음.-먼저 검증 없이 논리적 실행 계획으로 변환한 뒤 카탈로그, 저장소, DataFrame 정보를 활용하여 검증된 논리적 실행계획을 생성. 이 논리적 실행 계획에 조건절 푸시 다운 등의 최적화 단계를 거쳐서 최종 논리적 실행 계획이 만들어진다.4.4.2 물리적 실행 계획 이어서 스파크 실행 계획이라고 불리는 물리적 실행 계획이 생성됨. 클러스터 환경에서 실행하는 방법에 대한 정의 물리적 실행 전략을 생성하고, 비용 모델을 이용해 비교 후 최적의 전략을 선택함.(테이블 크기, 팣티션 수 등 물리적 속성 고려) 일련의 RDD와 트랜스포메이션으로 컴파일되어 실행됨.4.4.3 실행 런타임에 전체 태스크나 스테이지를 제거할 수 있는 자바 바이트 코드를 생성해 추가적인 최적화를 수행하고, 결과를 사용자에게 반환Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 3. 스파크 기능 둘러보기", "url": "/posts/spark-guide-3/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2022-10-29 18:52:00 +0900", "snippet": "[스파크 완벽 가이드] 3. 스파크 기능 둘러보기3.1 스파크의 기능3.2 Dataset : 타입 안정성을 제공하는 구조적 API 자바와 스칼라의 정적 데이터 타입 코드를 지원하기 위해 고안된 구조적 API 이는 타입 안정성을 지원하고, 동적 타입 언어인 파이썬과 R에서는 사용할수 없음.case class Flight(DEST_COUNTRY_NAM...", "content": "[스파크 완벽 가이드] 3. 스파크 기능 둘러보기3.1 스파크의 기능3.2 Dataset : 타입 안정성을 제공하는 구조적 API 자바와 스칼라의 정적 데이터 타입 코드를 지원하기 위해 고안된 구조적 API 이는 타입 안정성을 지원하고, 동적 타입 언어인 파이썬과 R에서는 사용할수 없음.case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME : String, count: BigInt) { val flightsDF = spark.read .parquet(\"/data/flight-data/parquet/2010-summray.parquet/\") val flights = flightsDF.as[Flight] flights .filter(flightRow =&gt; flightRow.0RIGIN_COUNTRY_NAME != ''Canada'') .map(flightRow =&gt; flightRow) .take(5)}3.3 구조적 스트리밍 구조적 스트리밍은 안정화된 스트림 처리용 고수준 API 구조적 APi로 개발된 배치 모드의 연산을 스트리밍 방식으로 실행할 수 있음.스트리밍이 아닌 일반적인 readval staticDataFrame = spark.read.format(\"csv\") .option(\"header\", \"true\") .option(\"inferSchema\", \"true\") .load(\"/data/retail-data/by-day/*.csv\")staticDataFrame.createOrReplaceTempView(\"retail_data\")val staticSchema = staticDataFrame.schemaimport org.apache.spark.sql.functions.{window, column, desc, col}staticDataFrame .selectExpr( \"CustomerId\", \"(UnitPrice * Quantity) as total_cost\", \"InvoiceDate\") .groupBy( col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\")) .sum(\"total_cost\") .show(5)스트리밍 방식으로 처리 read 대신 readStream을 사용한 것이 가장 큰 차이점. 아래 예제와 같은 방식을 운영 환경에 적용하는것은 추천하지 않음.val streamingDataFrame = spark.readStream .schema(staticSchema) .option(\"maxFilesPerTrigger\", 1) .format(\"csv\") .option(\"header\", \"true\") .load(\"/data/retail-data/by-day/*.csv\")// 총 판매금액 계산val purchaseByCustomerPerHour = streamingDataFrame .selectExpr( \"CustomerId\", \"(UnitPrice * Quantity) as total_cost\", \"InvoiceDate\") .groupBy( col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\")) .sum(\"total_cost\")purchaseByCustomerPerHour.writeStream .format(\"memory\") // memory = store in-memory table .queryName(\"customer_purchases\") // the name of the in-memory table .outputMode(\"complete\") // complete = all the counts should be in the table .start() 스트림이 시작되면 실행 결과가 어떠한 형태로 인메모리 테이블에 기록되는지 확인할수 있음.3.4 머신러닝과 고급 분석 스파크는 내장된 MLlib를 사용해 대규모 머신러닝을 수행할 수 있음. 대용량 데이터를 대상으로 preprocessing, munging(rawdata를 접근/분석이 쉽도록 가공하는 행위) , model training, prediction 등을 할수 있음. classfication, regression, clustering, deep learning 등 정교한 API 제공3.5 저수준 API 스파크는 RDD를 통해 자바와 파이썬 객체를 다루는 데 필요한 다양한 기능을 제공함. 스파크의 거의 모든 기능은 RDD를 기반으로 만들어졌음. 파티션 제어 등 DataFrame보다 더 세밀한 제어가 가능함. RDD는 toDF() 와 같은 API를 통해 손쉽게 DataFrame으롭 변환할 수 있음. 기본적으로 사용이 권장되지는 않으나, 비정형 데이터나 정제되지 않은 rawdata를 처리해야 한다면 사용할 일이 있을수도 있음.spark.sparkContext.parallelize(Seq(1, 2, 3)).toDF()3.6 SparkR R 언어를 사용하기 위한 기능을 제공함.3.7 스파크 에코시스템과 패키지 스파크의 최고 장점은 커뮤니티가 만들어낸 패키지 에코시스템과 다양한 기능. 개발자 누구나 자신이 개발한 패키지를 공개할수 있는 spark-packages.org 저장소가 있음.Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 2. 스파크 간단히 살펴보기", "url": "/posts/spark-guide-2/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2022-10-29 16:46:00 +0900", "snippet": "[스파크 완벽 가이드] 2. 스파크 간단히 살펴보기2.1 스파크의 기본 아키텍쳐2.1.1 스파크 애플리케이션 driver 프로세스와 executor 프로세스로 구성됨. driver 프로세스는 zeppelin 같은 BI 툴이 될수도 있고, spring application같은것이 될수도 있음.로컬 모드 위 그림과 같이 클러스터 모드로도 동작할수 있...", "content": "[스파크 완벽 가이드] 2. 스파크 간단히 살펴보기2.1 스파크의 기본 아키텍쳐2.1.1 스파크 애플리케이션 driver 프로세스와 executor 프로세스로 구성됨. driver 프로세스는 zeppelin 같은 BI 툴이 될수도 있고, spring application같은것이 될수도 있음.로컬 모드 위 그림과 같이 클러스터 모드로도 동작할수 있으나, 로컬 모드도 지원함. 드라이버, 익스큐터는 단순 프로세스 단위로 동작할 수 있음. 로컬 모드로 실행될 경우 단일 머신에서 스레드 형태로 실행됨.2.2 스파크의 다양한 언어 API 스칼라, 자바, 파이썬, SQL, R 등 다양한 언어를 제공함. kotlin 진영에서 https://github.com/Kotlin/kotlin-spark-api 와 같이 spark-api 라이브러리를 제공하기도 하여 최근 JVM 진형 트랜드에 맞게 개발하는 것도 가능.SparkSession과 다른언어 API와의 관계 스파크가 사용자를 대신해 다른 언어로 작성된 코드를 JVM에서 실행될수 있는 코드로 변환하여 실행됨.2.3 Spark API 저수준 비구조적 API : RDD 고수준 구조적 API, Dataset, DataFrame2.4 Spark Session 스파크 애플리케이션은 SparkSession이라 불리는 드라이버 프로세스를 통해 제어됨. SparkSession은 사용자가 정의한 처리 명령을 클러스터에서 실행함.2.5 DataFrame 가장 대표적인 구조적 API 테이블 데이터를 로우와 컬럼으로 단순하게 표현(스키마) 그림과 같이 분산된 데이터를 하나의 Dataset으로 프로세싱 할수 있음.2.5.1 파티션 스파크는 모든 익스큐터가 병렬로 작업을 수행할수 있도록 파티션이라 불리는 chunk 단위로 데이터를 분할 파티션은 클러스터의 물리 머신에 존재하는 로우의 집합을 의미함. 파티션이 1이라면 수천개의 익스큐터가 있는 분산환경이더라도 병렬성은 1이 됨.(그 반대도 마찬가지)2.6 트랜스포메이션 스파크의 핵심 데이터 구조는 불변성(immutable)을 가짐. DataFrame을 변경하는 방법을 트랜스포메이션이라 지칭.2.6.1 좁은 의존성(narrow dependency) 하나의 입력 파티션이 하나의 출력 파티션에만 영향을 미치는 경우 이 케이스에서는 모든 작업들이 메모리를 통해 파이프라이닝 처리됨.2.6.2 넓은 의존성(wide dependency) 하나의 입력 파티션이 여러개의 출력 파티션에 영향을 미치는 경우 좁은 의존성과 달리 셔플이 발생하면 이를 셔플 결과를 디스크에 저장하여 처리됨. 이 최적화를 어떻게 하냐에 따라 작업의 성능에 큰 영향이 미침. 2.6.3. 지연 연산 스파크는 연산명령을 즉시 처리하는 방식이 아니라 실행계획을 먼저 생성하면서 최적화가 이루어진다.조건절 푸시다운 하나의 로우만 가져오는 필터를 가지고있다면 실제 데이터를 1개만 읽는것이 효율적인데 이를 자동적으로 수행하고 최적화됨.2.7 액션 실제 연산 수행을 위해서는 액션 명령을 실행해야 함. Java Stream API의 중단연산과 종단연산과 유사하다고 이해할수 있음.divisBy .count()Reference Spark 완벽 가이드" }, { "title": "[스파크 완벽 가이드] 0. 스파크 시작하기", "url": "/posts/spark-guide/", "categories": "DevLog, Spark", "tags": "Spark", "date": "2022-10-21 16:46:00 +0900", "snippet": "스파크 시작하기 곧 업무에서 스파크를 사용해야 해서 스파크를 재대로 공부해봐야겠다고 생각했다. 그래서 일단 local 환경에서 spark를 설치하고, 간단한 작업들을 해보려고 한다.1. 스파크 설치 2020 m1 osx 를 기준으로 작성되었음을 알려드립니다.# java 설치arch -arm64 brew install openjdk@11# sca...", "content": "스파크 시작하기 곧 업무에서 스파크를 사용해야 해서 스파크를 재대로 공부해봐야겠다고 생각했다. 그래서 일단 local 환경에서 spark를 설치하고, 간단한 작업들을 해보려고 한다.1. 스파크 설치 2020 m1 osx 를 기준으로 작성되었음을 알려드립니다.# java 설치arch -arm64 brew install openjdk@11# scala 설치arch -arm64 brew install scala# Apache sparkarch -arm64 brew install apache-spark 이렇게만 설치하면 spark-shell을 사용할 준비가 모두 끝났습니다.2. 트러블 슈팅 위처럼 설칠하고 spark-shell을 실행시켜보니 정상적으로 구동되지 않았다. 원인은 hostname 설정 관련 문제가 있는듯 했다.sudo hostname -s 127.0.0.13. spark-shell 실행import spark.implicits._val data = Seq((\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\"))val df = data.toDF()df.show() 이제 책 예제를 실행시켜볼 준비 완료!4. Spark UI 로컬로 구동시킨 Spark Job의 상태를 확인할수 있는 방법을 제공 spark 3.3.0 기준으로 http://localhost:4040/ 에서 확인하실 수 있습니다. 스피크 UI에서 스피크 Job 상태, 환경 설정, 클러스터 상태 등의 정보를 확인할 수 있습니다. 스파크 UI는 스피크 집을 튜닝하고 디버깅할 때 매우 유용하다고 합니다.Reference https://sparkbyexamples.com/spark/install-apache-spark-on-mac/ https://stackoverflow.com/questions/34601554/mac-spark-shell-error-initializing-sparkcontext" }, { "title": "[Hands-On Reactive Programming in Spring 5] 9. 리액티브 애플리케이션 테스트하기", "url": "/posts/spring5-reactive-9/", "categories": "DevLog, Spring", "tags": "Java, Spring, Hands-On Reactive Programming in Spring 5, Reactive", "date": "2022-07-26 13:28:00 +0900", "snippet": "9. 리액티브 애플리케이션 테스트하기 테스트 도구에 대한 요구사항 StepVerifier를 사용해 Publisher를 테스트할 때의 핵심 고급 StepVerifier 사용 시나리오 웹플럭스 테스트를 위한 툴 세트9.1 리액티브 스트림을 테스트하기 어려운 이유 대규모 시스템에 많은 수의 클래스가 포함된 수많은 컴포넌트가 있고, 테스트 피라미드(...", "content": "9. 리액티브 애플리케이션 테스트하기 테스트 도구에 대한 요구사항 StepVerifier를 사용해 Publisher를 테스트할 때의 핵심 고급 StepVerifier 사용 시나리오 웹플럭스 테스트를 위한 툴 세트9.1 리액티브 스트림을 테스트하기 어려운 이유 대규모 시스템에 많은 수의 클래스가 포함된 수많은 컴포넌트가 있고, 테스트 피라미드(Test Pyramid) 제안을 따라야 모든 것을 재대로 검증할 수 있다. 리액티브 프로그래밍이 리소스 최적화에 도움을 주고, 리액터를 사용할 경우 지저분한 코드(콜백 지옥 등)에도 유용하지만 테스트하는 데에는 어려움이 있다. Publisher 인터페이스를 사용해 스트림을 게시하고 Subscriber 인터페이스를 구현하고 게시자의 스트림을 수집해서 그 정확성을 확인할 수 있는데, 이 방식을 테스트로 검증하는게 쉽지 안흥ㅁ.Test Pyramid ( https://eunjin3786.tistory.com/86 ) Y축 위로 올라갈 수록 실제로 돌아가는, 유저가 쓰는 영역이라 믿을 만한 영역입니다. 하지만 실행시간이 오래 걸리고 유지보수, 디버깅 하기 더욱 어려운 영역입니다. X축은 밑으로 내려 올 수록 더 많은 테스트 코드를 작성하게 됩니다. 피라미드 모델 접근법은 철저성(thoroughness), 품질(quality), 실행 속도(execution speed) 사이의 균형을 잡는 데 도움이 된다고 합니다. 그리고 이 피라미드는 세 가지 다른 테스트 유형 간의 균형을 유지하는 것도 도와준다고 합니다.9.2 StepVerifier를 이용한 리액티브 스트림 테스트 Reactor Test 모듈을 사용하여 리액티브 스트림을 테스트해봅시다.9.2.1 StepVerifier의 핵심요소StepVerifier &lt;T&gt; create (Publisher&lt;T&gt; Soruce) 원소가 올바른 순서로 생성되는지를 확인 빌더 기법을 사용하여 이벤트가 발생하는 순서를 정의하여 검증할 수 있다. 검증을 실행하려면 .verify() blocking 호출 메소드를 호출해야 함.StepVerifier\t.create(Flux.just(\"foo\",\"bar\"))\t.expectSubscription()\t.expectNext(\"foo\")\t.expectNext(\"bar\") // stream 순서가 다르면 Error 발생\t.expectComplete()\t.verify();StepVerifier\t.create(Flux.range(0,100))\t.expectSubscription()\t.expectNext(0)\t.expectNextCount(98)\t.expectNext(99)\t.expectComplete()\t.verify(); StepVerifier를 사용하면 자바 Hamcrest(Matcher)와 같은 도구를 사용해 스트림 데이터 기록과 검증을 한번에 할 수 있음.Publisher&lt;Wallet&gt; usersWallets = findAllUsersWallets();StepVerifier .create(usersWallets) .expectSubscription() .recordWith(ArrayList::new) .expectNextCount(1) .consumeRecordedWith(wallets -&gt; assertThat(wallets, everyItem(hasProperty(\"ownner\", equalTo(\"admin\"))))) .expectComplete() verify(); 기대값에 대해 기본 builder method를 통해서도 검증할 수 있음.StepVerifier\t.create(Flux.just(\"alpha-foo\", \"betta-bar\"))\t.expectSubscription()\t.expectNextMatches(e -&gt; e.startsWith(\"alpha\"))\t.expectNextMatches(e -&gt; e.startsWith(\"betta\"))\t.expectComplete()\t.verify(); assertNext(), consumeNextWith() 등이 있음. Error 발생에 대한 검증 StepVerifier .create(Flux.error(new RuntimeException(\"Error\"))) .expectError() .verify(); 9.2.2 StepVerifier을 이용한 고급 테스트 리액티브 스트림 스펙에 따르면 무한 스트림은 Subscriber#onComplete() 메소드를 호출하지 않는데 이런 경우 이전에 학습한 테스트 검증을 사용할 수 없음. 이러한 처리를 위한 구독 취소 API를 제공합니다.\t\tFlux&lt;String&gt; infiniteStream = ... // 무한스트림\t\tStepVerifier\t\t\t.create(infiniteStream)\t\t\t.expectSubscription()\t\t\t.expectNext(\"Connected\")\t\t\t.expectNext(\"Price: $12.00\")\t\t\t.thenCancel()\t\t\t.verify();BackPressure 검증 thenRequest()를 통해 BackPressure를 검증할수 있다. 하지만 이 경우 어보플로가 발생할 수 있음.\t\tFlux&lt;String&gt; infiniteStream = Flux.just(\"Connected\",\"Price: $12.00\",\"3\",\"4\",\"5\",\"6\",\"7\");\t\tStepVerifier\t\t\t.create(infiniteStream.onBackpressureBuffer(5), 0)\t\t\t.expectSubscription()\t\t\t.thenRequest(1)\t\t\t.expectNext(\"Connected\")\t\t\t.thenRequest(1)\t\t\t.expectNext(\"Price: $12.00\")\t\t\t.expectError(Exceptions.failWithOverflow().getClass())\t\t\t.verify(); then()을 통해 특정 검증 후에 추가 작업을 실행할 수 있다.\t\tTestPublisher&lt;String&gt; idsPublisher = TestPublisher.create()\t\tStepVerifier\t\t\t.create(walletsRepository.findAllByid(idsPublisher))\t\t\t.expectSubscription()\t\t\t.then(() -&gt; idsPublisher.next(\"1\"))\t\t\t.assertNext(w -&gt; assertThat(w, hasProperty(\"id\", equalsTo(\"1\"))))// ...// ...\t\t\t.expectComplete()\t\t\t.verify();9.2.3 가상 시간 다루기\t\tFlux&lt;String&gt; sendWithInterval = Flux.interval(Duration.ofMinutes(1))\t\t\t\t.zipWith(Flux.just(\"a\",\"b\",\"c\"))\t\t\t\t.map(Tuple2::getT2);\t\tStepVerifier\t\t\t.create(sendWithInterval)\t\t\t.expectSubscription()\t\t\t.expectNext(\"a\", \"b\",\"c\")\t\t\t.expectComplete()\t\t\t.verify(); 위 코드를 테스트하려면 평균 테스트 시간이 3분이 넘게 걸림. Flux&lt;String&gt; sendWithInterval이 1분씩 지연시키기 때문인데 withVirtualTime() build 메소드를 사용하여 이를 개선할 수 있다.@Test\t\tStepVerifier\t\t\t.withVirtualTime(() -&gt; Flux.interval(Duration.ofMinutes(1))\t\t\t\t.zipWith(Flux.just(\"a\", \"b\", \"c\"))\t\t\t\t.map(Tuple2::getT2))\t\t\t.expectSubscription()//\t\t\t.then(() -&gt; VirtualTimeScheduler.get().advanceTimeBy(Duration.ofMinutes(3)))\t\t\t.thenAwait(Duration.ofMinutes(3))\t\t\t.expectNext(\"a\", \"b\",\"c\")\t\t\t.expectComplete()\t\t\t.verify(); VirtualTimeScheduler API와 함께 .then()을 사용해 특정 시간만큼 시간을 처리할 수 있다. 이는 .thenAwait()로 대체할 경우에도 동일한 효과를 얻을 수 있음.9.2.4 리액티브 컨텍스트 검증하기 리액터 컨텍스트르 검증하기 위해서 다음과 같은 방식을 제공\t\tStepVerifier\t\t\t.create(securityService.login(\"admin\", \"admin\"))\t\t\t.expectSubscription()\t\t\t.expectAccessibleContext()\t\t\t.hasKey(\"security\")\t\t\t.then()\t\t\t.expectComplete()\t\t\t.verify();9.2.5 웹플럭스 테스트 교재에 있는 예제를 동작시켜보려 했으나 잘안됩니다.ㅠm1 mac에서는 embed mongo를 사용할수 없으며, local환경에서 docker-compose를 통해 mongo를 띄우고 처리하려고 해도 예제코드의 webClient api 호출을 위한 서버는 동작하지 않아서 api call 후에 reactive mongo repo에 데이터가 셋되는것을 확인이 잘 안됨.api를 localhost에 만들고 처리하는것도 해봤는데 뭔가 잘안되서 스킵..9.2.6 WebTestClient를 이용해 컨트롤러 테스트하기@Test\tpublic void verifyPaymentsWasSentAndStored() {\t\tMockito.when(exchangeFunction.exchange(Mockito.any()))\t\t .thenReturn(Mono.just(MockClientResponse.create(201, Mono.empty())));\t\tclient.post()\t\t .uri(\"/payments/\")\t\t .syncBody(new Payment())\t\t .exchange()\t\t .expectStatus().is2xxSuccessful()\t\t .returnResult(String.class)\t\t .getResponseBody()\t\t .as(StepVerifier::create)\t\t .expectNextCount(1)\t\t .expectComplete()\t\t .verify();\t\tMockito.verify(exchangeFunction).exchange(Mockito.any());\t} WebMVC와 비슷한 방식으로 웹플럭스 애플리케이션에서 HTTP 통신을 통한 외부 호출을 모킹하는 것이 가능함. 이 경우 모든 HTTP 통신이 WebClient로 구현된다는 가정하에 테스트가 수행되고, http client 라이브러리가 변경되면 더이상 테스트는 유효하지 않게 된다.9.2.7 웹소켓 테스트 교재에 있는 예제를 동작시키기까지 하려면 mock server를 만들고 작업을 해야하는데 이 부분까지는 시간이 너무 많이 들것으로 보여 스킵하고 간략한 코드 작성법만 살펴봅니다. 웹플럭스는 웹소켓 API 테스트를 위한 솔루션을 제공하지 않는다. 서버에 연결할 수 있다하더라도 StepVerifier를 사용해 수신 데이터를 확인하는 것이 어려움.\t@Test\t@WithMockUser\tpublic void checkThatUserIsAbleToMakeATrade() {\t\tURI uri = URI.create(\"ws://localhost:8080/stream\");\t\tTestWebSocketClient client = TestWebSocketClient.create(uri);\t\tTestPublisher&lt;String&gt; testPublisher = TestPublisher.create();\t\tFlux&lt;String&gt; inbound = testPublisher.flux()\t\t\t.subscribeWith(ReplayProcessor.create(1))\t\t\t.transform(client::sendAndReceive)\t\t\t.map(WebSocketMessage::getPayloadAsText);\t\tStepVerifier\t\t\t.create(inbound)\t\t\t.expectSubscription()\t\t\t.then(() -&gt; testPublisher.next(\"TRADES|BTC\"))\t\t\t.expectNext(\"PRICE|AMOUNG|CURRENCY\")\t\t\t.then(() -&gt; testPublisher.next(\"TRADE: 10123|1.54|BTC\"))\t\t\t.expectNext(\"10123|1.54|BTC\")\t\t\t.then(() -&gt; testPublisher.next(\"TRADE: 10090|-0.01|BTC\"))\t\t\t.expectNext(\"10090|-0.01|BTC\")\t\t\t.thenCancel()\t\t\t.verify();\t} 웹소켓 API 검증과 함께 WebSocketClient와 외부 서비스 간의 상호 작용을 모킹해야 할 수도 있습니다. 이런 상호 작용을 모킹하는것은 어려움(일반적인 WebSocketClient.build 가 없기 때문) 결과적으로 이런 케이스를 현재 하려고한다면 목 서버를 만들어서 통합테스트 코드에서 목서버를 접속하는 방법뿐입니다.Reference Test Pyramid Reactor Test Source Code Reactor Test Example Blog Reactor Test Example Source Code" }, { "title": "[Hands-On Reactive Programming in Spring 5] 2. 스프링을 이용한 리액티브 프로그래밍 기본 개념", "url": "/posts/spring5-reactive-2/", "categories": "DevLog, Spring", "tags": "Java, Spring, Hands-On Reactive Programming in Spring 5, Reactive", "date": "2022-06-18 13:28:00 +0900", "snippet": "2. 스프링을 이용한 리액티브 프로그래밍 기본 개념 관찰자 패턴스프링 서버에서 보낸 이벤트를 구현한 발행-구독(Publish-Subscribe) 구현RxJava 역사 및 기본 개념마블(Marble) 다이어그램리액티브 프로그래밍을 적용한 비즈니스 사례리액티브 라이브러리의 현재 상황2.1 리액티브를 위한 스프링 프레임워크의 초기 해법 콜백 및 Comp...", "content": "2. 스프링을 이용한 리액티브 프로그래밍 기본 개념 관찰자 패턴스프링 서버에서 보낸 이벤트를 구현한 발행-구독(Publish-Subscribe) 구현RxJava 역사 및 기본 개념마블(Marble) 다이어그램리액티브 프로그래밍을 적용한 비즈니스 사례리액티브 라이브러리의 현재 상황2.1 리액티브를 위한 스프링 프레임워크의 초기 해법 콜백 및 CompletableFuture 는 메시지 기반 아키텍처를 구현하는데 널리 사용된다. 이러한 역할을 수행하는 주요 후보로 리액티브 프로그래밍을 이야이기할수 있다. 스프링 프레잌워크는 리액티브 애플리케이션을 구축하는데 유용한 기능들을 제공하고 이를 일부 살펴보도록 하자2.1.1 관찰자(Observer) 패턴 관찰자 패턴이 리액티브 프로그래밍과 관련이 없는것처럼 보일수 있으나 약간만 수정하면 그것이 리액티브 프로그래밍 기초가 된다. 관찰자 패턴은 관찰자라고 불리는 자손의 리스트를 가지고 있는 주체(subject)를 필요로 한다. subject는 자신의 메소드 중 하나를 호출해 관찰자에게 상태변경을 알린다. 관찰자 패턴을 사요용하면 런타임에 객체 사이에 일대다 의존성을 등록할 수 있다. 보통 이런 유형의 통신은 단방향으로 이루어지고, 각 부분이 활발히 상호작용하게 하면서 각각의 결합도를 낮출수 있다. 이 시스템을 통해 효율적으로 이벤트를 배포할수 있다. SubJect와 Observer 2개의 인터페이스로 구성되고, Observer는 usbject에 등록되고 Subject로부터 알림을 수신한다.public interface Observer&lt;T&gt; { void observe(T event);}public interface Subject&lt;T&gt; { void registerObserver(Observer&lt;T&gt; observer); void unregisterObserver(Observer&lt;T&gt; observer); void notifyObservers(T event);} Observer와 Subject 모두 인터페이스에 기술된 것 이상은 서로에 대해 알지 못한다. Observer 인스턴스는 Subject의 존재를 전혀 인식하지 못할수도 있음. 이 경우 Observers을 등록해주는 역할을 담당할 세번쨰 컴포넌트가 필요할 수도 있다.(Srping DI와 같은 방식이 예시가 될수 있음.)public class ConcreteObserverA implements Observer&lt;String&gt; { @Override public void observe(String event) { System.out.println(\"Observer A: \" + event); }}public class ConcreteObserverB implements Observer&lt;String&gt; { @Override public void observe(String event) { System.out.println(\"Observer B: \" + event); }}public class ConcreteSubject implements Subject&lt;String&gt; { private final Set&lt;Observer&lt;String&gt;&gt; observers = new CopyOnWriteArraySet&lt;&gt;(); public void registerObserver(Observer&lt;String&gt; observer) { observers.add(observer); } public void unregisterObserver(Observer&lt;String&gt; observer) { observers.remove(observer); } public void notifyObservers(String event) { observers.forEach(observer -&gt; observer.observe(event)); }} SUbject의 구현체 안에는 notify를 받는 데 관심이 있는 Observer Set이 있다. registerObserver 및 unregisterObserver 메서드를 이용해 수정(구독 or 구독 취소)가 가능하다.2.1.2 관찰자 패턴 사용 예시@Test public void observersHandleEventsFromSubjectWithAssertions() { // given Subject&lt;String&gt; subject = new ConcreteSubject(); Observer&lt;String&gt; observerA = Mockito.spy(new ConcreteObserverA()); Observer&lt;String&gt; observerB = Mockito.spy(new ConcreteObserverB()); // when subject.notifyObservers(\"No listeners\"); subject.registerObserver(observerA); subject.notifyObservers(\"Message for A\"); subject.registerObserver(observerB); subject.notifyObservers(\"Message for A &amp; B\"); subject.unregisterObserver(observerA); subject.notifyObservers(\"Message for B\"); subject.unregisterObserver(observerB); subject.notifyObservers(\"No listeners\"); // then Mockito.verify(observerA, times(1)) .observe(\"Message for A\"); Mockito.verify(observerA, times(1)) .observe(\"Message for A &amp; B\"); Mockito.verifyNoMoreInteractions(observerA); Mockito.verify(observerB, times(1)) .observe(\"Message for A &amp; B\"); Mockito.verify(observerB, times(1)) .observe(\"Message for B\"); Mockito.verifyNoMoreInteractions(observerB); }구독을 취소할 필요가 없는 경우 Java 8 람다을 사용해서 구현할수 있다.@Test public void subjectLeveragesLambdas() { Subject&lt;String&gt; subject = new ConcreteSubject(); subject.registerObserver(e -&gt; System.out.println(\"A: \" + e)); subject.registerObserver(e -&gt; System.out.println(\"B: \" + e)); subject.notifyObservers(\"This message will receive A &amp; B\"); }CopyOnWriteArraySet CopyOnWriteArraySet은 크기가 일반적으로 작고 읽기 전용 작업이 변경 작업보다 훨씬 많을 때 사용하면 좋은 자료구조인데, thread safety 가 보장되기 떄문에 멀티 스레드 환경에서 사용할 수 있습니다. 하지만 변경 작업 같은 경우(add, set, remove) snapshot(복제본)을 이용하여 변경작업을 하기 때문에 비용이 비싸다. 내부적으로 object lock, synchronized 등이 사용되기 때문에 읽기 작업이 많고 변경작업이 적은 경우에 사용하는 것이 좋다.public boolean add(E e) { final ReentrantLock lock = this.lock; lock.lock(); try { Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; } finally { lock.unlock(); } }thread pool을 활룡한 메시지 병렬 처리 대기 시간이 긴 이벤트를 처리하는 관찰자가 많은 경우 다음과 같이 병렬 처리할 수 있다. private final ExecutorService executorService = Executors.newCachedThreadPool(); public void notifyObservers(String event) { observers.forEach(observer -&gt; executorService.submit( () -&gt; observer.observe(event) ) ); } 주의할 점은 자바에서 스레드당 약 1MB를 소비하므로 일반 JVM 응용 프로그램은 몇천개의 스레드만으로도 사용 가능한 메모리를 모두 소모할 수 있다. 추가적으로 thread 1개가 생성될떄 1MB 메모리를 바로 소비한다는 의미는 아니고, 64bit OS 기준으로 stack size가 1MB까지만 할당된다는 의미 그리고 실제로 malloc은 단순히 가상 페이지 를 지정 하기만 할뿐 실제로 사용될 때까지 물리적 페이지 가 할당되지는 않는다. 아마도 추측하기에는 JVM애플리케이션의 스레드 개수 * 1MB &gt; Max Heap Size 더라도 애플리케이션 구동 자체에 문제가 없는 이뉴는 이런 이유가 아닐까 싶음. JVM Thread stack size 확인 : java -XX : + PrintFlagFinal -version grep ThreadStackSize java.util 패키지 Observer 및 Observable JDK 1.0에서 릴리즈되어 제네릭 적용이 안되어 있음. 컴파일 타임에 타입 안정성을 보장하지 않기 떄문에 사용을 지양 해야 함. 직접 구현해서 사용할수 도있지만 오류 처리, 비동기 실행, 스레드 안정성, 성능 등 직접적인 관리 비용이 크므로 성숙한 구현체 라이브러리를 사용하도록 하자!2.1.3 @EventListener을 사용한 발행-구독 패턴 Spring에서는 @EventListener 애토네이션과 이벤트 발행을 위한 ApplicationEventPublisher 클래스를 제공한다. 이는 관찰자 패턴과 달리 게시자와 구독자는 다음 그림과 같이 서로를 알 필요가 없다. 발행-구독 패턴은 게시자와 구독자 간에 간접적인 계층을 제공한다. 이벤트 채널(메시지 브로커 또는 이벤트 버스라고도 함)은 수신 메시지를 구독자에게 배포하기 전에 필터링 작업을 할수도 있음. 토픽 기반 시스템(topic-based-system)의 구독자는 관심 토픽에 게시된 모든 메시지를 수신하게 된다. @EventListener 애노테이션은 토픽 기반 라우팅과 내용 기반 라우팅 모두에 사용할 수 있다. 조건 속성(condition attribute)은 스프링 표현 언어(SPring Expression Languages, SpEL)을 사용하는 내용 기반 라우팅 이벤트 처리를 가능하게 한다.2.1.4 @EventListener을 활용한 애플리케이션 개발 서버에서 클라이언트로의 비동기 메시지를 전달할 수 있는 웹소켓(WebSocket) 및 SSE(Server-Sent Events) 프로토콜이 있다. 일반적으로 SSE는 브라우저에 메시지를 업데이트하거나 연속적인 데이터 스트림을 보내는 데 사용 SSE기 리액티브 시스템의 구성 요소간에 통신 요구사항을 충족시키는 최고의 후보이다.WebSocket vs SSE 간단한 차이점 비교스프링 웹 MVC를 이용한 비동기 HTTP 통신 서비릇 3.0에서 추가된 비동기 지원 기능은 HTTP 요청을 처리하는 기능을 확장했고, 컨테이너 스레드를 사용하는 방식으로 구현됨. Callable&lt;T&gt;는 컨테이너 스레드 외부에서 실행될수도 있지만 블로킹 호출 DeferredResult&lt;T&gt;는 setResult(T result) 메서드를 호출해 컨테이너 스레드 외부에서도 비동기 응답을 생성하므로 이벤트 루프 안에서 사용할 수 있음. Spring 4.2부터 지원하는 ResponseBodyEmitter가 DeferredResult와 비슷하게 동작한다. SseEmitter는 ResponseBodyEmitter을 상속하는 구조로 되어있고, 이걸 사용하면 데이터(payload)를 비동기적으로 보낼 수 있다. 이때 서블릿 스레드를 차단하지 않기 때문에 큰 파일을 스트리밍해야 하는 경우 매우 유용하다. SSE 엔드포인트 노출 온도 센서로부터 사용자에게 랜덤한 온도를 전달하는 SSE 엔드포인트@RestControllerpublic class TemperatureController { static final long SSE_SESSION_TIMEOUT = 30 * 60 * 1000L; private static final Logger log = LoggerFactory.getLogger(TemperatureController.class); private final Set&lt;SseEmitter&gt; clients = new CopyOnWriteArraySet&lt;&gt;(); @RequestMapping(value = \"/temperature-stream\", method = RequestMethod.GET) public SseEmitter events(HttpServletRequest request) { log.info(\"SSE stream opened for client: \" + request.getRemoteAddr()); SseEmitter emitter = new SseEmitter(SSE_SESSION_TIMEOUT); clients.add(emitter); // Remove SseEmitter from active clients on error or client disconnect emitter.onTimeout(() -&gt; clients.remove(emitter)); emitter.onCompletion(() -&gt; clients.remove(emitter)); return emitter; } @Async @EventListener public void handleMessage(Temperature temperature) { log.info(format(\"Temperature: %4.2f C, active subscribers: %d\", temperature.getValue(), clients.size())); List&lt;SseEmitter&gt; deadEmitters = new ArrayList&lt;&gt;(); clients.forEach(emitter -&gt; { try { Instant start = Instant.now(); emitter.send(temperature, MediaType.APPLICATION_JSON); log.info(\"Sent to client, took: {}\", Duration.between(start, Instant.now())); } catch (Exception ignore) { deadEmitters.add(emitter); } }); clients.removeAll(deadEmitters); }} SseEmitter 클래스는 SSE 이벤트를 보내는 목적으로만 이 클래스를 사용할 수 있다. Controller에서 SseEmitter을 반환하지만 실제로는 SseEmitter.complete() 메소드가 호출되거나 오류 발생 또는 시간 초과 발생할 때까지 실제 요청 처리는 계속 된다.비동기 지원 설정@EnableAsync@SpringBootApplicationpublic class Application implements AsyncConfigurer { public static void main(String[] args) { SpringApplication.run(Application.class, args); } @Override public Executor getAsyncExecutor() { ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setThreadNamePrefix(\"sse-\"); executor.setCorePoolSize(2); executor.setMaxPoolSize(100); executor.setQueueCapacity(5); executor.initialize(); return executor; } @Override public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() { return new SimpleAsyncUncaughtExceptionHandler(); }} @EnableAsync을 통해 Async Auto Configration을 수행 Executor setQueueCapacity 설정을 통해 스레드풀 사이즈를 적절히 조절 이걸 지정하지 않으면 내부적으로는 Integer.MAX_VALUE 사이즈의 LinkedBlockingQueue를 생성해서 core 사이즈만큼의 스레드에서 task를 처리할 수 없을 경우 queue에서 대기하게 됩니다. queue가 꽉 차게 되면 그때 max 사이즈만큼 스레드를 생성해서 처리하게 됩니다. 이런 옵션 조절을 통해 동시성 성능을 제어한다고 이해하면 됨.SSE를 지원하는 UI 작성&lt;body&gt;&lt;ul id=\"events\"&gt;&lt;/ul&gt;&lt;script type=\"application/javascript\"&gt;function add(message) { const el = document.createElement(\"li\"); el.innerHTML = message; document.getElementById(\"events\").appendChild(el);}var eventSource = new EventSource(\"/temperature-stream\");eventSource.onmessage = e =&gt; { const t = JSON.parse(e.data); const fixed = Number(t.value).toFixed(2); add('Temperature: ' + fixed + ' C');}eventSource.onopen = e =&gt; add('Connection opened');eventSource.onerror = e =&gt; add('Connection closed');&lt;/script&gt;&lt;/body&gt; 웹페이지는 EventSource를 통해 클라이언트 및 서버 접속을 유지하고 이벤트를 수신한다. 또 네트워크 문제 발생이나 접속 시간이 초과하는 경우 자동으로 재접속한다.솔루션에 대한 평가 지금까지 설명한 솔루션에는 몇가지 문제가 있다. Spring에서 제공하는 발행-구독 매커니즘은 애플리케이션 생명주기 이벤트를 처리하기 위해 도입되었고, 고부하 및 고성능 시나리오를 위한 것이 아니다. 또한 비즈니스 로직을 정의하고 구현하기 위해 스프링 내부 매커니즘을 사용해야 하므로 프레임워크의 사소한 변경으로 인해 전체 애플리케이션의 안정성을 위협할 수 있다. 또 많은 메서드에 @EventListener 애노테이션이 붙어 있고, 전체 워크플로를 설명하는 한줄의 명시적 스크립트도 없는 애플리케이션이라는 것도 단점일수 있다. SseEmitter을 사용하면 스트림의 종료와 오류 처리에 대한 구현을 추가할 수 있지만 @EventListener는 그렇지 않다. 이벤트를 비동기적으로 브로드캐스팅하기 위해 스레드풀을 사용하는데 이는 진정한 비동기적 리액티브 접근에서는 필요 없는 일. 이러한 문제점 해결을 위해 널리 채택된 최초의 리액티브 라이브러리인 RxJava를 알아보도록 하자.2.2 리액티브 프레임워크 RxJava 현재는 RxJava말고도 Akka Streams와 Reactor 프로젝트도 있으나 시작은 RxJava부터 시작됨. Rxjava 라이브러니는 Reacative Extensions(ReactiveX라고도 함)의 자바 구현체이다. 일반적으로 ReactiveX는 관찰자 패턴, 반복자 패턴 및 함수형 프로그래밍의 조합으로 정의된다.2.2.1 관찰자 + 반복자 = 리액티브 스트림public interface RxObserver&lt;T&gt; { void onNext(T next); void onComplete(); void onError(Exception e);} Iterator와 매우 비슷하지만 next() 대신 onNext() 콜백에 의해 새로운 값이 통지된다. 그리고 hasNext() 대신 onComplete() 메소드를 통해 스트림의 끝을 알린다. 오류 전파를 위해 onError(Exception e) 콜백이 제공됨.리액티브 Observable 클래스 리액티브 Observable 클래스는 관찰자 패턴의 주체(Subject)와 일치 이벤트를 발생시킬 때 이벤트 소스 역할을 수행Subscriber 추상 클래스 Observer 인터페이스를 구현하고 이벤트를 소비한다.(실제로 기본 구현체) 런타임에 Observable과 Subscriber간의 관계는 메시지 구독 상태를 확인하고 필요한 경우 이를 취소할 수도 있는 구독의 의해 제어됨.2.2.2 스트림의 생산과 소비 Observable은 구독자가 구독하는 즉시 구독자에게 이벤트를 전파하는 이벤트 생성기public void simpleRxJavaWorkflow() { Observable&lt;String&gt; observable = Observable.create( new Observable.OnSubscribe&lt;String&gt;() { @Override public void call(Subscriber&lt;? super String&gt; sub) { sub.onNext(\"Hello, reactive world!\"); sub.onCompleted(); } } ); } 자바 스트림 API와 달리 Observable은 재사용이 가능하며 모든 구독자는 구독하자마자 “Hello, reactive workld!” 이벤트를 받게 된다. 위 방식은 backpressure)을 지원하지 않아 현재는 사용되지 않는 방식Subscriber&lt;String&gt; subscriber = new Subscriber&lt;String&gt;() { public void onNext(String s) { System.out.println(s); } public void onCompleted() { System.out.println(s\"Done!; } public void onError(Throwable e) { System.out.println(e); } };람다 표현식 사용방식public void simpleRxJavaWorkflowWithLambdas() { Observable.create( sub -&gt; { sub.onNext(\"Hello, reactive world!\"); sub.onCompleted(); } ).subscribe( System.out::println, System.err::println, () -&gt; System.out.println(\"Done!\") ); } 위와 같이 람다를 사용해서 간략하게 작성할 수도 있으며, 이 외에도 배열, Iterable 컬렉션, Callable, Future 등을 이용해 인스턴스를 만드는 방법도 제공한다.Observable.concat 생성과 함께 Observable 스트림을 다른 Observable 인스턴스와 결합해 생성하여 복잡한 워크플로를 쉽게 구현할 수 있다.Observable.concat(hello, world, Observable.just(\"!\")) .forEach(System.out::print);2.2.3 비동기 시퀀스 생성 주기적으로 비동기 이벤트 시퀀스를 생성할 수 있다.public void timeBasedSequenceExample() throws InterruptedException { Observable.interval(1, TimeUnit.SECONDS) .subscribe(e -&gt; System.out.println(\"Received: \" + e)); Thread.sleep(5000); } 이벤트가 생성되는 것과는 별개의 스레드를 통해 처리되기 때문에 메인스레드 실행을 지연시켜야만 동작한다.다른 방법public void managingSubscription2() throws InterruptedException { CountDownLatch externalSignal = new CountDownLatch(3); Subscription subscription = Observable .interval(100, MILLISECONDS) .subscribe(System.out::println); externalSignal.await(450, MILLISECONDS); subscription.unsubscribe(); } 위와 같이 CountDownLatch가 전파될때까지 이벤트를 계속 소비하는 방식으로도 처리가 가능.2.2.4 스트림 변환과 마블 다이어그램마블 다이어그램(marble diagram) 메소드 시그니쳐만으로 이해가 어려울수 있기떄문에 발명된 스트림 변환 시각화 다이어그램Map 연산자&lt;R&gt; Observable&lt;R&gt; map(Func1&lt;T, R&gt; func) func 함수가 타입 &lt;T&gt;를 타입 &lt;R&gt;로 변환하고, map을 통해 Observable&lt;T&gt;를 Observable&lt;R&gt;로 변환할 수 있음을 의미Filter 연산자Count 연산자 스트림이 무한대일 때는 완료되지 않거나 아무것도 반환할 수 없음.Zip 연산자 두 개의 병렬 스트림 값을 결합하는 연산자public void zipOperatorExample() { Observable.zip( Observable.just(\"A\", \"B\", \"C\"), Observable.just(\"1\", \"2\", \"3\"), (x, y) -&gt; x + y ).forEach(System.out::println); }/**** A1* B2* C3***/사용자 지정 연산자 작성 Observable.Transformer&lt;T, R&gt;에서 파생된 클래스를 구현해 사용자 지정 연산자를 작성할수도 있음. 이는 Observable.compose(transformer) 연산자를 적용해 워크플로에 포함될 수 있다.2.2.5 RxJava 사용의 전제 조건 및 이점 서로 다른 리액티브 라이브러리는 API도 조금씩 다르고 구현 방식도 다양하다.(구독자가 관찰 가능한 스트팀에 가입한 후, 비동기적으로 이벤트를 생성해 프로세스를 시작한다는 핵심 개념은 모두 동일) 이런 접근방식은 매우 융통성이 있는 구조이고, 생성 및 소비되는 이벤트의 양을 제어할 수 있다. 그로 인해 데아터 작성시에만 필요하고 그 이후에는 CPU 리소스 사용량을 줄일 수 있음.// 페이지와 관계 없이 전체 결과를 반환 받는 구조라서 문제public interface SearchEngine { List&lt;URL&gt; search(String query, int limit);}// 다음 데이터 반환을 기라딜때 스레드가 블로킹되는 문제가 있음.public interface IterableSearchEngine { Iterable&lt;URL&gt; search(String query, int limit);}// CompletableFuture 결과가 List라 한번에 전체를 반환하거나 아무것도 반환하지 않는 방식으로만 동작하는 문제public interface FutureSearchEngine { CompletableFuture&lt;List&lt;URL&gt;&gt; search(String query, int limit);}public interface RxSearchEngine { Observable&lt;URL&gt; search(String query);} Rx의 접근방식을 사용하면 응답성을 크게 높힐 수 있음. 최초 데이터 수신 시간(Time To First Byte) 또는 주요 랜더링 경로(Critical Rendering Path) 메트릭으로 성능을 평가하는데 여기서 기존 방식보다 훨씬 나은 결과를 보여준다.public void deferSynchronousRequest() throws Exception { String query = \"query\"; Observable.fromCallable(() -&gt; doSlowSyncRequest(query)) .subscribeOn(Schedulers.io()) .subscribe(this::processResult); Thread.sleep(1000); } 위 워크플로우는 한 스레드에서 시작해 소수의 다른 스레드로 이동하고, 완전히 다른 새 스레드에서 처리가 완료될 수 있음. 이 과정에서 객체 변형은 리스크가 있을수 있으며, 일반적으로 불변 객체를 사용한다. 불변 객체는 함수형 프로그래밍 핵심 원리중 하나 Java8 Stream에서도 final Variable만 참조할수 있는 이유가 이러한 이유라고 보면 된다. 이 간단한 규칙으로 병렬 처리에서 발생할 수 있는 대부분의 문제를 예방할수 있다. 2.2.6 RxJava를 이용해 애플리케이션 다시 만들기@Componentpublic class TemperatureSensor { private static final Logger log = LoggerFactory.getLogger(TemperatureSensor.class); private final Random rnd = new Random(); // private filed로 하나만 정의해서 재사용할수 있음 private final Observable&lt;Temperature&gt; dataStream = Observable .range(0, Integer.MAX_VALUE) // 사실상 무한 스트림 .concatMap(ignore -&gt; Observable .just(1) .delay(rnd.nextInt(5000), MILLISECONDS) .map(ignore2 -&gt; this.probe())) // ignore2는 단일 원소 스트림을 생성하는데 필요해서 정의된 것이기에 무시해도 괜찮. .publish() // 각 구독자(SSE 클라이언트)는 각 센서 판독 결과를 공유하지 않는다. .refCount(); // 하나 이상의 구독자가 있을때만 입력 공유 스트림에 대한 구독을 생성 public Observable&lt;Temperature&gt; temperatureStream() { return dataStream; } private Temperature probe() { double actualTemp = 16 + rnd.nextGaussian() * 10; log.info(\"Asking sensor, sensor value: {}\", actualTemp); return new Temperature(actualTemp); }}Custom Sse Emitterstatic class RxSseEmitter extends SseEmitter { static final long SSE_SESSION_TIMEOUT = 30 * 60 * 1000L; private final static AtomicInteger sessionIdSequence = new AtomicInteger(0); private final int sessionId = sessionIdSequence.incrementAndGet(); private final Subscriber&lt;Temperature&gt; subscriber; RxSseEmitter() { super(SSE_SESSION_TIMEOUT); this.subscriber = new Subscriber&lt;Temperature&gt;() { @Override public void onNext(Temperature temperature) { try { RxSseEmitter.this.send(temperature); // onNext() 신호를 수신하면 응답으로부터 SSE 클라이언트에게 신호를 보낸다. log.info(\"[{}] &lt;&lt; {} \", sessionId, temperature.getValue()); } catch (IOException e) { log.warn(\"[{}] Can not send event to SSE, closing subscription, message: {}\", sessionId, e.getMessage()); unsubscribe(); } } @Override public void onError(Throwable e) { log.warn(\"[{}] Received sensor error: {}\", sessionId, e.getMessage()); } @Override public void onCompleted() { log.warn(\"[{}] Stream completed\", sessionId); } }; onCompletion(() -&gt; { log.info(\"[{}] SSE completed\", sessionId); subscriber.unsubscribe(); }); onTimeout(() -&gt; { log.info(\"[{}] SSE timeout\", sessionId); subscriber.unsubscribe(); }); } Subscriber&lt;Temperature&gt; getSubscriber() { return subscriber; } int getSessionId() { return sessionId; } }SSE 엔드포인트 노출@RequestMapping(value = \"/temperature-stream\", method = RequestMethod.GET) public SseEmitter events(HttpServletRequest request) { RxSeeEmitter emitter = new RxSeeEmitter(); log.info(\"[{}] Rx SSE stream opened for client: {}\", emitter.getSessionId(), request.getRemoteAddr()); temperatureSensor.temperatureStream() .subscribe(emitter.getSubscriber()); return emitter;} SSE 세션을 온도 측정 스트림을 구독한 새로운 RxSseEmitter에만 바인딩한다. 이 구현은 스프링의 EventBus를 사용하지 않으므로 이식성이 더 높고 스프링 컨텍스트가 없이도 테스트가 가능하다. 또한 @EventListener, @EnableAsync 같은 의존성이 없어서 애플리케이션 구성도 더 간단하다. RxJava Schedular를 구성한 세밀한 스레드 관리를 할수도 있지만, 이러한 구성이 스프링 프레임워크에 의존하지 않는다. 이는 리액티브 프로그래밍이 가지는 능동적 구독이라는 개념의 자연스러운 결과이다.2.3 리액티브 라이브러리의 간략한 역사 MS에서 Rx의 개념이 시작되었고 Rx.NET을 시작으로, 언제부턴가 외부로 퍼져나감. 깃헙의 저스틴 스파서머스와 폴 베츠는 2012년 objectiveC용 Rx를 구현했고, 넷플릭스에서 Rx Java로 이식하여 오픈소스로 공개했다.2.4 리액티브의 전망 Ratpack에서 RxJava 채택 Android 유명한 http client인 Retrofit에서도 RxJava 지원 Vert.x에서도 정식 리액티브 스트림 제공(ReadStream, WriteStream)주의사항 하나의 자바 응용 프로그램에 다른 종류의 리액티브 라이브러리 또는 프레임워크를 동시에 사용하면 문제가 발생할 수 있다. 리액티브 라이브러리들의 동작은 일반적으로 비슷하지만 세부 구현은 조금씩 다를수 있어서 동시 사용시 발견 및 수정하기 어려운 숨겨진 에러를 유발할 수 있다. 이런 전체 리액티브 환경을 아우르며 호환성을 보장하기 위한 표준 API가 바로 리액티브 스트림이다. 다음에 이를 자세히 알아보도록 하자!Reference CopyOnWriteArraySet JVM Thread Memory usage 관련 웹소켓과 SSE 비교" }, { "title": "[Hands-On Reactive Programming in Spring 5] 1. 왜 리액티브 스프링인가?", "url": "/posts/spring5-reactive-programing-1/", "categories": "DevLog, Spring", "tags": "Java, Spring, Hands-On Reactive Programming in Spring 5, Reactive", "date": "2022-06-11 11:35:00 +0900", "snippet": "1. 왜 리액티브 스프링인가?1.1 왜 리액티브인가? 서버진영에서 reactive(반응형)이라는 말이 2019년쯤부터 굉장히 빈번하게 들려왔다.시스템의 가용량 예시 tomcat worker thread 500으로 설정API 응답시간이 250ms 이 기준으로 계산해보면 시스템의 최대 TPS 2,000임을 알수 있다.특수한 상황이 생기면? 시스템의...", "content": "1. 왜 리액티브 스프링인가?1.1 왜 리액티브인가? 서버진영에서 reactive(반응형)이라는 말이 2019년쯤부터 굉장히 빈번하게 들려왔다.시스템의 가용량 예시 tomcat worker thread 500으로 설정API 응답시간이 250ms 이 기준으로 계산해보면 시스템의 최대 TPS 2,000임을 알수 있다.특수한 상황이 생기면? 시스템의 thread pool 정보와 api 응답시간 등을 기준으로 서버에서 가용량을 계산해서 HA 구성을 해놓았으면 평상시에는 예상되는 트래픽을 잘 커버할수 있을 것이다. 다만 블랙 프라이데이와 같이 특수한 경우에는 이런 예상치를 훨씬 상회하는 트래픽이 유입될수 있음. 이런 케이스에서는 가용한 모든 worker thread들이 사용될것이고, 그 이후에 들어오는 request들을 처리하기 위해 worker thead pool에 thread 할당을 요청하지만 가용하지 않아 대기하게 되고 응답시간이 증가하다가 결국 서비스가 불가능한 상태에 이를수 있다. tomcat worker thread 외에도 db connection pool, API 서비스 로직에서 호출하는 api client의 thread pool 가용량도 초과하는 경우 동일한 문제가 발생할 수 있음. 어떻게 대응할것인가? 탄력성(elasticity)을 통한 것이다. 트래픽에 따라서 또는 server metric에 따라 탄력적으로 시스템 처리량이 자동 증가/감소할 수 있다면 위와 같은 문제를 해결할 수 있을 것이다. thread 부족으로 인한 지연시간이 발생하지 않도록 시스템 확장 암달의 법칙(Amdahl’s Law)과 건터(Gunther)의 보편적 확장성 모델(Universal Scalability Model)로 설명응답성의 나빠지는것이란? 쇼핑 시스템에서 외부 결제 서비스가 중단되고 모든 사용자의 상품 구매 결제가 실패하는상황, 일반적으로 발생해서는 안되는 상황이다. 고품질 사용자 경험을 제공하기 위해서는 응답성에 관심을 기울여야 한다. 응답시간이 오래걸리는 API가 있을떄 트래픽이 몰리면 위의 문제처럼 가용한 thread들이 모두 블로킹된 상태에서 계속 요청이 들어오게 되어 결국 서버 전체적으로 문제가 생김. 응답성이 나빠지는것을 방지하려면? 시스템 실패에도 반응성을 유지할 수 있는 능력, 즉 시스템 복원력을 갖추어야 한다. 시스템의 기능 요소를 격리해 모든 내부 장애를 격리하고 서비스 독립성을 확보하여 달성할 수 있다. MSA에서 Circuit Breakers 도입하여 장애 격리 1.2 메시지 기반 통신@RequestMapping(\"/resource\")public Object processRequest() { RestTemplate template = new RestTemplate(); ExamplesCollection result = template.getForObject( \"http://example.com/api/resource2\", ExamplesCollection.class ); ... processResultFurther(result);} 일반적으로 spring mvc 기반으로 requestMapping을 작성하는 경우 위와 같이 개발을 할것이다. request per thread model을 사용하는것으로 서비스 기술스택/아키텍쳐로 정했다면 사실 문제가 있는것은 아니다. 여기서 말하고 싶은 내용은 i/o 효율 관점에서의 이야기라고 보면 좋을것 같다. 위와 같은 request per thread model에서는 i/o를 처리하는 동안 thread가 blocking되어 대기하게 된다. thread A는 blocking된 동안 다른 요청을 처리할 수 없다. Java에서는 병렬 처리를 위해 thread pool을 이용해 추가 스레드를 할당하는 방법이 있지만, 부하가 높은 상태에서는 이러한 기법이 새로운 I/O 작업을 동시에 처리하는데에는 매우 비효율적일 수 있다.비동기 논블로킹 모델(asynchronous and non-blocking model) 위 그림은 사람이 SMS을 처리하는 방식에 대한 내용이다. 이 방식이 바로 대표적인 non-blocking 통신 방식이라고 이해할 수 있다.message-driven 통신 자원을 효율적으로 사용하기 위해서는 message-driven 통신원칙을 따라야 한다. 이를 수행하는 방법의 하나는 메시지 브로커(message broker)를 사용하는 것이다. 메시지 대기열을 관리하여 시스템 부하 관리, 탄력성을 제어할 수 있다. 메시지 브로커로 kafka를 사용한다고 했을떄 consumer와 partition수를 니즈에 따라 적절히 셋팅하여 처리량을 조절할 수 있다. 리액티브 선언문 모든 비즈니스의 핵심 가치는 응답성이다. 응답성을 확보한다는 것은 탄력성 및 복원력 같은 기법을 따른다는 의미이다. 탄력성 및 복원력을 확보하는 기본적인 방법의 하나는 메시지 기반 통신을 사용하는 것이다.1.3. 반응성에 대한 유스케이스 위 그림은 modern micro service pattern을 적용한 웹 스토어 아키텍쳐이다. 위치 투명성을 달성하기 위해 api gateway pattern을 사용한다. 서비스 요소 일부에 복제본을 구성해 높은 시스템 응답성을 얻을 수 있다. 복제본 하나가 중단된 경우에 복원력을 유지할 수 있다. kafka을 이용해 구성한 메시지 기반 통신과 독립적인 결제 서비스 fail-over 처리도 하고 빠른 응답성을 제공한다. 실제 주문 처리를 하지 않고 바로 응답할 수 있으므로 thread가 blocking 되는 시간이 줄어든다. 다만 transaction이 무조건 보장되어야 하는 케이스라면 위와 같은 비동기 처리방식이 적절하지 않을수 있다. 예를 들면 은행의 계좌이체를 생각했을때 돈이 오고가는데 결과적으로 데이터 일관성이 맞춰지겠으나 시점에 따라 데이터 일관성이 꺠질수 있고, 즉각적으로 fail-over를 할수 없는 상황도 있을수 있다.(상대 은행의 점검시간인 경우 즉시 fail-over하더라도 처리할수 없고 점검이 끝날떄가 되어서야 처리가 이루어질수 있다.) 리액티브가 적절한 시스템 분야 애널리틱스(analytics)분야는 엄청난 양의 데이터를 다루면서 런타임에 처리하고 사용자에게 실시간으로 통계를 제공해야 할수도 있는데 이런 케이스에서 리액티브가 효과적일 수 있다. 스트리밍(streaming)이라는 효율적인 아키텍쳐를 사용할수 있다. 가용성이 높은 시스템을 구축하려면 리액티브 선언문에서 언급한 기본 원칙을 지켜야한다. 복원력 확보를 위해 배압 지원 활성화 등 1.4 서비스 레벨에서의 반응성 큰 시스템은 더 작은 규모의 시스템들로 구성되기 때문에 구성 요소의 리액티브 특성에 의존한다. 즉 리액티브 시스템은 설게 원칙을 저굥ㅇ하고 이 특성을 모든 규모에 저굥ㅇ해 그 구성요소들을 합성할 수 있게 하는것을 의미힌다. (리액티브 선언문 중) 따라서 구성 요소 수준에서도 리액티브 설계 및 구현을 제공하는 것이 중요하다. 설게 원칙이란 컴포넌트 사이의 관계, 예를 들면 각 기본 요소를 조합하는 데 사용되는 프로그래밍 기법일반적인 자바의 코드 작성 : 명령형 프로그래밍(imperative programing)interface ShoppingCardService { Output calculate(Input value);}class OrdersService { private final ShoppingCardService scService; void process() { Input input = ...; Output output = scService.calculate(input); ... }} scService.calculate()에서 I/O 작업을 수행한다고 가정했을떄 해당 메소드를 수행하는동안 스레드는 blocking된다. orderService에서 별도의 독립적인 처리를 실행하려면 추가 스레드 할당이 필요하다.(하지만 이러하 방식은 낭비이고 리액티브 관점에 본다면 그렇게 하지 말아야 한다.Callback 활용 방식interface ShoppingCardService { void calculate(Input value, Consumer&lt;Output&gt; c);}public class OrdersService { private final ShoppingCardService shoppingCardService; void process() { Input input = new Input(); shoppingCardService.calculate(input, output -&gt; { ... }); } 실제 자바 코드 관점에서 thread blocking이 발생하기는 하지만 ShoppingCardService로부터 결과를 반환받지않고, OrderService가 작업을 완료 후에 반응할 콜백 함수를 미리 전달하므로 ShoppingCardService로부터 분리(decoupled)됐다고 볼수 있다.Thread 사용 방식public class AsyncShoppingCardService implements ShoppingCardService { @Override public void calculate(Input value, Consumer&lt;Output&gt; c) { // blocking operation is presented, better to provide answer asynchronously new Thread(() -&gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } c.accept(new Output()); }).start(); }} ShoppingCardService에서 데이터를 처리하고 파라미터로 전달된 Callback function에 그 결과를 전달하여 처리하는 방식 이러하 방식의 장점은 컴포넌트가 콜배 함수에 의해 분리된다는것이다. 다점이라면 공유 데이터 변경 등 롤백 지옥을 피하기 위헤ㅐ 개발자가 멀티 스레딩을 잘 이해해야 한다.Future 사용 방식public interface ShoppingCardService { Future&lt;Output&gt; calculate(Input value);}public class OrdersService { private final ShoppingCardService shoppingCardService; public OrdersService(ShoppingCardService shoppingCardService) { this.shoppingCardService = shoppingCardService; } void process() { Input input = new Input(); Future&lt;Output&gt; result = shoppingCardService.calculate(input); System.out.println(shoppingCardService.getClass().getSimpleName() + \" execution completed\"); try { result.get(); } catch (InterruptedException | ExecutionException e) { e.printStackTrace(); } }} Future을 삳용하여 결과값 반환을 지연시킬 수 있다. Future을 이용하면 멀티 스레드의 복잡성을 숨길 수 있지만 어쩃든 필요한 결과를 얻으려면 현재 스레드를 블로킹시키는 방식으로 처리해야만 한다. 그래서 이는 확장성을 현저히 저하시킬 수 있다.CompletableFuture 사용 방식public interface ShoppingCardService { CompletionStage&lt;Output&gt; calculate(Input value);}public class OrdersService { private final ShoppingCardService shoppingCardService; void process() { Input input = new Input(); shoppingCardService.calculate(input) .thenAccept(v -&gt; System.out.println(shoppingCardService.getClass().getSimpleName() + \" execution completed\")); ... } Future와 비슷하지만 반환된 결과를 기능적 선언 방식으로 처리할 수 있다. 이를 통해함수형 스타일 또는 선언형 스타일로 코드를 작성할수 있고 결과를 비동기적으로 처리할 수 있게 되었다.CompletableFuture 등의 문제점 위와 같은 방식은 비동기 처리를 가능하게 하지만 Spring 자체에서 i/o와 같은 블로킹 네트워크 호출을 모두 별도의 스레드로 래핑해서 처리하기 떄문에 비효율적인 방식이다. Spring5에서 큰변화가 있었던 이유는 기존 Spring에서는 서블릿 3 API에 포함된 대부분의 비동기 논블로킹 기능이 잘 통합되어있었으나 Spring MVC 자체가 비동기 논블로킹 클라이언트를 제공하지 않음으로써 개선된 서블릿 API의 이점을 무효로 만들었기 떄문에 많은 부분에서 변화가 생겼다. 자바의 기본적인 멀티 스레딩 모델은 몇몇 스레드가 그들의 작업을 동시에 실행하기 위해 하나의 CPU를 공유할수도 있다고 가정했다. 이말은 즉 CPU 시간이 여러 스레드간에 공유되는것이고 이러한 처리를 위해 컨텍스트 스위칭(context switching)이 필연적으로 발생한다. 스레드 컨택스트를 변경할때 레지스터, 메모리 맵 및 기타 관련요소를 저장하거나 불러오는 행위가 필요하다. 스레드 개수의 제한적인 요소는 일반적인 스레드 사이즈인 1MB를 기준으로 생각해볼때 request per thread model에서 64,000개의 동시성을 제공하려면 64GB의 메모리가 필요하다는 사실이다.리액티브 파이프라인 비동기 처리는 일반적인 요청-응답 패턴에만 국한되지 않는다. 떄로는 데이터의 연속적인 스트림으로 처리해야 할수도 있고, 정렬된 변환 흐름으로 처리해야 하는 ㄴ경우도 있을수 있다.마치며 책의 내용을 읽고 정리하는 과정에서 느낀 부분은 주로 기존 Spring MVC의 request per thread model의 한계점에 대해 이해하고, webflux 이전 비동기 처리를 어떻게 제공했는지 이 방식에서의 한계점에 대해 간략히 다루는 내용이었습니다. 데이터 소스 레벨까지 리액티브가 가능하다면 이를 사용해서 스레드를 효율적으로 사용할수 있다면 베스트일것이라 생각합니다. 다만 아직 r2dbc와 같은 DB 리애틱브 드라이버가 완벽히 성숙단계에 이른것은 아니므로 RDB로 서비스하는 경우라면 기술 선정에 좀 신중을 가할 필요가 있을것이라 생각됩니다. 반대로 redis, mongodb 등 reactive를 잘 지원할수 있는 스토리지를 사용한다면 reactive를 도입해서 서버의 자원 효율을 증대시키는것도 좋은 방법이라 생각됩니다!Reference Hands-On Reactive Programming in Spring 5 https://github.com/PacktPublishing/Hands-On-Reactive-Programming-in-Spring-5" }, { "title": "[kubernetes-in-action] 11. 쿠버네티스 내부 이해", "url": "/posts/devlog-platform-kubernetes-in-action11/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-09-03 17:34:00 +0900", "snippet": "11. 쿠버네티스 내부 이해11.1 쿠버네티스 아키텍처 이해쿠버네티스 클러스터 구성요소 컨트롤 플레인 (워커)노드컨트롤 플레인 구성요소 클러스터 기능을 제어하고 전체 클러스터가 동작하게 만드는 역할을 한다. etcd 분산 저장 스토리지 API 서버 스케줄러 컨트롤러 매니저워커 노드에서 실행하는 구성 요소 kubelet 쿠버네티스 서비스...", "content": "11. 쿠버네티스 내부 이해11.1 쿠버네티스 아키텍처 이해쿠버네티스 클러스터 구성요소 컨트롤 플레인 (워커)노드컨트롤 플레인 구성요소 클러스터 기능을 제어하고 전체 클러스터가 동작하게 만드는 역할을 한다. etcd 분산 저장 스토리지 API 서버 스케줄러 컨트롤러 매니저워커 노드에서 실행하는 구성 요소 kubelet 쿠버네티스 서비스 프록시(kube-proxy) 컨테이너 런타임(Docker, rkt 외 기타)애드온 구성 요소 컨트롤 플레인과 노드에서 실행되는 구성 요소 외에 클러스터에서 추가 기능을 위한 구성 요소 쿠버네티스 DNS 서버 대시보드 인그레스 컨트롤러 힙스터 컨테이너 네트워크 인터페이스(CNI) 플러그인11.1.1 쿠버네티스 구성 요소의 분산 특성 모든 구송요소는 개별 프로세스로 실행된다.컨트롤 플레인 구성 요소의 상태 확인kubectl get componentstatuses구성 요소가 서로 통신하는 방법 쿠버네티스 시스템 구성 요소는 오직 API 서버하고만 통신한다. API서버는 etcd와 통신하는 유일한 구성 요소 클러스터 내에 존재하는 리소스 데이터는 etcd에 저장되고, 이를 처리하는건 API 서버라는 의미 kubectl을 이용해 로그를 가져오거나 attach 명령으로 실행중인 컨테이너에 연결할 때와 kubectl port-forward명령을 실행할때는 API 서버가 kubelet에 접속한다.(이 경우를 제외하고는 모두 API서버로만 요청한다)개별 구성 요소의 여러 인스턴스 실행 컨트롤 플레인의 구성 요소는 여러 서버에 걸쳐 실행될 수 있다. 컨트롤 플레인 구성 요소 중 etcd와 API 서버는 둘 이상 실행해 가용성을 높일수도 있다. 스케줄러와 컨트롤러 매니저는 하나의 인스턴스만 활성화되고 나머지는 대기 상태로 존재한다.구성 요소 실행 방법 kube-proxy와 같은 컨트롤 플레인 구성 요소는 시스템에 직접 배포하거나 파드로 실행할 수 있따. kubelet은 항상 시스템 구성 요소로 실행되는 유일한 구성 요소이며, kubelet이 다른 구성 요소를 파드로 실행한다. etcd, API 서버, 스케줄러, 컨트롤러 매니저, DNS 서버는 마스터에서 실행되고, kube-proxy, flannel 네트워킹 파드는 워커노드에서 실행된다.# 쿠버네티스 구성요소의 pod의 node 확인kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system11.1.2 쿠버네티스 etcd를 사용하는 방법 API 서버가 다시 시작되거나 실패하더라도 유지하기 위해 매니페스트가 영구적으로 저장될 필요가 있다. 그래서 쿠버네티스는 빠르고, 분산해서 저장되며, 일관된 키-값 저장소를 제공하는 etcd를 사용한다. 둘 이상의 etcd 인스턴스를 실행해 고가용성과 우수한 성능을 제공할 수 있다. 쿠버네티스가 클러스터 상태와 메타데이터를 저장하는 유일한 장소는 etcd뿐이다.낙관적 동시성 제어 데이터 조작에 잠금을 설정해 그동안 데이터를 읽거나 업데이트하지 못하도록 하기 위해서 데이터 업데이트시 버전 번호를 포함시키는 방법.(DB에서의 낙관적 락과 동일한 방식)리소스를 etcd 저장하는 방법 etcd v2는 키를 계층적 키 공간에 저장해 파일시스템의 파일과 유사한 키-값 쌍을 만든다.(디렉터리 존재) etcd v3는 디렉터리를 지원하지 않지만 키 형식은 동일하게 유지되고, 키 이름에 ‘/’을 포함시켜서 마치 디렉터리인것처럼 사용 가능하다. etcd v3에서는 ls를 사용할 수 없고 etcdctl get /registry --prefix=true을 이용하면 v2와 동일한 결과를 얻을 수 있다. etcd의 계층적 키 공간때문에, 저장된 모든 리소스를 단순하게 파일시슽메에 있는 JSON 파일로 생각할 수 있다. 쿠버네티스 1.7 이전에 Secret 리소스의 JSON을 암호화없이 etcd에 저장했으나 1.7부터 암호화하여 저장하고 있다.# etcd registry 하위 조회etcdctl ls /registry/registry/configmaps/registry/daemonsets/registry/deployments/registry/events/registry/namespaces/registry/pods# deafult 네임스페이스의 pods 조회etcdctl ls /registry/pods/default/# deafult 네임스페이스의 pods 조회etcdctl ls /registry/pods/default/# deafult 네임스페이스의 pod 중 하나의 상세 정보 조회etcdctl get /registry/pods/default/kubia-159041347-wt6ga...저장된 오브젝트의 일관성과 유효성 보장 쿠버네티스는 다른 모든 구성 요소가 API 서버를 통하도록 함으로써 데이터 불일치를 방지하고 일관성을 유지한다. API 서버 한곳에서 낙관적 락 매너키즘을 구현해서 클러스터의 상태를 업데이트하기 때문에, 오류가 발생할 가능성을 줄이고 항상 일관성을 가질 수 있다.클러스터링된 etcd의 일관성 보장 고가용성을 위해 2개 이상의 etcd 인스턴스를 실행하는것이 일반적이다. 분산 시스템은 실제 상태가 무엇인지 합의(consensus)에 도달해야 한다. etcd는 RAFT 합의 알고리즘을 사용해 어느 순가이든 각 노드의상태가 대다수의 노드가 동의하는 현재 상태이거나 이전에 동의한 상태 중에 하나임을 보장한다. 합의 알고리즘은 클러스터가 다음 상태로 진행하기 위해 과반수(혹은 쿼럼[quorum])가 필요하다. 2개의 노드는 여전히 과반을 가지고 있어 클라이언트에서 상태 변경 요청을 받을수 있고, 과반을 충족하지 못한 노드는 상태 변경 요청을 받을 수 없다. etcd 인스턴스 수를 홀수로 하는 이유 일반적으로 홀수로 배포한다. 2개로 할 경우 둘 중 하나라도 실패하면 과반이 존재하지 않기 때문에 상태를 변경할 수 없음.. 대규모 etcd 클러스터에서는 일반적으로 5대 혹은 7대 노드면 충분하다.11.1.3 API 서버의 기능 쿠버네티스 APi 서버는 다른 모든 구성 요소와 kubectl 같은 클라이언트에서 사용하는 중심 구성 요소이다. 유효성 검사와 함께 낙관적 락도 처리하기 때문에 동시에 업데이트가 발생하더라도 다른 클라이언트에 의해 오브젝트의 변경 사항이 재정의되지 않는다.kubectl을 이용해 처리하는 과정 kubectl은 파일의 내용을 API 서버에 HTTP POST 요청으로 전달하면 내부에서는 다음과 같은 일이 발생한다. Authentication, Authorization, Admission control을 거친후에 resoruce validation을 거치고 이를 통과해야 etcd에 데이터를 처리한다.인증 플러그인으로 클라이언트 인증 인증 작업은 API 서버에 구성된 하나 이상의 플러그인에 의해 수행되고 HTTP 요청을 검사해 수행한다. 클라이언트 인증서 혹은 HTTP Header를 가져와 인증하는 그런 처리들이 해당된다.인가 플러그인을 통한 클라이언트 인가 인증된 사용자가 요청한 작ㅇ버이 요청한 리소스를 대상으로 수행할 수 있는지를 판별한다. 사용자가 요청한 네임스페이스 안에 파드를 생성할 수 있는지를 결정한다.어드미션 컨트롤 플러그인으로 요청된 리소스 확인과 수정 해당 플러그인은 리소스를 여러 가지 이유로 수정할 수 있다. 누락된 필드를 기본값으로 초기화하거나 재정의하기도 하고, 요청에 없는 관계된 리소스를 수정하거나 어떤 이유로든 요청을 거부할 수도 있다. 데이터를 읽는 GET 요청의 경우에는 어드미션 컨트롤러를 거치지 않는다.어드미션 컨트롤러 플러그인 예시 AlwasysPullImages : 이미지를 항상 강제로 가져오도록 재정의 ServiceAccount : 명시적으로 지정하지 않는 경우 default 서비스 어카운트 NamespaceLifecycle : 네임스페이스가 존재하지 않을 경우 파드 생성 방지 ResourceQuota : 특정 네임스페이스 안에 있는 파드가 네임스페이스에 할당된 CPU와 메모리를 사용하도록 강제한다.리소스 유효성 확인 및 영구 저장 요청이 모든 어드미션 컨트롤 플러그인을 통과하면 API 서버는 오브젝트의 유효성을 검증하고 etcd에 저장한다.11.1.4 API 서버가 리소스 변경을 클라이언트에 통보하는 방법 API 서버는 위 과정을 제외한 다른 일은 하지 않는다. 파드를 만든다던지, 서비스의 엔드포인트를 관리한다던지.. 이런 일들 아무것도 하지 않는다.(컨트롤러 매니저의 역할) API가 직접 구성요소에게 명령을 내리는 방식이 아니다. 단지 각각 구성요소에서 관찰한 변경사항을 노티해주기만 한다. 클라이언트는 API 서버에 HTTP 연결을 맺고 변경 사항을 감지한다. 오브젝트가 갱신될 때마다, 서버는 오브젝트를 감시하고 있는 연결된 모든 클라이언트에게 오브젝트의 새로운 버전을 보낸다. 갱신된 오브젝트를 감시하고 있는 모든 관찰자에게 갱신된 오브젝트를 전달한다. kubectl 도구 또한 리소스 변경을 감시할 수 있는 API 서버의 클라이언트 중 하나이고, --watch 옵션을 통해 파드의 생성, 수정, 삭제 통보를 받을 수 있다.# watch 설정kubectl get po --watchkubectl get po -o yaml --watch11.1.5 스케줄러 스케줄러는 API 서버의 감시 매커니즘을 통해 새로 생성될 파드를 기다리고 있다가 할당된 노드가 없는 새로운 파드를 노드에 할당한다. 선택된 노드(해당 노드에서 실행중인 kubelet)에 파드를 실행하도록 지시하지 않는다. 단지 스케줄러는 API 서버로 파드 정의를 갱신한다. 이러면 API 서버는 kubelet에 파드 스케줄링된 것을 통보하고, 대상 노드의 kubelet은 파드의 컨테이너를 생성하고 실행한다. 스케줄러의 중요한 작업 중 하나는 파드에 가장 적합한 노드를 선택하는 것인데 이 작업은 단순하지는 않다. 가장 쉬운 방법 : 이미 실행중인 파드를 신경 쓰지 않고 무작위로 노드를 선택하는 것 머신 러닝 등 고급 기술을 이용해 향후 몇분 혹은 몇 시간 내에 어떤 종류의 파드를 스케줄링할지 예측해 기존 파드를 다시 스케줄링하지 않고도 하드웨어 활용을 극대화 시킬수 있는 노드를 선택하는 것. 쿠버네티스의 기본 스케줄러는 위 2가지의 사이 정도 기본 스케줄링 알고리즘 모든 노드 중에서 파드를 스케줄링할 수 있는 노드 목록을 필터링 수용 가능한 노드의 우선순위를 정하고 점수가 높은 노드를 선택 노드가 같은 차상위 점수를 가지고 있다면, 파드가 모든 노드에 고르게 배포되도록 라운드-로빈을 사용한다.수용 가능한 노드 찾기 미리 설정된 조건 함수 목록에 각 노드를 전달하여 처리된다. 노드가 하드웨어 리소스에 대한 파드 요청을 충족할 수 있는가? 노드에 리소스가 부족한가? 파드를 특정 노드로 스케줄링하도록 요청한 경우에 해당 노드인가? 노드가 파드 정의 안에 있는 노드 셀렉터와 일치하는 레이블을 가지고 있는가(정의한 경우에 한하여)? 파드가 특정 호스트 포트에 할당되도록 요청한 경우 해당 포트가 이 노드에서 이미 사용중인가? 파드 요청이 특정한 유형의 볼륨을 요청하는 경우 이 노드에서 해당 볼륨을 파드에 마운트할 수 있는가? 아니면 이 노드에 있는 다른 파드가 이미같은 볼륨을 사용하고 있는가? 파드가 노드의 테인트를 허용하는가? 파드가 노드의 파드 어피니티, 안티-어피니티 규칙을 지정했는가? 그렇다면 노드에 파드를 스케줄링하면 이 규칙을 어기게 되는가?파드에 가장 적합한 노드 선택 일반적으로는 리소스가 남아 도는 쪽의 노드를 선택하도록 되어야 한다.고급 파드 스케줄링 레플리카가 여러개인 경우, 한 노드에 스케줄링하는 것보다 가능한 많은 노드에 분산되는 것이 이상적일 것이다. 동일한 서비스 또는 레플리카셋에 속한 파드는 기본적으로 여러 노드에 분산되는데, 어피니티와 안티-어피니티 규칙을 정의해서 클러스터 전체에 퍼지거나 가깝게 유지되도록 강제할 수 있다.다중 스케줄러 사용 여러 개의 스케줄러를 실행할 수 있다.(파드 정의 안에 schedulerName 속성에 파드를 스케줄링할 때 사용할 스케줄러를 지정할 수 있음) 사용자가 직접 스케줄러를 구현해 클러스터에 배포하거나 다른 설정 옵션을 가진 쿠버네티스 스케줄러를 배포할 수도 있다.11.1.6 컨트롤러 매니저에서 실행되는 컨트롤러 소개 다양한 조정작업을 수행하는 여러 컨트롤러가 하나의 컨트롤러 매니저 프로세스에서 실행된다. 거의 대부분의 리소스에는 그에 해당하는 컨트롤러가 있고, 컨트롤러는 리소스를 배포함에 따라 실제 작업을 수행하는 활성화된 쿠버네티스 구성요소이다.컨트롤러 종류 레플리케이션 매니저 레플리카셋, 데몬셋, 잡 컨트롤러 디플로이먼트 컨트롤러 스테이트풀셋 컨트롤러 노드 컨트롤러 서비스 컨트롤러 엔드포인트 컨트롤러 네임스페이스 컨트롤러 퍼시스턴트볼륨 컨트롤러 그 밖의 컨트롤러컨트롤러 역할과 동작 방식 이해 모두 API 서버에서 리소스가 변경되는 것을 감시하고 각 변경 작업을 수행한다. 컨트롤러는 조정 루프를 실행해, 실제 상태를 원하는 상태로 조정하고, 새로운 상태의 리소스의 status 섹션에 기록한다. 감시 매커니즘을 이용해 변경사항을 통보받지만 모든 이벤트를 놓치지 않고 받는다는 것을 보장하진 않기 때문에, 정기적으로 목록을 가져오는 작업을 수행해 누락된 이벤트가 없는지 확인을 하기도 한다. 각 컨트롤러는 API 서버에 연결하고 감시 메커니즘을 통해 컨트롤러가 담당하는 리소스 유형에서 변경이발생하면 통보해줄 것을 요청한다.컨트롤러의 동작 방식 모든 컨트롤러는 API 서버로 API 오브젝트를 제어하고, 자신이 맡은 감시 리소스를 기준으로 적절히 정의된 동작을 수행한다.11.1.7 kubelet이 하는 일 워커 노드에서 실행하는 모든것을 담당하는 구성요소kubelet의 작업 이해 첫번째 작업은 실행중인 노드를 노드 리소스로 만들어 API 서버에 등록하는 것이다. 그리고 API 서버를 지속적으로 모니터링하다가 해당 노드에 파드가 스케줄링 되면, 파드의 컨테이너를 실행시킨다. 실행중인 컨테이너를 계속 모니터링하면서 상태, 이벤트, 리소스 사용량을 API 서버에 보고한다. kubelet은 컨테이너 라이브니스 프로브를 실행도 담당한다.API 서버 없이 정적 파드 실행 특정 로컬 디렉터리 안에 있는 매니페스트 파일을 기반으로 파드를 실행할 수도 있다. 컨트롤 플레인 구성요소를 파드로 실행하는데 이 방식을 이용한다. 시스템 구성 요소 파드 매니페스트를 kubelet의 매니페스트 디렉터리 안에 너허서 kubelet이 실행하고 관리하도록 할 수 있다.11.1.8 쿠버네티스 서비스 프록시의 역할 모든 워커 노드는 클라이언트가 쿠버네티스 API로 정의한 서비스에 연결할 수 있도록 해주는 kube-proxy도 같이 실행시킨다. kube-proxy는 서비스의 IP와 포트로 들어온 접속을 서비스를 지원하는 하나의 파드와 연결시켜준다. 파드간에 로드 밸런싱도 수행해준다.프록시라고 부르는 이유 kube-proxy의 초기 구현은 userspace에서 동작하는 프록시였기 떄문. userspace 프록시 모드에서는 아래와 같이 iptables 규칙을 설정해 kube-proxy로 전송하여 처리하였다. 현재는 훨씬 성능이 우수한 구현체에서 iptables 규칙만 사용해 프록시 서버를 거치지 않고 패킷을 무작위로 선택한 백엔드 파드로 전달한다.(iptables 프록시모드)userspace proxy vs iptables proxy 이 두 모드의 가장 큰 차이점은 패킷이 kube-proxy를 통과해 사용자 공간에서 처리되는지, 아니면 커널에서 처리되는지 여부이다.iptables proxy 유의사항 iptables 프록시 모드는 파드를 무작위로 선택하여 라운드 로빈 방식으로 처리되지 않는 다는 것이다. 클라이언트와 파드 수가 적다면 문제가 두드러지나, 많다면 특별한 문제는 없다.11.1.9 쿠버네티스 애드온 쿠버네티스 서비스의 DNS 조회, 여러 HTTP 서비스를 단일 외부 IP 주소로 노출하는 인그레스 컨트롤러, 쿠버네티스 웹 대시보드 등이 있다.애드온 배포 방식 minikube 안에는 이런 애드온들이 레플리케이션 컨트롤러로 배포되어 있다. dns 애드온은 deployment로 배포되어있다.# rc 조회kubectl get rc -n kube-system# deployment 조회kubectl get deploy -n kube-systemDNS서버 동작 방식 클러스터의 모든 파드는 기본적으로 클러스터의 내부 DNS 서버를 사용하도록 설정되어 있다. DNS 서버 파드는 kube-dns 서비스로 노출된다. 배포된 모든 컨테이너가 가지고 있는 /etc/resolv.conf 안에 nameserver로 지정되어 있다. kube-dns 파드는 API 서버 감시 매커니즘을 이용해 모든 클라이언트가 거의 항상 최신 DNS 정보를 얻을 수 있도록 한다. 서비스 혹은 엔드포인트 리소스가 갱신되는 짧은 시간동안 유효하지 않는 타이밍이 있기는 하다. 인그레스 컨트롤러 동작 방식 리버스 프록시 서버(nginx)를 실행하고 클러스터에 정의된 인그레스, 서비스, 엔드포인트 리소스 설정을 유지한다. 인그레스 컨트롤러는 트래픽을 서비스의 IP로 보내지 않고 서비스의 파드로 직접 전달한다. 외부에서 접속한 클라이언트가 인그레스 컨트롤러로 연결할 때 IP를 보존하는데 영향을 주기 때문에 특정 사용 사례에서는 서비스를 선호하도록 만든다.다른 애드온 사용 모두 클러스터 상태를 관찰하고 변화가 생기면 그에 맞는 필요한 조치를 수행한다.11.1.10 모든 것을 함께 가져오기 쿠버네티스는 시스템 전체가 상대적으로 작고, 관심의 분리로 느슨하게 결합된 구성요소로 이뤄져있다. 또한 이런 구성요소들은 모두 함꼐 상호작용하여 실제 상태가 사용자가 지정한 원하는 상태와 일치하도록 유지시켜준다.11.2 컨트롤러가 협업하는 방법 대략적인 모습은 다음 그림과 같다.11.2.2 이벤트 체인디플로이먼트 컨트롤러가 레플리카셋 생성레플리카셋 컨트롤러가 파드 리소스 생성스케줄러가 새로 생성한 파드에 노드 할당kubelet은 파드의 컨테이너를 실행12.2.3 클러스터 이벤트 관찰 컨트롤 플레인 구성 요소와 kubelet은 이러한 작업을 수행할 때 API 서버로 이벤트를 발송한다. # event 관찰kubectl get events --watch 11.3 실행중인 파드에 관한 이해 실행중인 파드에 접근해서 컨테이너를 조회해보면 다음과 같은 내용을 확인할 수 있다.퍼즈(pause) 컨테이너 파드의 모든 컨테이너를 함꼐 담고 있는 컨테이너로, 파드의 모든 컨테이너가 동일한 네트워크와 리눅스 네임스페이스를 보유하는게 유일한 목적인 인프라스트럭쳐 컨테이너이다. 이로인해 파드의 각 컨테이너들은 인프프라스르럭처 컨테이너의 리눅스 네임스페이스를 사용할수 있다. 만약 2개의 컨테이너를 가진 파드를 실행시켰다면 결과적으로는 3개의 컨테이너가 실행되는 셈이다. 컨테이너가 종료되고 다시 시작하기 위해서는 이전과 동일한 리눅스 네임스페이스의 일부가 돼야 하기 떄문에 인프라스트럭처 컨테이너의 라이프사이클은 파드의 라이프사이클과 동일하다. 인프라스트럭처 컨테이너가 그 중간에 종료되면 kubelet이 파드의 모든 컨테이너를 다시 생성한다.11.4 파드간 네트워킹 쿠버네티스 클러스터에서는 각 파드가 고유한 Ip 주소를 가지고 다른 모든 파드와 NAT 없이 플랫 네트워크로 서로 통신할 수 있다. 이 역할은 쿠버네티스 자체가 아닌 시스템 관리자 또는 컨테이너 네트워크 인터페이스(CNI) 플러그인에 의해 ㅔㅈ공된다.11.4.1 네트워크는 어떤 모습이어야 하는가? 파드가 동일한 워커 노드에서 실행중인지 여부와 관계 없이 파드끼리 서로 통신할 수 있어야 한다. 패킷은 네트워크 주소 변환(NAT) 없이 파드 A에서 파드 B로 출발지와 목적지 주소가 변경되지 않은 상태로 도착해야 한다. 파드 내붕부에서 실행중인 애플리케이션의 네트워킹이 동일한 네트워크 스위치에 접속한 시스템에서 실행되는 것처럼 간단하고 정확하게 이뤄지도록 해주기 때문이다. 인터넷에 있는 서비스와 통신할 때는 패킷의 출발지 IP를 변경하는 것이 필요하다. 파드의 IP는 모두 사설(private)이기 때문인데, 외부로 난가는 패킷의 출발지 IP는 워커 노드의 IP로 변경된다. 이 경우에는 X-Forwarded-For HTTP header 등을 통해 client ip를 알아낼수 있다. 11.4.2 네트워킹 동작 방식 자세히 살펴보기 여기서 각 파드의 네트워크 인터페이스는 인프라스트럭처 컨테이너(퍼즈 컨테이너)로부터 설정한 것이다.동일한 노드에서 파드 간의 통신 활성화 컨테이너를 위한 가상 이더넷 인터페이스 쌍(veth쌍)이 생성된다. 이 2개의 가상 인터페이스는 파이프의 양쪽 끝과 같다. 이 이더넷 인터페이스는 브리지의 주소 범위 안에서 IP를 할당받고, 컨테이너에서 패킷을 전송할떄는 이 eth0 인터페이스로 나와서 브리지를 통해 다른쪽으로 전달된다.(브리지에 연결된 모든 네트워크 인터페이스에서 수신 가능) 노드에 있는 모든 컨테이너는 같은 브리지에 연결되어 있어서 서로 통신이 가능하다. 다른 노드에서 실행중인 컨테이너가 서로 통신하려면 노드 사이의 브리지가 서로 연결되어있으면 가능하다.서로 다른 노드에서 파드간의 통신 활성화 오버레이, 언더레이 네트워크, 일반적인 계층3 라우팅을 통해서 처리가 가능하다. 다만 파드 IP 주소는 전체 클러스터 내에서 유일해야 하기 때문에 노드 사이의 브리지는 겹치지 않는 주소 범위를 사용해서 클러스터 내 파드들이 같은 IP주소를 얻지 못하도록 한다.(이렇게 IP 충돌을 막는다.) 다른 노드로 패킷을 전송할떄는 패킷이 먼저 veth쌍을 통과하고 브리지를 통해 노드의 물리 어댑터로 전달된다. 그 다음 회선을 통해 다른 노드의 물리 어댑터로 전달되고, 노드의 브리지를 지나 목표 컨테이너의 veth쌍을 통과하게 된다. 두 노드가 라우터 없이 같은 네트워크 스위치에 연결된 경우만 동작한다. 라우터의 경우 노드간의 라우터가 늘어날수록 점점 어려워지고, 오류가 발생할 여지가 늘어나서, 소프트웨어 정의 네트워크(SDN, Software Defined Network)을 사용하는 것이 더 쉽다. SDN을 이용하면 하부 네트워크 토폴로지가 아무리 복잡해지더라도 노드들이 같은 네트워크에 연결된 것으로 볼 수 있다. 파드에서 전송한 패킷은 캡슐화돼 네트워크로 다른 파드가 실행중인 노드로 전달되고 디캡슐화 단계를 거쳐 원래 패킷 형태로 대상 파드에 전달된다.11.4.3 컨테이너 네트워크 인터페이스 소개 컨테이너를 네트워크에 쉽게 연결하기 위해 시작된 프로젝트. 쿠버네티스는 어떤 CNI 플러그인이든 설정이 가능하다. 쿠버네티스에 네트워크 플러그인 설치는 데몬셋과 다른 지원 리소스를 가지고 있는 yaml을 배포하면 된다. kubelet을 시작할 떄 –network-plugin=cni 옵셥능 주고 시작하면 노드의 CNI 인터페이스에 연결할 수 있다.CNI 플러그인 종류 Calico Flannel Romana Weave Net 그 외 기타11.5 서비스 구현 방식11.5.1 kube-proxy 소개 서비스와 관련된 모든 것은 각 노드에서 동작하는 kube-proxy 프로세스에 의해 처리된다. 초기에는 실제 프록시로서 연결을 기다리다가 들어온 연결을 위해 해당 파드로 가는 새로운 연결을 생성했었다(userspace 프록시 모드) 현재는 성능이 더 우수한 iptables 프록시 모드를 사용한다. 서비의 Ip주소는 가상이고, 서비스의 주요 핵심 사항인 서비스는 IP와 포트의 쌍으로 구성된다는것이 중요한 사항이고, 서비스 IP만으로는 아무것도 나타내지 않는다.(ping을 할 수 없는 이유)11.5.2 kube-proxy가 iptables를 사용하는 방법 서비스 리소스가 생성되면 가상 IP주소가 바로 할당되고, API 서버는 워커 노드에 실행중인 모든 kube-proxy 에이전트에 새로운 서비스가 생성됐음을 통보한다. 각 kube-proxy는 실행 중인 노드에 해당 서비스 주소로 접근할 수 있도록 만든다. 서비스의 IP/포트 쌍으로 향하는 패킷을 가로채서, 목적지 주소를 변경해 패킷이 서비스를 지원하는 여러 파드 중 하나로 리디렉션되도록 하는 몇개의 iptables 규칙을 설정하는 것. kubeproxy는 모든 엔드포인트 오브젝트를 감시한다. 처음 패킷의 목적지가 서비스의 IP와 포트로 지정되지만, 패킷이 네트워크로 전송되기 전에 노드A의 커널이 노드에 설정된 iptables 규칙에 따라 먼저 처리가 이루어지고, 임의로 선택한 파드의 IP와 포트로 교체한다.11.6 고가용성 클러스터 실행 서비스를 중단 없이 계속 실행하게 하기 위해서는 애플리케이션 뿐만 아니라 쿠버네티스 컨트롤 플레인 구성 요소도 항상 동작하고 있어야 한다. 이를 위해서는 고가용성이 필요할 수 있다.11.6.1 애플리케이션 가용성 높이기가동 중단 시간을 줄이기 위한 다중 인스턴스 실행 일반적으로 애플리케이션을 수평으로 확장할 수 있어야 하지만 수평확장할수 없는 경우라도 레플리카수를 1로 지정된 디플로이먼트를 사용하는 것이 좋다. 레플리카를 사용할수는 없지만, 문제가 생겼을때 새 레플리카로 빠르게 교체하기 위함. 수평 스케일링이 불가능한 애플리케이션을 위한 리더 선출 매커니즘 사용 중단 시간이 발생하는 것을 피하려면, 활성 복제본과 함께 비활성 복제본을 실행해두고, 빠른 임대 혹은 리더 선출 매커니즘을 이용해 단 하나만 활성화 상태로 만들어야 한다. 리더가 아닌 인스턴스들은 리더가 되는것을 기다리고 리더만 데이터를 처리한다. 그러다 리더가 문제가 생기면 대기하던 인스턴스들이 리더가 되기를 경쟁하는 방식 리더만 쓰기가 가능하고 나머지는 읽기전용으로 처리해서 사용하는 방식 경쟁 조건(race condition)으로 예측할 수 없는 시스템 동작이 발생하더라도 두 인스턴스가 같은 작업을 하지 않도록 할 수 있다. 11.6.2 쿠버네티스 컨트롤 플레인 구성 요소의 가용성 향상 컨트롤 플레인의 구성요소인 etcd, API 서버, 컨트롤러 매니저, 스케줄러의 가용성을 확장한 경우 다음과 같이 구성된다.etcd 클러스터 실행 etcd 자체가 분산 시스템으로 설게뙤어 있어서 필요한 수의 머신(3,5,7 : 홀수로 지정)에서 인스턴스를 실행하고 서로를 인식할 수 있게만 하면 된다. 모든 인스턴스에 걸쳐 데이터를 복제하기 때문에 3개의 머신으로 구성된 클러스터는 한 노드가 실패하더라도 읽기와 쓰기 작업을 모두 수행할 수 있다. 7대보다 더 크게 하는 경우 레플리케이션으로 인해 오히려 성능에 영향을 줄수도 있따.여러 APi 서버 인스턴스 실행 stateless하기 때문에 서로 인지할 필요도 없고 레플리카수를 쉽게 늘려도 된다.컨트롤러와 스케줄러의 고가용성 확보 컨트롤러 매니저나 스케줄러는 여러 인스턴스에서 동시에 실행하는 것은 어려운 일이다.(하나의 작업이 중복 수행되면 안되기 떄문에) 컨트롤러 매니저나 스케줄러 같은 구성 요소는 여러 인스턴스를 실행하기 보다는 한 번에 하나의 인스턴스만 활성화되게 해야 하고, 가용성을 위해 –leader-elect 옵션으로 제어할 수 있다. 리더만 실제로 작업을 수행하고 나머지 다른 인스턴스는 대기하면서 현재 리더가 실패할 경우를 기다린다.컨트트롤 플레인 구성 요소에서 사용되는 리더 선출 매커니즘 이해 리더를 선출하기 위해 서로 직접 대화할 필요가 없고, API 서버에 오브젝트를 생성하는 것만으로 완전히 동작시킬 수 있다. 쿠버네티스에서는 “control-plane.alpha.kubernetes.io/leader” 어노테이션의 holderIdentity 필드에 이름을 넣는데 처음 성공한 인스턴스가 리더가 되는 방식을 사용한다.(승자는 언제나 하나뿐, 낙관적 락에 의해)Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 10. 스테이트풀셋: 복제된 스테이트풀 애플리케이션 배포하기", "url": "/posts/devlog-platform-kubernetes-in-action10/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-09-02 17:34:00 +0900", "snippet": "10. 스테이트풀셋: 복제된 스테이트풀 애플리케이션 배포하기 볼륨이나 퍼시스턴트볼륨클레임에 바인딩된 퍼시스턴트볼륨을 통해서 데이터베이스를 일반 파드에 실행했었는데, 이 파드들을 스케일아웃하려면 어떻게 할수 있을지 살펴본다.10.1 스테이트풀 파드 복제하기 스테이트풀 파드를 복제할떄 레플리카셋을 이용한다면 어떨까? 레플리카셋은 하나의 파드 템플릿에...", "content": "10. 스테이트풀셋: 복제된 스테이트풀 애플리케이션 배포하기 볼륨이나 퍼시스턴트볼륨클레임에 바인딩된 퍼시스턴트볼륨을 통해서 데이터베이스를 일반 파드에 실행했었는데, 이 파드들을 스케일아웃하려면 어떻게 할수 있을지 살펴본다.10.1 스테이트풀 파드 복제하기 스테이트풀 파드를 복제할떄 레플리카셋을 이용한다면 어떨까? 레플리카셋은 하나의 파드 템플릿에서 여러 개의 파드 레플리카를 생성한다. 여러 개개의 파드 레플리카를 복제하는 데 사용하는 파드 템플릿에는 클레임에 관한 참조가 있으므로 각 레플리카가 별도의 퍼시스턴트볼륨클레임을 사용하도록 만들 수가 없음.10.1.1 개별 스토리지를 갖는 레플리카 여러 개 실행하기 이에 대한 방법은 여러가지가 있겠지만 현실성이 대부분 부족하다수동으로 파드 생성하기 레플리카셋이 파드를 감시하지 못하므로 가능한 옵션이 아니다.파드 인스턴스별로 하나의 레플리카셋 사용하기 이 경우 의도된 레플리카 수를 변경할 수 없고, 추가 레플리카셋을 생성해야 한다.(이러면 레플리카셋을 쓰는 의미가 있는가..? 없다.)동일 볼륨을 여러 개 디렉터리로 사용하기 인스턴스간 조정이 필요하고 올바르게 수행하기 쉽지 않다. 심지어 공유 스토리지 볼륨에서 병목이 발생할수도 있다.10.1.2 각 파드에 안정적인 아이덴티티 제공하기 특정 애플리케이션이 안정적인 네트워크 아이덴티티를 요구하는 이유는 무엇일까? 분산 스테이트풀 애플리케이션에서는 이러한 요구사항이 상당히 일반적이다. 이를테면 ACL을 구성하기도 하고.. 쿠버네티스에서는 매번 파드가 재스케줄링될 수 있고, 새로운 파드는 새로운 호스트 이름과 IP주소를 할당받으므로 구성 멤버가 재스케줄링될 때마다 모든 애플리케이션 클러스터가 재구성돼야 한다.각 파드 인스턴스별 전용 서비스 사용하기 개별 파드는 자신이 어떤 서비스를 통해 노출되는지 알수 없으므로 안정적인 IP를 알수 있는것도 아니고, 모든 문제를 해결할 수 조차 없다.10.2 스테이트풀셋 10.1에서 나열한 문제들을 해결할 수 있도록 쿠버네티스에서는 스테이트풀셋(StatefulSet)을 제공한다. 스테이트풀셋은 애플리케이션의 인스턴스가 각각 안정적인 이름과 상태를 가지며 개별적으로 취급돼야 하는 애플리케이션에 알맞게 만들어졌다.10.2.1 스테이트풀셋과 레플리카셋 비교애완동물 vs 가축스테이트풀셋 - 애완동물 각 인스턴스에 이름을 부여하고 개별적으로 관리한다 스테이트풀 파드가 종료되면 새로운 파드 인스턴스는 교체되는 파드와 동일한 이름, 네트워크 아이덴티티, 상태 그대로 다른 노드에서 되살아나야 한다.레플리카셋 - 가축 스테이트리스 애플리케이션의 인스턴스는 이름을 정확히 알고 있을 필요가 없고 몇마리가 있는지만 중요하다. 언제든 완전히 새로운 파드로 교체되어도 된다.10.2.2 안정적인 네트워크 아이덴티티 제공하기 스테이트풀셋으로 생성된 파드는 서수 인덱스(0부터 시작)가 할당되고 파드의 이름과 호스트 이름, 안정적인 스토리지를 붙이는 데 사용된다.거버닝 서비스 스테이트풀 파드는 때떄로 호스트 이름을 통해 다뤄져야 할 필요가 있다.(스테이트리스는 이런 니즈가 없음) 스테이트풀 파드는 각각 서로 다르므로(다른 상태를 가지거나) 요구사항에 따라 특정 스테이트풀 파드에서 동작하기를 원할수도 있다. 위와 같은 이유로 스테이트풀셋은 거버닝 헤드리스 서비스(governing headless service)를 생성해서 각 파드에게 실제 네트워크 아이덴티티를 제공해야 한다.(헤드리스 서비스를 통해 고정된 하나의 IP가 아닌 서비스에 맵핑된 모든 파드의 IP 목록을 얻는다.)스테이트풀셋 교체하기 스테이트풀셋은 레플리카셋이 하는 것과 비슷하게 새로운 인스턴스로 교체되도록 한다. 하지만 교체되더라도 이전에 사라진 파드와 동일한 호스트 이름을 갖는다. 그렇기 떄문에 파드가 다른 노드로 재스케줄링되더라도 같은 클러스터 내에서 이전과 동일한 호스트 이름으로 접근이 가능하다.스테이트풀셋 스케일링 스테이트풀셋의 스케일 다운의 좋은 점은 항상 어떤 파드가 제거될지 알수 있다는 점이다. 스테이트풀셋의 스케일 다운은 항상 가장 높은 서수 인덱스의 파드를 먼저 제거한다. 스테이트풀셋은 인스턴스 하나라도 비정상인 경우 스케일 다운 작업을 허용하지 않는다. 그 이유는 여러개 노드가 동시에 다운되는 경우 데이터를 잃을수도 있기 떄문이다.)10.2.3 각 스테이트풀 인스턴스에 안정적인 전용 스토리지 제공하기 스테이트풀 파드의 스토리지는 영구적이어야 하고 파드와는 분리되어야 한다. 스테이트풀셋의 각 파드는 별도의 퍼시스턴트볼륨을 갖는 다른 퍼시스턴트볼륨클레임을 참조해야 한다.(이를 스테이트풀셋에서 제공함.)볼륨 클레임 템플릿과 파드 템플릿을 같이 구성 스테이트풀셋이 파드를 생성하는 것과 같은 방식으로 퍼시스턴트볼륨클레임 또한 생성할 수 있다.퍼시스턴트볼륨클레임의 생성과 삭제의 이해 생성할떄는 파드와 퍼시스턴트볼륨클레임 등 2개 이상의 오브젝트가 생성이 되는데.. 스케일 다운을 할 떄는 파드만 삭제하고 클레임은 남겨둔다.(클레임이 삭제된 후 바인딩됐던 퍼시스턴트볼륨은 재활용되거나 삭제돼 콘텐츠가 손실될 수 있기 때문) 그래서 기반 퍼시스턴트볼륨을 해제하려면 퍼시스턴트볼륨클레임을 수동을 삭제해주어야 한다.동일 파드의 새 인스턴스에 퍼시스턴트볼륨클레임 다시 붙이기 스테이트풀셋은 스케일 다운되었을 때 기존에 있던 퍼시스턴트볼륨클레임을 유지했다가 스케일 업될떄 다시 해당 볼륨클레임을 연결한다.10.2.4 스테이트풀셋 보장(guarantee)안정된 아이덴티티와 스토리지의 의미 쿠버네티스상에서 보장해주지 않는다고 가정할때, 동일한 아이덴티티를 가지는 교체 파드를 생성되면 애플리케이션의 두 개 인스턴스가 동일한 아이덴티티로 시스템에서 실행하게 되면 큰 문제가 될수도 있다.스테이트풀셋 최대 하나의 의미 쿠버네티스는 두 개의 스테이트풀 파드 인스턴스가 절대 동일한 아이덴티티로 실행되지 않고, 동일한 퍼시스턴트볼륨클레임에 바인딩되지 않는것을 보장한다. 스테이트풀셋은 교체 파드를 생성하기 전에 파드가 더 이상 실행중이지 않는다는 점을 절대적으로 확신해야 처리가 이루어진다.10.3 스테이트풀셋 사용하기10.3.1 스테이트풀셋을 위해 준비사항 데이터 파일을 저장하기 위한 퍼시스턴트볼륨(동적 프로비저닝을 해도됨) 스테이트풀셋에 필요한 거버닝 헤드리스 서비스 스테이트풀셋10.3.2 스테이트풀셋 배포퍼시스턴트볼륨 생성kind: List # 여러개의 리소스를 정의할때 List를 사용할 수 있다.apiVersion: v1items:- apiVersion: v1 kind: PersistentVolume metadata: name: pv-a spec: capacity: storage: 1Mi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle hostPath: path: /tmp/pv-a- apiVersion: v1 kind: PersistentVolume metadata: name: pv-b spec: capacity: storage: 1Mi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle hostPath: path: /tmp/pv-b- apiVersion: v1 kind: PersistentVolume metadata: name: pv-c spec: capacity: storage: 1Mi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle hostPath: path: /tmp/pv-c# 볼륨 생성kubectl create -f persistent-volumes-hostpath.yaml거버닝 서비스 생성하기 apiVersion: v1kind: Servicemetadata: name: kubiaspec: clusterIP: None # 헤드리스 서비스를 위함 selector: app: kubia ports: - name: http port: 80# 거버닝 헤드리스 서비스 생성kubectl create -f kubia-service-headless.yaml스테이트풀셋 생성하기 첫번째 파드가 생성되고 준비가 완료되어야만 두 번째 파드가 생성된다. 두 개 이상의 멤버가 동시에 생성되면 레이스 컨디션에 빠질 가능성이 있기 떄문에 스테이트풀셋은 순차적으로 하나씩만 처리된다.apiVersion: apps/v1kind: StatefulSetmetadata: name: kubiaspec: serviceName: kubia replicas: 2 selector: matchLabels: app: kubia # has to match .spec.template.metadata.labels template: metadata: labels: app: kubia spec: containers: - name: kubia image: luksa/kubia-pet ports: - name: http containerPort: 8080 volumeMounts: # 볼륨 마운트 - name: data mountPath: /var/data volumeClaimTemplates: # 볼륨클레임 템플릿 - metadata: name: data spec: resources: requests: storage: 1Mi accessModes: - ReadWriteOnce# 스테이트풀셋 생성kubectl create -f kubia-statefulset.yaml# statefulset 조회kubectl get statefulset# pod 조회kubectl get po | grep kubia-# 생성된 statefulset pod 조회kubectl get po kubia-0 -o yaml10.3.3 생성된 파드 확인 파드에 피기백(exec를 통해 파드 내부로 진입해서 처리하는 방식)하거나 API 서버를 통해 데이터를 확인해본다.# api server local 프록시kubectl proxy# 파드 엔드포인트 조회curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/스테이트풀셋 파드를 삭제해 재스케줄링된 파드가 동일한 스토리지에 연결되는지 확인 새 파드는 클러스터의 어느 노드에서나 스케줄링될 수 있으며 이전 파드가 스케줄링 됐던 동일한 노드일 필요가 없다. 이전 파드의 모든 아이덴티티(이름, 호스트 이름, 스토리지)는 새 노드로 효과적으로 이동된다.# 스테이트풀셋 파드 데이터 수정curl -X POST -d \"Hey three! This greeting was submitted to kubia-0.\" localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/# 변경한 파드 삭제(삭제하면 직후 스테이트풀셋이 새로운 파드를 생성한다.)kubectl delete po kubia-0# 재확인(수정한 데이터가 그대로 남아있음.)curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/스테이트풀셋 스케일링 가장 중요한 점은 스케일 업/다운이 점진적으로 수행되며 스테이트풀셋이 초기에 생성됐을 때 개별 파드가 생성되는 방식과 유사하다는 점이다. 또한 스케일 다운시 가장 높은 서수의 파드가 먼저 삭제되고, 파드가 완전히 종료된 이후부터 다음 스케일다운이 수행된다.스테이트풀 파드를 헤드리스가 아닌 일반적인 서비스로 노출하기apiVersion: v1kind: Servicemetadata: name: kubia-publicspec: selector: app: kubia ports: - port: 80 targetPort: 8080API 서버를 통해 클러스터 내부 서비스에 연결# cluster ip service 호출curl localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/``## 10.4 스테이트풀셋의 피어 디스커버리 - 클러스터된 애플리케이션의 중요한 요구사항은 피어 디스커버리(클러스터의 다른 멤버를 찾는 기능)이다. - API서버와 통신해서 찾을수 있지만, 쿠버네티스의 목표 중 하나는 애플리케이션을 완전히 쿠버네티스에 독립적으로 유지하며 기능을 노출하는 것이다.### SRV 레코드 - srvlookup이라 부르는 일회용 파드(--restart=Never)를 실행하고 콘솔에 연결하며(-it) 명령어를 수행하고 종료되며, 바로 삭제된다.(--rm) - ANSWER SECTION에는 헤드리스 서비스를 뒷받침하는 두 개의 파드를 가리키는 두 개의 SRV 레코드를 확인할 수 있다. - 파드가 스테이트풀셋의 다른 모든 파드의 목록을 가져오려면 SRC DNS 룩업을 수행해서 얻을수 있다는 것이다.``` sh# srvlookup을 할수 있는 dnsutils pod을 하나 수행시켜서 명령어를 수행하고 파드 종료kubectl run -it srvlookup --image=tutum/dnsutils --rm --restart=Never -- dig SRV kubia.default.svc.cluster.local;; ANSWER SECTION:kubia.default.svc.cluster.local. 30 IN\tSRV\t0 50 80 kubia-0.kubia.default.svc.cluster.local.kubia.default.svc.cluster.local. 30 IN\tSRV\t0 50 80 kubia-1.kubia.default.svc.cluster.local.;; ADDITIONAL SECTION:kubia-1.kubia.default.svc.cluster.local. 30 IN A 172.17.0.8kubia-0.kubia.default.svc.cluster.local. 30 IN A 172.17.0.310.4.1 DNS를 통한 피어 디스커버리 스테이트풀셋과 SRV 레코드를 사용하여 헤드리스 서비스의 모든 파드를 직접 찾는다.import dns from 'dns'dns.resolveSrv(serviceName, (err, addresses) { // TODO 무언가 처리})10.4.2 스테이트풀셋 업데이트 편집기로 statefulset 정의 내에서 레플리카 수를 수정한다면 수정 직후 파드가 생성되는걸 확인할 수 있다. 만약 이미지를 변경한다면 디플로이먼트처럼 롱링 업데이트도 필요할텐데 쿠버네티스 1.7부터 이 기능도 지원한다.# 편집기로 statefulset 정의 열어서 수정kubectl edit statefulset kubia# pod 확인kubectl get po10.4.3 클러스터된 데이터 저장소 사용하기 SRV lookup을 통해 서비스에 포함된 모든 파드를 찾을수 있기 때문에 스테이트풀셋을 스케일 업하거나 스케일 다운하더라도 클라이언트 요청을 서비스하는 파드는 항상 그 시점에 실행중인 모든 피어를 찾을 수 있다.10.5 스테이트풀셋이 노드 실패를 처리하는 과정 스테이트풀셋은 노드가 실패한 경우 동일한 아이덴티티와 스토리지를 가진 두 개의 파드가 절대 실행되지 않는 것을 보장하므로, 스테이트풀셋은 파드가 더 이상 실행되지 않는다는 것을 확실할 때까지 대체 파드를 생성할 수 없으며, 생성해서도 안된다. 이 경우 오직 클러스터 관리자가 알려줘야만 알 수 있고, 이를 위해 관리자는 파드를 삭제하거나 전체 노드를 삭제해야 한다.(노드 삭제시 노드에 스케줄링된 모든 파드가 삭제됨) 노드가 다운된 상태에서 파드를 삭제하게 되면 쿠버네티스 클러스터(마스터) 기준으로는 파드가 삭제됐으나, 다운된 노드에서는 이를 알 방법이 없기 때문에 실제 노드에 스케줄링된 파드가 삭제되지는 않는다. 이 경우 파드를 강제로 삭제해야 할 수 있다. 노드가 더 이상 실행중이 아니거나 연결 불가함을 아는 경우가 아니라면, 스테이트풀 파드를 강제로 삭제해서는 안된다.(영구적으로 유지되는 좀비 파드가 생성될수 있다.)Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 9. 디플로이먼트 : 선언적 애플리케이션 업데이트", "url": "/posts/devlog-platform-kubernetes-in-action9/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-30 17:34:00 +0900", "snippet": "9. 디플로이먼트 : 선언적 애플리케이션 업데이트 쿠버네티스 클러스터에서 실행되는 애플리케이션을 업데이트 하는 방법과 쿠버네티스가 어떻게 무중단 업데이트 프로세스로 전환하는 데 도움을 주는지 살펴본다.9.1 파드에서 실행중인 애플리케이션 업데이트 쿠버네티스에서 실행되는 애플리케이션 기본 구성은 아래와 같다. 여기서 파드에서 실행중인 컨테이너 이미지...", "content": "9. 디플로이먼트 : 선언적 애플리케이션 업데이트 쿠버네티스 클러스터에서 실행되는 애플리케이션을 업데이트 하는 방법과 쿠버네티스가 어떻게 무중단 업데이트 프로세스로 전환하는 데 도움을 주는지 살펴본다.9.1 파드에서 실행중인 애플리케이션 업데이트 쿠버네티스에서 실행되는 애플리케이션 기본 구성은 아래와 같다. 여기서 파드에서 실행중인 컨테이너 이미지 버전을 업데이트한다고 할때 어떻게 해야할까?모든 파드를 업데이트 하는 방법 기존 파드를 모두 삭제한 다음 새 파드를 시작한다. 새로운 파드를 시작하고, 기동하면 기존 파드를 삭제한다.9.1.1 오래된 파드를 삭제하고 새 파드로 교체 (v1 -&gt; v2로 업데이트한다고 했을때) v1 파드 세트를 관리하는 레플리카셋이 있는 경우 이미지의 버전 v2를 참조하도록 파드 템플릿을 수정한 다음 이전 파드 인스턴스를 삭제해 쉽게 교체할 수 있을것이다.9.1.2 새 파드 기동과 이전 파드 삭제 한 번에 여러 버전의 애플리케이션이 실행하는 것을 지원하는 경우(다른 버전의 애플리케이션이 같이 서빙되어도 문제가 없는 경우) 새 파드를 모두 기동한 후 이전 파드를 삭제할 수 있다. 잠시동안 동시에 두 배의 파드가 실행되므로 더 많은 하드웨어 리소스가 필요하다.한 번에 이전 버전에서 새 버전으로 전환 새 버전을 실행하는 파드를 불러오는 동안 서비스는 파드의 이전 버전에 연결된다. 새 파드가 모두 실행되면 서비스의 레이블 셀렉터를 변경하고 서비스를 새 파드로 전환할 수 있다.(블루-그린 디플로이먼트) kubectl set selector 명령어를 사용해 서비스의 파드 셀렉터 변경이 가능롤링 업데이트 수행 이전 파드를 한번에 삭제하는 방법 대신 파드를 단계적으로 교체하는 롤링 업데이트를 수행할 수도 있다. 2개의 레플리카셋을 이용해서 상태를 보아가면서 수행할수도 있겠지만, 쿠버네티스에서는 하나의 명령으로 롱링 업데이트를 수행할 수 있다.9.2 레플리케이션컨트롤러로 자동 롤링 업데이트 수행하나의 YAML에 여러개의 쿠버네티스 리소스를 정의하는 방법 ---(대시 3개)를 구분자로 여러 리소스 정의를 포함할 수 있다.``` yamlapiVersion: v1kind: ReplicationControllermetadata: name: kubia-v1spec: replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: image: luksa/kubia:v1name: nodejs— # YAML 구분자apiVersion: v1kind: Servicemetadata: name: kubiaspec: type: LoadBalancer selector: app: kubia ports: port: 80targetPort: 8080```9.2.2 kubectl을 이용한 롤링 업데이트# v1 rc and service 생성kubectl create -f kubia-rc-and-service-v1.yaml# v2 롤링 업데이트 수행kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2 rolling-update 명령어를 수행하면 일단 kubia/v2에 대한 rc가 만들어지면서 롤링 업데이트가 수행된다.롤링 업데이트 과정동일한 이미지 태그로 업데이트 푸시하기 워커 노드에서 일단 이미지를 한번 가져오면 이미지는 노드에 저장되고, 동일한 이미지를 사용해 새 파드를 실행할 때 이미지를 리모트에서 다시 가져오지 않는다.(도커 이미지 기본 정책) 즉 이미지의 변경한 내용을 같은 이미지 태그로 푸시하더라도 이미지가 변경되지 않는다. 이런 일을 해결할 수 있는 방법으로 컨테이너의 imagePullPolicy속성을 Always로 설정하면 가져올 수 있다. 이미지의 latest 태그를 참조하는 경우에는 imagePullPolicy의 기반값은 always로 항상 리모트에서 가져오지만, 다른 태그인 경우에는 기본 정책인 ifNotPresnet이다. 가장 좋은 방법은 이미지를 변경할 때마다 새로운 태그로 지정하는 방식이다.롤링 업데이트가 시작되기 전 kubeclt이 수행한 단계 이해하기 롤링 업데이트 프로세스는 첫 번째 rc의 셀렉터도 수정한다. 레이블에 deployment라는 키가 추가되고 value로 hashValue가 추가된다. v2에는 다른 value를 가진 deployment 가 추가된다.# rc kubia-v1 상태 확인kubectl describe rc kubia-v1# pod label 확인kubectl get po --show-labels레플리케이션컨트롤러 두 개를 스케일업해 새 파드로 교체 service selector는 app=kubia로만 참조되므로, rc에서 하나씩 scale up / scale down이 일어나면서 파드가 교체되고, 롤링 업데이트가 이루어진다. 롤링 업데이트를 계속하면 v2 파드에 대한 요청 비율이 점점 더 높아지기 시작한다. 마지막 v1 파드가 삭제되고, 서비스가 이제 v2 파드에 의해서만 지원하게 되는데, 이때 kubectl은 v1 rc를 삭제하고 업데이트 프로세스가 완료된다.Scaling kubia-v2 up to 1Scaling kubia-v1 down to 29.2.3 kubectl rolling-update를 더 이상 사용하지 않는 이유1) 스스로 만든 오브젝트를 쿠버네티스가 수정하기 떄문에 클러스터 개발자가 등록한 매니페스트를 무시하고 쿠버네티스가 변경한다.(deployment label 같은 경우)2) 롤링 업데이트를 수행하는 레벨이 클라이언트에서 이루어지기 때문 --v 옵션을 사용해 자세한 로딩을 켜면 이를 확인할 수 있다. kubectl 클라이언트가 쿠버네티스 마스터 대신 스케일링을 수행하는 중임을 볼수 있다. 서버가 아닌 클라이언트가 업데이트 프로세스를 수행하면 왜 문제일까? 업데이트를 수행하는 동안 네트워크 연결이 끊어지는 경우, 중간 상태로 프로세스가 종료될수 있기 떄문에 리스크가 있다. 3) rolling-update 자체가 명령(imperative)을 나타내기 떄문 쿠버네티스에 파드를 추가하거나 초과된 파드를 제거하라고 지시하지 마라(대신 레플리카 수를 변경하여 쿠버네티스 서버가 알아서 하도록 맡겨야 한다.)9.3 애플리케이션을 선언적으로 업데이트 하기 위한 디플로이먼트 사용하기 낮은 수준의 개념으로 간주되는 RC, RS을 통해 수행하는 대신 애플리케이션을 배포하고 선언적으로 업데이트 하기 위한 높은 수준의 리소스 디플로이먼트를 생성하면 레플리카셋 리소스가 그 아래에 생성된다. 애플리케이션 업데이트할 때는 추가 레플리케이션컨트롤러를 도입하고, 두 컨트롤러가 잘 조화하도록 조정해야 하는데, 이를 전체적으로 통제하는 것이 디플로이먼트이다.9.3.1 디플로이먼트 생성 디플로이먼트는 레이블 셀렉터, 원하는 레플리카수, 파드 템플릿으로 구성된다. 리소스가 수정될 때 업데이트 수행 방법을 정의하는 디폴로이먼트 전략을 지정할수 도 있음.디플로이먼트 매니페스트 생성apiVersion: apps/v1kind: Deploymentmetadata: name: kubiaspec: replicas: 3 template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v1 name: nodejs selector: matchLabels: app: kubia# rc 제거kubectl delete rc --all# deployment 생성# --recode 명령줄을 포함시켜 개정 이력(revision history)에 기록하여야 한다.kubectl create -f kubia-deployment-v1.yaml --record# deployment 조회kubectl get deploy# deployment에 의해 생성된 rc 조회kubectl get rs# deployment, rs에 의해 생성된 pod 조회ubectl get po | grep kubia-59d857디플로이먼트 롤아웃 상태 출력# 롤아웃 상태 출력kubectl rollout status deploy kubia# pod 확인kubectl get po디플로이먼트가 레플리카셋을 생성하는 방법과 레플리카셋이 파드를 생성하는 방식 이해 컨트롤러 이름과 임의로 생성된 문자열로 구성된다. 디플로이먼트에서 생성한 파드 3개에는 이름 중간에 숫자 값이 추가로 포함된다. 이 숫자 값은 파드 템플릿의 해시값을 나타낸다.# 레플리카셋 조회kubectl get rs 디플로이먼트에서 생성된 레플리카셋의 이름을 보면 여기에도 파드 템플릿의 해시값이 포함되어 있다. 디플로이먼트는 파드 템플릿의 각 버전마다 하나씩 여러 개의 레플리카셋을 만든다. 이 파드 템플릿의 해시 값을 사용하면 디플로이먼트에서 지정된 버전의 파드 템플릿에 관해 항상 동일한 레플리카셋을 사용할 수 있다.9.3.2 디플로이먼트 업데이트 디플로이먼트 리소스에 정의된 파드 템플릿을 수정하기만 하면 쿠버네티스가 실제 시스템 상태를 리소스에 정의된 상태로 만드는 데 필요한 모든 단계를 수행한다.사용 가능한 디플로이먼트 전략 기본 전략은 RollingUpdate 전략이다. RollingUpdate 전략은 이전 버전과 새 버전을 동시에 실행할 수 있는 경우에만 사용해야 한다. 대안으로 존재하는 Recreate 전략은 한 번에 기존 모든 파드를 삭제한 뒤 새로운 파드를 만드는 전략이다. 이는 앱이 여러 버전을 병렬로 실행하는 것을 지원하지 않고 새 버전을 시작하기 전에 이전 버전을 완전히 중지해야 하는 경우 사용할 수 있는데, 짧게 서비스 다운타임이 발생하는 문제가 있다.롤링 업데이트 속도 느리게 하기 디플로이먼트의 minReadySeconds 속성을 설정하여 롤링 업데이트 속도를 느리게 만들 수 있다.# deployment spec 수정kubectl patch deployment kubia -p '{\"spec\":{\"minReadySeconds\":10}}'# deployment 상태 확인kubectl get deploy -o yaml롤링 업데이트 시작 kubectl set image 명령어를 사용해 컨테이너가 포함된 모든 리소스(rc, rs, deployment 등등)을 수정할 수 있다.# image 변경kubectl set image deployment kubia nodejs=luksa/kubia:v2 --record# deployment 상태 확인kubectl get deploy -o yaml depolyment의 파드 템플릿이 업데이트돼 nodejs 컨테이너에 사용된 이미지가 kubia:v2로 변경된다.디플로이먼트와 그 외의 리소스를 수정하는 방법 명령 설명 example kubectl edit 기본 편집기로 수정 kubectl edit deploy kubia kubectl patch 오브젝트의 개별 속성 수정 kubectl patch deployment kubia -p ‘{“spec”:{“minReadySeconds”:10}}’ kubectl apply 전체 yaml/json 파일의 속성 값을 적용해 오브젝트를 수정 kubectl apply -f kubia-deployment-v2.yaml kubectl replace yaml / json 파일로 오브젝트를 새것으로 교체 kubectl replace -f kubia-deployment-v2.yaml kubectl set image 정의된 컨테이너 이미지 변경 kubectl set image deployment kubia nodejs=luksa/kubia:v2 디플로이먼트의 놀라움 파드 템플릿을 변경하는 것만으로 애플리케이션을 최신 버전으로 업데이트할 수 있음. 디플로이먼트의 파드 템플릿이 컨피그맵(또는 시크릿)을 참조하는 경우 컨피그맵은 수정하더라도 업데이트를 시작하지 않는다.(단, 새 컨피그맵을 만들고 파드 템플릿이 새 컨피그맵을 참조하도록 수정하면 업데이트가 수행된다.) 여기서 중요한 사실 중 하나는 기존 rs도 여전히 남아있는다는 것이다. 이는 롤백이나 이런 부분에서 재사용될 수 있음. 단일 디플로이먼트 오브젝트를 관리하는 것이 여러 레플리케이션 컽느롤러를 처리하고 추적하는것보다 훨씬 쉬움# rs 조회 ( 2개가 그대로 남아있는것을 알 수 있다.)kubectl get rs9.3.3 디플로이먼트 롤백# 이미지 변경kubectl set image deploy kubia nodejs=luksa/kubia:v3 --record# rollout 상태 확인kubectl rollout status deploy kubia롤아웃 되돌리기 업데이트 된 v3가 에러를 발생하기 시작할떄 롤백을 수행할 수 있다. 롤아웃 프로세스가 진행중인 동안에도 롤아웃을 중단하려면 실행 취소 명령을 사용해서 중단시킬 수 있다. (롤아웃 중에 생성된 파드는 제거되고 이전 파드로 다시 교체된다.)# 이전 버전으로 롤백kubectl rollout undo deploy kubia디플로이먼트 롤아웃 이력 표시 롤아웃 이력에 포함시키려면 –record 명령줄을 포함시켜야만 한다.# 롤아웃 이력 표시kubectl rollout history deploy kubia특정 디플로이먼트 개정으로 롤백 개정(revision) 번호를 지정해 특정 개정으로 롤백할 수 있다. 디플로이먼트를 처음 수정했을 때 비활성화된 레플리카셋이 남아있던 이유는 이 롤백을 위함이다. 모든 개정 내역의 수는 디폴로이먼트 리소스의 editionHistoryLimit 속성에 의해 제한된다.(쿠버네티스 버전이 올라가면서 revisionHistoryLimit로 변경된듯 하다) 현재 쿠버네티스 버전 기준으로 revisionHistoryLimit의 기본값은 10이다. https://kubernetes.io/ko/docs/concepts/workloads/controllers/deployment/# 1번 revision으로 롤백kubectl rollout undo deployment kubia --to-revision=19.3.4 롤아웃 속도 제어 롤링 업데이트 전략의 두 가지 추가 속성을 통해 새 파드를 만들고 기존 파드를 삭제하는 과정에서 속도를 제어할 수 있다.롤링 업데이트 전략의 maxSurge와 maxUnavailable 속성 소개 이 2개의 속성에 의해서 한 번에 몇개의 파드를 교체할지를 결정된다. maxSurge : 레플리카 수보다 얼마나 많은 파드 인스턴스 수를 허용할지를 나타낸다.(기본값 : 25%), 만약 레플리카수가 4로 설정한 경우 4의 25%인 1개만큼 더 허용될수 있다. 즉 5개의 파드까지 생성될 수 있음을 의미한다. 백분위로 설정하기 떄문에 이 값은 반올림 처리 된다. maxUnavailable : 업데이트 중에 의도하는 레플리카 수를 기준으로 사용할 수 없는 파드 인스턴스 수(기본값 25%), 레플리카수가 4인 경우 25%인 1개만큼 unavailable되는것을 허용한다. 업데이트 과정에서 사용할 수 없는 파드는 최대 1개여야 한다. 즉 최소 3개의 파드는 항상 가용한 상태를 유지하면서 롤링 업데이트가 수행되어야 한다는 의미이다. 이 백분위 값을 처리할떄는 내림으로 처리된다.apiVersion: apps/v1beta1kind: Deploymentmetadata: name: kubiaspec: replicas: 3 minReadySeconds: 10 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate 위 에제에서 레플리카수가 3이고, maxSurge가 1이고, maxUnavailable을 0으로 설정한 경우에는 다음과 같이 동작한다. 모든 파드수는 4개가 되는 것까지 허용했으며, maxUnavailable은 0이기 떄문에 available한 파드는 레플리카수와 동일한 3개여야 한다.(항상 3개의 파드를 사용할 수 있어야 한다.)maxUnavailable 속성 이해 위 예제에서 maxUnavailable만 1로 변해보면 어떻게 동작할까? 레플리카수는 3개인데 maxUnavailable이 1이기 떄문에, 롤링 업데이트 과정에서 최소 2개의 파드가 사용가능한 상황을 허용한다. 최대 동시에 실행될 수 있는 파드는 기존과 동일하게 4개이고, 2개가 가용하지 않아도 된다. 위 예제와 비교했을떄 maxUnavailable=1로 인해서 2개까지만 가용한 상태면 되기 때문에 결과적으로는 롤링 업데이트가 더 빠르게 수행될수 있다.9.3.5 롤아웃 프로세스 일시 중지 롤아웃 프로세스를 일시정지해서 일종의 카나리 릴리스를 실행할 수가 있다.(카나리 릴리스를 하는데 아주 좋은 방법은 아닌듯 하다.) 정확히 내가 원하는 레플리카수를 보장하기 힘들다. 타이밍 이슈가 발생하기 떄문에 좋은 방법이 아니다.(책의 저자가 아닌 글 작성자의 개인적인 생각) n개의 파드가 있을 때 v4의 파드는 하나만 구동시키고 나머지는 기존 버전으로 구동시킨다.(이러한 방식으로 사용자들에게 영향을 최소화하면서 변경된 로직이 문제없는지 체크하는 방식)# v4로 버전 변경kubectl set image deploy kubia nodejs=luksa/kubia:v4 --record# 롤아웃 일시 정지kubectl rollout pause deploy kubia# pod 확인kubectl get po 위 파드 목록을 보면 v4로 구동된 파드 ( kubia-586b45dbdc-5cgc6) 1개가 추가로 구동중인것을 볼수 있다.롤아웃 재개 새 버전이 제대로 작동한다고 확신하면 디플로이먼트를 다시 시작해 이전 파드를 모두 새 파드로 교체할 수 있다. 책이 쓰여진 시점 기준, 카나리 릴리스를 수행하는 적절한 방법은 두 가지 다른 디플로이먼트를 사용해 적절하게 확장하는 것이다.# 롤아웃 재개kubectl rollout resume deploy kubia롤아웃을 방지하기 위한 일시 중지 기능 사용 일시 중지 기능을 사용하면 롤아웃 프로세스가 시작돼 디플로이먼트를 업데이트하는 것을 막을 수 있고, 여러 번 변경하면서 필요한 모든 변경을 완료한 후에 롤아웃을 시작하도록 할 수 있다.9.3.6 잘못된 버전의 롤아웃 방지 minReadySeconds 속성으로 롤아웃 속도를 늦춰 롤링 업데이트 과정을 직접 볼수 있는데, 이 기능은 오작동 버전의 배포를 방지하는 목적으로도 사용할 수 있다.minReadySeconds의 적용 가능성 이해 minReadySeconds는 파드를 사용 가능한 것으로 취급하기 전에 새로 만든 파드를 준비할 시간을 지정하는 속성이다. 이것과 레디니스 프로브를 함께 이용하여 오작동 버전의 롤아웃을 효과적으로 차단할 수 있다. 모든 파드의 레디니스 프로브가 성공하면 파드가 준비상태가 되는데, minReadySeconds가 지나기전에 레디니스 프로브가 실패하기 시작하면 새 버전의 롤아웃은 차단이 된다. 적절하게 구성된 레디니스 프로브와 적절한 minReadySeconds 설정으로 쿠버네티스는 버그가 있는 버전을 배포하지 못하게 할 수 있다.적절한 minReadySeconds와 레디니스 프로브 정의apiVersion: apps/v1kind: Deploymentmetadata: name: kubiaspec: replicas: 3 minReadySeconds: 10 # minReadySeconds를 10초로 설정 selector: matchLabels: app: kubia strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 # 디플로이먼트가 파드를 하나씩만 교체하도록 0으로 설정 type: RollingUpdate template: metadata: name: kubia labels: app: kubia spec: containers: - image: luksa/kubia:v3 name: nodejs readinessProbe: # 레디니스 프로브 정의 periodSeconds: 1 # 매 초마다 레디니스 프로브 수행 httpGet: path: / port: 8080kubectl apply 를 통한 deploy 업데이트 apply를 통해 업데이트할 때 원하는 레플리카 수를 변경하지 않으려면 replicas 필드를 포함시키면 안된다.# kubectl apply 를 통한 deploy 업데이트kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml# 롤아웃 상태 확인kubectl rollout status deploy kubia레디니스 프로브가 잘못된 버전으로 롤아웃되는 것을 방지하는 법 잘못된 버전은 레디니스 프로브 단계에서 차단되어 파드가 생성되지 않는다. rollout status 명령어느 하나의 새 레플리카만 시작됐음을 보여준다. 사용 가능한 것으로 간주되려면 10초 이상 준비돼 있어야 하기 때문에 해당 파드가 사용 가능할 때까지 롤아웃 프로세스는 새 파드를 만들지 않는다. 여기서 maxUnavailable 속성이 0으로 설정되었기 때문에 원래 파드도 제거되지 않는다. 만약 위 상황에서 minReadySeconds를 짧게 설정했더라면 레디니스 프로브의 첫 번쨰 호출이 성공한 후 즉시 새 파드가 사용 가능한것으로 간주해버릴 수도 있다. 그러면 잘못된 버전으로 롤아웃이 일어나기 때문에 이 값을 적절하게 잘 설정해야 한다.롤아웃 데드라인 설정 기본적으로 쿠버네티스에서는 롤아웃이 10분동안 진행되지 않으면 실패한 것으로 간주된다. 이 값은 progressDeadlineSeconds 속성을 통해 설정할 수 있다. progressDeadlineSeconds에 지정된 시간이 초과되면 롤아웃이 자동으로 중단된다.# deploy 정보 확인kubectl describe deploy kubia잘못된 롤아웃 중지# 롤아웃 중지kubectl rollout undo deployment kubiaReference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 8. 애플리케이션에서 파드 메타데이터와 그 외의 리소스에 엑세스하기", "url": "/posts/devlog-platform-kubernetes-in-action8/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-23 17:34:00 +0900", "snippet": "8. 애플리케이션에서 파드 메타데이터와 그 외의 리소스에 엑세스하기 Downward API사용방법과 쿠버네티스 REST API 사용방법, 인증과 서버 검증을 kubectl proxy에 맡기는 방법, 컨테이너 내에서 API 서버에 접근하는 방법, 앰배서더 컨테이너 패턴의 이해, 쿠버네티스 클라이언트 라이브러리 사용방법 등을 살펴본다. 특정 파...", "content": "8. 애플리케이션에서 파드 메타데이터와 그 외의 리소스에 엑세스하기 Downward API사용방법과 쿠버네티스 REST API 사용방법, 인증과 서버 검증을 kubectl proxy에 맡기는 방법, 컨테이너 내에서 API 서버에 접근하는 방법, 앰배서더 컨테이너 패턴의 이해, 쿠버네티스 클라이언트 라이브러리 사용방법 등을 살펴본다. 특정 파드와 컨테이너 메타데이터를 컨테이너로 전달하는 방법과 컨테이너 내에서 실행중인 애플리케이션이 쿠버네티스 API 서버와 통신해 클러스터에 배포된 리소스의 정보를 얻는 것이 얼마나 쉬운지, 이런 리소스를 생하거나 수정하는 방법을 살펴보자. 8.1 Downward API로 메타데이터 전달 파드의 IP, 호스트 노드 이름, 파드 자체의 이름과 같이 실행 시점까지 알려지지 않은 데이터는 어떻게 얻어와야할까? 이러한 정보들을 여러 곳에서 반복해서 설정하는건 말이 안된다. 위 2가지 문제는 쿠버네티스의 Downward API를 사용하면 해결할 수 있다. Downward API는 애플리케이션이 호출해서 데이터를 가져오는 REST 엔드포인트와는 다르다. 파드 매니페스트에 정의한 메타 데이터를 기준으로 volume을 정의하고 이를 환경변수에 할당하여 사용할 수 있다.8.1.1 사용 가능한 메타데이터 이해 Downward API를 사용하면 파드 자체의 메타데이터를 해당 파드 내에서 실행중인 프로세스에 노출시킬 수 있다. 이러한 데이터는 OS로 직접 얻을수도 있겠지만, Downward API는 더 간단한 대안을 제공한다.메타데이터 종류 파드의 이름 파드의 IP 주소 파드가 속한 네임스페이스 파드가 실행중인 노드의 이름 파드가 실행중인 서비스 어카운트 이름 ( 일단 파드가 API 서버와 통신할 때 인증하는 계정 정도로 이해하면 된다.) 각 컨테이너의 CPU와 메모리 요청 각 컨테이너의 CPU와 메모리 제한 파드의 레이블 파드의 어노테이션8.1.2 환경변수로 메타데이터 노출하기 환경변수로 파드와 컨테이너의 메타데이터를 컨테이너에 전달하는 방법apiVersion: v1kind: Podmetadata: name: downwardspec: containers: - name: main image: busybox command: [\"sleep\", \"9999999\"] resources: requests: cpu: 15m memory: 100Ki limits: cpu: 100m memory: 20Mi # 메모리 사이즈 4Mi로는 파드가 안뜨고 20으로 올려줘야 정상동작( https://github.com/kubernetes/minikube/issues/6160 ) env: - name: POD_NAME # 특정 값을 설정하는 대신 파드 매니페스트의 metadata.name을 참조 valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: SERVICE_ACCOUNT valueFrom: fieldRef: fieldPath: spec.serviceAccountName - name: CONTAINER_CPU_REQUEST_MILLICORES valueFrom: resourceFieldRef: # 컨테이너의 CPU/메모리 요청과 제한은 fieldRef 대신 resourceFieldRef를 사용해 참조 resource: requests.cpu divisor: 1m # 리소스 필드의 경우 필요한 단위의 값을 얻으려면 제수(divisor)을 정의한다. - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES valueFrom: resourceFieldRef: resource: limits.memory divisor: 1Ki 제수는 어떤 수를 나누는 수라는 뜻으로 위에서 CPU 메모리 요청의 용량 단위# downward API를 사용하는 pod 생성kubectl create -f downward-api-env.yaml# pod의 env 조회kubectl exec downward envPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=downwardPOD_NAMESPACE=defaultPOD_IP=172.17.0.4NODE_NAME=minikubeSERVICE_ACCOUNT=defaultCONTAINER_CPU_REQUEST_MILLICORES=15CONTAINER_MEMORY_LIMIT_KIBIBYTES=20480POD_NAME=downwardKUBERNETES_PORT_443_TCP_PROTO=tcpKUBERNETES_PORT_443_TCP_PORT=443KUBIA_PORT=tcp://10.102.194.76:80KUBIA_PORT_80_TCP_ADDR=10.102.194.76KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1KUBIA_SERVICE_HOST=10.102.194.76KUBIA_PORT_80_TCP=tcp://10.102.194.76:80KUBERNETES_PORT=tcp://10.96.0.1:443KUBIA_SERVICE_PORT=80KUBIA_PORT_80_TCP_PORT=80KUBERNETES_SERVICE_HOST=10.96.0.1KUBERNETES_SERVICE_PORT=443KUBERNETES_SERVICE_PORT_HTTPS=443KUBIA_PORT_80_TCP_PROTO=tcpHOME=/root8.1.3 downwardAPI 볼륨에 파일로 메타데이터 전달 환경변수 대신 파일로 메타데이터를 노출하려는 경우 downwwardAPI 볼륨을 정의해서 컨테이너에 마운트할 수 있다. downward라는 볼륨을 정의하고 컨테이너의 /etc/downward 아래에 마운트하는 예제apiVersion: v1kind: Podmetadata: name: downward labels: # 이 레이블과 어노테이션은 downwardAPI 볼륨으로 노출된다. foo: bar annotations: key1: value1 key2: | multi line valuespec: containers: - name: main image: busybox command: [\"sleep\", \"9999999\"] resources: requests: cpu: 15m memory: 100Ki limits: cpu: 100m memory: 20Mi volumeMounts: # downward 볼륨 /etc/downward에 마운트 - name: downward mountPath: /etc/downward volumes: - name: downward # downwardAPI 볼륨 정의 downwardAPI: items: - path: \"podName\" # metadata.name에 정의한 이름은 podName 파일에 기록된다. fieldRef: fieldPath: metadata.name - path: \"podNamespace\" fieldRef: fieldPath: metadata.namespace - path: \"labels\" fieldRef: fieldPath: metadata.labels - path: \"annotations\" fieldRef: fieldPath: metadata.annotations - path: \"containerCpuRequestMilliCores\" resourceFieldRef: containerName: main resource: requests.cpu divisor: 1m - path: \"containerMemoryLimitBytes\" resourceFieldRef: containerName: main resource: limits.memory divisor: 1# pod 생성kubectl create -f downward-api-volume.yaml# pod 데이터 확인kubectl exec downward -- ls -al /etc/downward/kubectl exec downward -- cat /etc/downward/labelskubectl exec downward -- cat /etc/downward/annotations레이블과 어노테이션 업데이트 파드가 실행되는 동안 레이블, 어노테이션 값을 업데이트될 수 있다. downwardAPI 볼륨을 이용하는 경우에는 업데이트시에도 최신 데이터를 볼수 있다. 환경변수를 사용하는 경우에는 나중에 업데이트할 수 없다.볼륨 스펙에서 컨테이너 수준의 메타데이터 참조 리소스 제한 또는 요청(resourceFieldRef)과 같은 컨테이너 수준의 메타데이터를 노출하는 경우 리소스 필드를 참조하는 컨테이너의 이름을 필수로 지정해야 한다.(컨테이너가 하나인 파드에서도 필수 지정) 볼륨이 컨테이너가 아니라 파드 수준에서 정의되었지만, 리소스 제한은 컨테이너 기준이기 때문 환경변수를 사용하는 것보다 약간 더 복잡하지만 필요할 경우 한 컨테이너의 리소스 필드를 다른 컨테이너에 전달할 수 있는 장점이 있다. 환경변수로는 컨테이너 자신의 리소스 제한과 요청만 전달할 수 있다.spec: volumes: - name: downward downwardAPI: items: - path: \"containerCpuRequestMilliCores\" resourceFieldRef: containerName: main # 컨테이너 이름이 필수로 지정되어야 한다. resource: requests.cpu divisor: 1mDownward API 사용 시기 이해 Downward API를 사용하면 애플리케이션은 쿠버네티스에 독립적으로 유지할 수 있게 한다.(기존에 환경변수의 특정 데이터를 활용하고 있는 경우 유용할 수도 있다.) Downward API로 가져올 수 없는 다른 데이터들이 필요한 경우 쿠버네티스 API를 통해 가져와야 한다.8.2 쿠버네티스 API 서버와 통신하기 Downward API는 단지 파드 자체의 메타데이터와 모든 파드의 데이터 중 일부만 노출한다. 애플리케이션에서 클러스터에 정의된 다른 파드나 리소스에 대한 정보가 필요한 경우도 있는데 이 경우에는 쿠버네티스 API를 이용해야 한다.8.2.1 쿠버네티스 REST API 살펴보기# 쿠버네티스 클러스터 정보 조회kubectl cluster-infoKubernetes master is running at https://192.168.64.2:8443KubeDNS is running at https://192.168.64.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy# 호출 ( Forbidden 403 에러)curl https://192.168.64.2:8443 -kkubectl proxy로 API 서버 엑세스하기# proxy 실행kubectl proxy# local proxy 호출curl http://localhost:8001{ \"paths\": [ \"/api\", \"/api/v1\", # 대부분의 리소스 타입을 여기서 확인 \"/apis\", \"/apis/\", \"/apis/admissionregistration.k8s.io\", \"/apis/admissionregistration.k8s.io/v1\", \"/apis/admissionregistration.k8s.io/v1beta1\", \"/apis/apiextensions.k8s.io\", \"/apis/apiextensions.k8s.io/v1\", \"/apis/apiextensions.k8s.io/v1beta1\", \"/apis/apiregistration.k8s.io\", \"/apis/apiregistration.k8s.io/v1\", \"/apis/apiregistration.k8s.io/v1beta1\", \"/apis/apps\", \"/apis/apps/v1\", \"/apis/authentication.k8s.io\", \"/apis/authentication.k8s.io/v1\", \"/apis/authentication.k8s.io/v1beta1\", \"/apis/authorization.k8s.io\", \"/apis/authorization.k8s.io/v1\", \"/apis/authorization.k8s.io/v1beta1\", \"/apis/autoscaling\", \"/apis/autoscaling/v1\", \"/apis/autoscaling/v2beta1\", \"/apis/autoscaling/v2beta2\", \"/apis/batch\", \"/apis/batch/v1\", \"/apis/batch/v1beta1\", \"/apis/certificates.k8s.io\", \"/apis/certificates.k8s.io/v1beta1\", \"/apis/coordination.k8s.io\", \"/apis/coordination.k8s.io/v1\", \"/apis/coordination.k8s.io/v1beta1\", \"/apis/discovery.k8s.io\", \"/apis/discovery.k8s.io/v1beta1\", \"/apis/events.k8s.io\", \"/apis/events.k8s.io/v1beta1\", \"/apis/extensions\", \"/apis/extensions/v1beta1\", \"/apis/networking.k8s.io\", \"/apis/networking.k8s.io/v1\", \"/apis/networking.k8s.io/v1beta1\", \"/apis/node.k8s.io\", \"/apis/node.k8s.io/v1beta1\", \"/apis/policy\", \"/apis/policy/v1beta1\", \"/apis/rbac.authorization.k8s.io\", \"/apis/rbac.authorization.k8s.io/v1\", \"/apis/rbac.authorization.k8s.io/v1beta1\", \"/apis/scheduling.k8s.io\", \"/apis/scheduling.k8s.io/v1\", \"/apis/scheduling.k8s.io/v1beta1\", \"/apis/storage.k8s.io\", \"/apis/storage.k8s.io/v1\", \"/apis/storage.k8s.io/v1beta1\", \"/healthz\", \"/healthz/autoregister-completion\", \"/healthz/etcd\", \"/healthz/log\", \"/healthz/ping\", \"/healthz/poststarthook/apiservice-openapi-controller\", \"/healthz/poststarthook/apiservice-registration-controller\", \"/healthz/poststarthook/apiservice-status-available-controller\", \"/healthz/poststarthook/bootstrap-controller\", \"/healthz/poststarthook/crd-informer-synced\", \"/healthz/poststarthook/generic-apiserver-start-informers\", \"/healthz/poststarthook/kube-apiserver-autoregistration\", \"/healthz/poststarthook/rbac/bootstrap-roles\", \"/healthz/poststarthook/scheduling/bootstrap-system-priority-classes\", \"/healthz/poststarthook/start-apiextensions-controllers\", \"/healthz/poststarthook/start-apiextensions-informers\", \"/healthz/poststarthook/start-cluster-authentication-info-controller\", \"/healthz/poststarthook/start-kube-aggregator-informers\", \"/healthz/poststarthook/start-kube-apiserver-admission-initializer\", \"/livez\", \"/livez/autoregister-completion\", \"/livez/etcd\", \"/livez/log\", \"/livez/ping\", \"/livez/poststarthook/apiservice-openapi-controller\", \"/livez/poststarthook/apiservice-registration-controller\", \"/livez/poststarthook/apiservice-status-available-controller\", \"/livez/poststarthook/bootstrap-controller\", \"/livez/poststarthook/crd-informer-synced\", \"/livez/poststarthook/generic-apiserver-start-informers\", \"/livez/poststarthook/kube-apiserver-autoregistration\", \"/livez/poststarthook/rbac/bootstrap-roles\", \"/livez/poststarthook/scheduling/bootstrap-system-priority-classes\", \"/livez/poststarthook/start-apiextensions-controllers\", \"/livez/poststarthook/start-apiextensions-informers\", \"/livez/poststarthook/start-cluster-authentication-info-controller\", \"/livez/poststarthook/start-kube-aggregator-informers\", \"/livez/poststarthook/start-kube-apiserver-admission-initializer\", \"/logs\", \"/metrics\", \"/openapi/v2\", \"/readyz\", \"/readyz/autoregister-completion\", \"/readyz/etcd\", \"/readyz/log\", \"/readyz/ping\", \"/readyz/poststarthook/apiservice-openapi-controller\", \"/readyz/poststarthook/apiservice-registration-controller\", \"/readyz/poststarthook/apiservice-status-available-controller\", \"/readyz/poststarthook/bootstrap-controller\", \"/readyz/poststarthook/crd-informer-synced\", \"/readyz/poststarthook/generic-apiserver-start-informers\", \"/readyz/poststarthook/kube-apiserver-autoregistration\", \"/readyz/poststarthook/rbac/bootstrap-roles\", \"/readyz/poststarthook/scheduling/bootstrap-system-priority-classes\", \"/readyz/poststarthook/start-apiextensions-controllers\", \"/readyz/poststarthook/start-apiextensions-informers\", \"/readyz/poststarthook/start-cluster-authentication-info-controller\", \"/readyz/poststarthook/start-kube-aggregator-informers\", \"/readyz/poststarthook/start-kube-apiserver-admission-initializer\", \"/readyz/shutdown\", \"/version\" ]}배치 api 그룹의 REST 엔드포인트 살펴보기# apis/batch 엔드포인트 조회curl http://localhost:8001/apis/batch{ \"kind\": \"APIGroup\", \"apiVersion\": \"v1\", \"name\": \"batch\", \"versions\": [ # 제공되는 groupVersion 종류(batch API그룹은 2가지 버전을 갖는다는것을 의미함) { \"groupVersion\": \"batch/v1\", \"version\": \"v1\" }, { \"groupVersion\": \"batch/v1beta1\", \"version\": \"v1beta1\" } ], \"preferredVersion\": { # 클라이언트는 preferredVersion을 사용하는것을 권장한다는 의미 \"groupVersion\": \"batch/v1\", \"version\": \"v1\" }}# batch/v1 리소스 유형curl http://localhost:8001/apis/batch/v1{ \"kind\": \"APIResourceList\", # batch/v1 API 그룹 내의 API 리소스 목록 \"apiVersion\": \"v1\", \"groupVersion\": \"batch/v1\", \"resources\": [ # 이 그룹의 모든 리소스 유형을 담는 배열 { \"name\": \"jobs\", \"singularName\": \"\", \"namespaced\": true, # 네임스페이스에 속하는 리소스라는 의미 ( persistentvolumes같은 것들은 false) \"kind\": \"Job\", \"verbs\": [ # 이 리소스와 함꼐 사용할 수 있는 제공되는 API(단일, 여러개를 한꺼번에 추가 삭제할수 있고, 검색, 감시 업데이트 할수 있음) \"create\", \"delete\", \"deletecollection\", \"get\", \"list\", \"patch\", \"update\", \"watch\" ], \"categories\": [ \"all\" ], \"storageVersionHash\": \"mudhfqk/qZY=\" }, { \"name\": \"jobs/status\", # 리소스의 상태를 수정하기 위한 특수한 REST 엔드포인트 \"singularName\": \"\", \"namespaced\": true, \"kind\": \"Job\", \"verbs\": [ \"get\", \"patch\", \"update\" ] } ]}클러스터 안에 있는 모든 잡 인스턴스 나열하기 items 하위에 나열된다.# job 생성kubectl create -f my-job.yaml# 모든 잡 인스턴스 조회curl http://localhost:8001/apis/batch/v1/jobs이름별로 특정 잡 인스턴스 검색# api를 통한 job 조회curl http://localhost:8001/apis/batch/v1/namespaces/default/jobs/my-job# job 정보 조회kubectl get job my-job -o json# 위 2가지 결과는 같다.8.2.2 파드 내에서 API 서버와 통신 파드 내에서 통신하려면 API 서버의 위치를 찾아야하고, 서버로 인증을 해야 한다.API 서버와의 통신을 시도하기 위해 파드 실행kubectl create -f curl.yaml# 파드의 shell 접근kubectl exec -it curl bashAPI 서버 주소 찾기 실제 애플리케이션에서는 서버 인증서 확인을 절대로 건너뛰면 안된다. 중간자 공격(man-in-the-middle attack)으로 인증 토큰을 공격자에게 노출할수 있기 때문 중간자 공격(man-in-the-middle attack)은 통신을 연결하는 두 사람 사이에 중간자가 침입해 두 사람은 상대방에 연결했다고 생각하지만 실제로는 두 사람은 중간자에게 연결돼 있으며, 중간자가 한쪽에서 전달된 정보를 도청 및 조작한 후 다른쪽으로 전달하는 방식 # kubernetes 서비스kubectl get svc# KUBERNETES_SERVICE_HOST, KUBERNETES_SERVICE_PORT 변수를 통해 얻을수 있다.env | grep KUBERNETES_SERVICEKUBERNETES_SERVICE_PORT=443KUBERNETES_SERVICE_HOST=10.96.0.1KUBERNETES_SERVICE_PORT_HTTPS=443# FQDN을 이용한 방법(403)curl https://kubernetes -k서버의 아이덴티티 검증 각 컨테이너의 /var/run/secrets/kubernetes.io/serviceaccount/에 마운트되는 자동 생성된 default-token-xyz 라는 이름의 시크릿을 기준으로 처리할수 있다.# 컨테이너 내부에서 조회ls /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\tnamespace token# --cacert 옵션을 통해 인증서 지정 ( 여전히 403)curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt https://kubernetes# 환경변수 지정export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt# 재호출curl https://kubernetesAPI 서버로 인증 Authorization HTTP 헤더 내부에 토큰을 전달하여 토큰을 인증된 것으로 인식하여 적절한 응답을 받을 수 있다. 이런 방식을 통해 네임스페이스 내에 있는 모든 파드를 조회할 수 있다. 그러나 먼저 curl 파드가 어떤 네임스페이서에서 실행중인지 알아야 한다.TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)# 호출 ( 왜안되느지 모르겠지만 안됨 ㅠ)curl -H \"Authorization: Bearer $TOKEN\" https://kubernetes역할 기반 엑세스 제어(RBAC) 비활성화 RBAC가 활성화된 쿠버네티스 클러스터를 사용하는 경우 서비스 어카운트가 API 서버에 엑세스할 권한이 없을 수 있다.# 모든 서비스 어카운트에 클러스터 관리자 권한이 부여됨kubectl create clusterrolebinding permissive-binding --clusterrole=cluster-admin --group=system:serviceaccounts# 위험하고 프로덕션 클러스터에서는 해서는 안됨.파드가 실행중인 네임스페이스 얻기 시크릿 볼륨 디렉터리에 있는 3개의 파일을 사용해 파드와 동일한 네임스페이스에서 실행중인 모든 파드를 나열할 수 있다. GET 대신 PUT이나 PATCH를 전송해 업데이트도 가능하다.# namespace 지정 ( 실제로 default인데 아무값도 없다..)NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)# 호출curl -H \"Authorization: Bearer $TOKEN\" https://kubernetes/api/v/namespaces/$NS/pods파드가 쿠버네티스와 통신하는 방법 정리 애플리케이션 API 서버의 인증서가 인증기관으로부터 서명됐는지를 검증해야 하고, 인증 기관의 인증서는 ca.cart 파일에 있다. 애플리케이션은 token 파일의 내용을 Authorization HTTP 헤더에 Bearer 토큰으로 넣어 전송해서 자신을 인증해야 한다. namespace 파일은 파드의 네임스페이스 안에 있는 API 오브젝트의 CRUD 작업을 수행할 때 네임스페이스를 API 서버로 전달하는데 사용해야 한다.8.2.3 앰배서더 컨테이너를 이용한 API 서버 통신 간소화 보안을 유지하면서 통신을 훨씬 간단하게 만들수 있다.(kubectl proxy 활용)앰배서더 컨테이너 패턴 소개 메인 컨테이너 옆의 앰배서더 컨테이너에서 kubectl proxy를 실행하고 이를 통해 APi 서버와 통신할 수 있다. 메인 컨테이너의 애플리케이션은 HTTPS 대신 HTTP로 앰배서더에 연결하고 앰배서더 프록시가 APi 서버에 대한 HTtPS 연결을 처리하도록해 보안을 투명하게 관리할 수 있다. 파드의 모든 컨테이너는 동일한 루프백 네트워크 인터페이스를 공유하므로 애플리케이션은 localhost의 포트로 프록세이 엑세스할 수 있다.추가적인 앰배서더 컨테이너를 사용한 curl 파드 실행 kubectl proxy는 8001에 바인딩되며 curl localhost:8001에 접속할 수 있다. 외부 서비스에 연결하는 복잡성을 숨기고 메인 컨테이너에서 실행되는 애플리케이션을 단순화하기 위해 앰배서더 컨테이너를 사용하는 좋은 예시이다. 단점은 추가 프로세스를 실행해야 해서 리소스가 추가로 소비된다는 것이다.apiVersion: v1kind: Podmetadata: name: curl-with-ambassadorspec: containers: - name: main image: tutum/curl command: [\"sleep\", \"9999999\"] - name: ambassador # kubectl-proxy 이미지를 실행하는 앰배서더 컨테이너 image: luksa/kubectl-proxy:1.6.2# pod 생성kubectl create -f curl-with-ambassador.yaml# shell 접속kubectl exec -it curl-with-ambassador -c main bash# 호출curl localhost:80018.2.4 클라이언트 라이브러리를 사용해 API 서버와 통신 단순한 API 요청 이상을 수행하려면 쿠버네티스 API 클라이언트 라이브러리 중 하나를 사용하는 것이 좋다. https://kubernetes.io/ko/docs/reference/using-api/client-libraries/ ( 다양한 언어를 지원함.) 현재 SIG(Special Interest Group)에서 지원하는 API는 Go, Python, Java ,.net, JavaScript, Haskell이 있다. 이 라이브러리를 사용하는 경우 기본적으로 HTTPS를 지원하고, 인증을 관리하므로 앰배서더 컨테이너를 사용할 필요가 없다.Java 예제 ( https://github.com/kubernetes-client/java/ ) 책에 있는 Fabric8 java 클라이언트가 아니라 SIG에서 지원하는 Java 클라이언트 라이브러리를 첨부// list all podspublic class Example { public static void main(String[] args) throws IOException, ApiException{ ApiClient client = Config.defaultClient(); Configuration.setDefaultApiClient(client); CoreV1Api api = new CoreV1Api(); // 라이브러리 메소드 설계는 좀 이상하게 해놓은듯, 모두 null이면 arguments 정의를 안했어야지..) V1PodList list = api.listPodForAllNamespaces(null, null, null, null, null, null, null, null, null); for (V1Pod item : list.getItems()) { System.out.println(item.getMetadata().getName()); } }}// watch on namespace object:public class WatchExample { public static void main(String[] args) throws IOException, ApiException{ ApiClient client = Config.defaultClient(); Configuration.setDefaultApiClient(client); CoreV1Api api = new CoreV1Api(); Watch&lt;V1Namespace&gt; watch = Watch.createWatch( client, api.listNamespaceCall(null, null, null, null, null, 5, null, null, Boolean.TRUE, null, null), new TypeToken&lt;Watch.Response&lt;V1Namespace&gt;&gt;(){}.getType()); for (Watch.Response&lt;V1Namespace&gt; item : watch) { System.out.printf(\"%s : %s%n\", item.type, item.object.getMetadata().getName()); } }}스웨거와 Open API를 사용해 자신의 라이브러리 구축 쿠버네티스 API 서버는 /swaggerapi 에서 스웨거 API 정의를 공개하고 /swagger.json에서 OepnAPI 스펙을 공개하고 있다.스웨거 UI로 API 살펴보기 스웨거 UI로 REST API를 더 나은 방식으로 탐색할 수 있다. API 서버를 --enable-swagger-ui=true옵션으로 실행하면 활성화된다.# minikube swaggerUI = true 적용minikube start --extra-config=apiserver.Features.EnableSwaggerUI=true# kubectl proxykubectl proxy --port=8080 &amp;# 호출 ( swagger-ui 안됨..)http://localhost:8080/swagger-uihttp://localhost:8080/swagger.jsonhttp://192.168.64.2:8443/swagger-ui8.3 요약 파드의 이름, 네임스페이스 및 기타 메타데이터가 환경변수 또는 downward API 볼륨의 파일로 컨테이너 내부의 프로세스에 노출시키는 방법 CPU와 메모리의 요청 및 제한이 필요한 단위로 애플리케이션에 전달되는 방법 파드에서 downward API 볼륨을 사용해 파드가 살아 있는 동안 변경될 수 있는 최신 메타데이터를 얻는 방법(레이블과 어노테이션 등) kubectl proxy로 쿠버네티스 REST API를 탐색하는 방법 쿠버네티스에 정의된 다른 서비스와 같은 방식으로 파드가 환경변수 또는 DNS로 API 서버의 위치를 찾는 방법 파드에서 실행되는 애플리케이션이 API 서버와 통신하는지 검증하고, 자신을 인증하는 방법 앰배서더 컨테이너를 사용해 애플리케이션 내에서 API 서버와 훨씬 간단하게 통신하는 방법 클라이언트 라이브러리ㅏ로 쉽게 쿠버네티스와 상호작용할 수 있는 방법Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 7. 컨피그맵과 시크릿 : 애플리케이션 설정", "url": "/posts/devlog-platform-kubernetes-in-action7/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-22 17:34:00 +0900", "snippet": "7. 컨피그맵과 시크릿 : 애플리케이션 설정 빌드된 애플리케이션 자체에 포함되지 말아야 하는 설정(배포된 인스턴스별로 다른 세팅, 외부 시스템 엑세스를 위한 자격증명 등)이 필요하다.쿠버네티스는 이런 앱을 실행할때 설정 옵션을 전달할수 있는 방법을 제공한다.7.1 컨테이너화된 애플리케이션 설정 필요한 모든 설정을 앱에 포함하는 경우를 제외하면 일반...", "content": "7. 컨피그맵과 시크릿 : 애플리케이션 설정 빌드된 애플리케이션 자체에 포함되지 말아야 하는 설정(배포된 인스턴스별로 다른 세팅, 외부 시스템 엑세스를 위한 자격증명 등)이 필요하다.쿠버네티스는 이런 앱을 실행할때 설정 옵션을 전달할수 있는 방법을 제공한다.7.1 컨테이너화된 애플리케이션 설정 필요한 모든 설정을 앱에 포함하는 경우를 제외하면 일반적으로 명령줄 인수를 통해 앱에 필요한 설정을 넘겨주면서 앱을 실행시킨다. 옵션 목록이 커지면 이 옵션들을 파일에 저장하고 사용하기도 한다. 아니면 서버의 환경변수를 통해 전달하기도 한다. 도커 컨테이너를 기반으로 한다고 했을때 내부에 있는 설정 파일을 사용하는것은 약간 까다롭다.(컨테이너 이미지 안에 넣는것은 소스코드에 하드코딩하는것과 다를바가 없음) 인증 정보나 암호화 키와 같이 비밀로 유지해야 하는 내용을 포함하게 되면 해당 이미지에 접근할 수 있는 모든 사람은 누구나 정보를 볼수 있게 되버린다. 쿠버네티스에서는 설정 데이터를 최상위 레벨의 쿠버네티스 오브젝트에 저장하고 이를 기타 다른 리소스 정의와 마찬가지로 깃 저장소 혹은 다른 파일 기반 스토리지에 저장할 수 있다. 이러한 목적으로 쿠버네티스는 컨피그맵이라는 리소스를 제공한다. 대부분의 설정 옵션에서는 민감한 정보가 포함돼있지 않지만, 자격증명, 개인 암호화 키, 보안을 유지해야 하는 유사한 데이터들도 있다. 이를 위한 시크릿이라는 또다른 유형의 오브젝트도 제공한다.애플리케이션에 설정을 전달하는 방법 컨테이너에 명령줄 인수 전달 각 컨테이너를 위한 사용자 정의 환경변수 지정 특수한 유형의 볼륨을 통해 설정7.2 컨테이너에 명령줄 인자 전달7.2.1 도커에서 명령어와 인자 정의ENTRYPOINT와 CMD 이해 ENTRYPOINT는 컨테이너가 시작될때 호출될 명령어 정의 CMD는 ENTRYPOINT에 전달되는 인자를 정의shell과 exec 형식간의 차이점 shell 형식 : ENTRYPOINT node app.js exec 형식 : ENTRYPOINT [“node”, “app.js”]fortune 이미지에서 간격을 설정할 수 있도록 만들기 INTERVAL 변수를 추가하고 첫 번쨰 명령줄 인자의 값으로 초기화# fortuneloop.sh#!/bin/bashtrap \"exit\" SIGINTINTERVAL=$1 # 인자echo Configured to generate new fortune every $INTERVAL secondsmkdir -p /var/htdocswhile :do echo $(date) Writing fortune to /var/htdocs/index.html /usr/games/fortune &gt; /var/htdocs/index.html sleep $INTERVALdone# DockerfileFROM ubuntu:latestRUN apt-get update ; apt-get -y install fortuneADD fortuneloop.sh /bin/fortuneloop.shENTRYPOINT [\"/bin/fortuneloop.sh\"] # exec 형태의 ENTRYPOINT 명령CMD [\"10\"] # 실행할때 사용할 기본 인자7.2.2 쿠버네티스에서 명령과 인자 재정의 command와 args 필드로 맵핑된다. 이는 파드 생성 이후에는 업데이트 할수 없다.apiVersion: v1kind: Podmetadata: name: fortune2sspec: containers: - image: some/image command: [\"/bin\"/commend\"] args: [\"arg1\", \"arg2\", \"arg3\"]사용자 정의 주기로 fortune 파드 실행apiVersion: v1kind: Podmetadata: name: fortune2sspec: containers: - image: luksa/fortune:args args: [\"2\"] # 스크립트가 2초마다 새로운 fortune 메시지를 생성하도록 인자 지정 name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {}여러개의 인자 처리 args: - foo - bar - \"15\"숫자 인자에 대한 처리 문자열인 경우는 ““로 묶을 필요 없지만 숫자는 묶어야 한다.7.3 컨테이너의 환경변수 설정 파드의 각 컨테이너를 위한 환경변수 리스트를 지정할 수 있다. 컨테이너 명령이나 인자와 마찬가지로 환경변수 목록도 파드 생성 후에는 업데이트 불가 컨테이너별로 다른 환경변수 설정도 가능하다.환경변수로 fortune 이미지 안에 간격을 설정할 수 있도록 만들기 변수 초기화하는 부분을 제거하면 끝.#!/bin/bashtrap \"exit\" SIGINTecho Configured to generate new fortune every $INTERVAL secondsmkdir -p /var/htdocswhile :do echo $(date) Writing fortune to /var/htdocs/index.html /usr/games/fortune &gt; /var/htdocs/index.html sleep $INTERVALdone7.3.1 컨테이너 정의에 환경변수 지정 각 컨테이너를 설정할 때, 쿠버네티스는 자동으로 동일한 네임스페이스 안에 있는 각 서비스에 환경변수를 노출하는것은 알고 있어야 한다.(파드에 정의한 환경변수명이 같아 덮어써지는 부분이 있을수 있어서인듯)apiVersion: v1kind: Podmetadata: name: fortune-envspec: containers: - image: luksa/fortune:env env: # 환경변수 목록에 단일 변수 추가 - name: INTERVAL value: \"30\" name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {}7.3.2 변숫값에서 다른 환경변수 참조env: - name: FIRST_VAR value: \"foo\" - name: SECOND_VAR value: \"${FIRST_VAR}bar\" # foobar가 됨7.3.3 하드코딩된 환경변수의 단점 하드코딩된 값을 가져오는게 효율적일 수 있지만, 프로덕션과 개발환경의 파드를 별도로 정의해야할수도 있다는 의미 여러 환경에서 동일한 파드 정의를 재사용하려면 파드 정의에서 설정을 분리하는것이 좋다.7.4 컨피그맵으로 설정 분리 환경에 따라 다르거나 자주 변경되는 설정 옵션을 애플리케이션 소스 코드와 별도로 유지하는 것.7.4.1 컨피그맵 소개 쿠버네티스에서는 설정 옵션을 컨피그맵이라 부르는 별도 오브젝트로 분리할 수 있다. 짧은 문자열에서 전체 설정 파일에 이르는 값을 가지는 키/값 쌍으로 구성된 맵이다. 쿠버네티스 REST API 엔드포인트를 통해 컨피그맵의 내용을 직접 읽는것이 가능하지만, 반드시 필요한 경우가 아니라면 애플리케이션 내부는 쿠버네티스와 무관하도록 유지해야 한다. 파드는 컨피그맵을 이름으로 참조하여 모든 환경에서 동일한 파드 정의를 사용해 각 환경에서 서로 다른 설정을 사용할 수 있다.7.4.2 컨피그맵 생성apiVersion: v1kind: ConfigMapmetadata: name: fortune-configdata: sleep-interval: \"25\" kubectl create configmap로 생성 가능 컨피그맵 키는 유효한 DNS 서브도메인이어야 한다.(영숫자, 대시, 밑줄, 점만 포함 가능)# 컨피그맵 생성 (file)kubectl create -f fortune-config.yaml# 컨피그맵 생성 (literal)kubectl create configmap fortune-config --from-literal=sleep-interval=25# 컨피그맵 생성 (여러개의 literal)kubectl create configmap myconfigmap --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two# 컨피그맵 목록 조회kubectl get cm# 컨피그맵 정의 확인kubectl get configmap fortune-config -o yaml파일 내용으로 컨피그맵 생성 전체 설정 파일 같은 데이터를 통째로 컨피그맵에 저장할 수 있다.# 파일 통째로 컨피그맵 생성# - 파일 자체가 값으로 지정됨kubectl create configmap my-config --from-file=config-file.conf# 파일 통째로 저장하되 customkey 지정kubectl create configmap my-config --from-file=customkey=config-file.conf디렉터리에 있는 파일 컨피그맵 생성kubectl create configmap my-config --from-file=/path/to/dir#### 다양한 옵션 결합``` shkubectl create configmap my-config --from-file=foo.json # 단일 파일 --from-file=bar=foobar.conf # 사용자 정의 키 밑에 파일 저장 --from-file=config-opts/ # 전체 디렉터리 --from-literal=some=thing # 문자열 값7.4.3 컨피그맵 항목을 환경변수로 컨테이너에 전달 생성한 맵의 값을 어떻게 파드 안의 컨테이너를 전달할수 있는 방법을 살펴보자apiVersion: v1kind: Podmetadata: name: fortune-env-from-configmapspec: containers: - image: luksa/fortune:env env: - name: INTERVAL # INTERVAL 환경변수 설정 valueFrom: configMapKeyRef: # 컨피그맵 키에서 값을 가져와 초기화 name: fortune-config # 참조하는 컨피그맵 이름 key: sleep-interval # 컨피그맵에서 해당 키 아래에 저장된 값으로 변수 셋팅 name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {}파드에 존재하지 않는 컨피그맵 참조 존재하지 않는 컨피그맵을 참조하려고 하면 컨테이너는 시작하는데 실패한다. 하지만 참조하지 않는 다른 컨테이너는 정상적으로 시자된다. 누락된 컨피그맵을 생성하면 실패했던 컨테이너는 파드를 다시 만들지 않아도 자동으로 시작된다. 컨피그맵 참조를 옵션으로 표시할수도 있다. ( configMapKeyRef.optional : true로 지정), 이런 경우는 컨피그맵이 존재하지 않아도 컨테이너가 시작된다.7.4.4 컨피그맵의 모든 항목을 한번에 환경변수로 전달 컨피그맵의 모든 항목을 환경변수로 노출할 수 있는 방법을 제공한다. 접두사는 선택사항이고, 이를 생략하면 환경변수의 이름은 키와 동일한 이름을 갖게 된다. CONFIG_FOO-BAR는 대시를 가지고 있어 올바른 환경변수 이름이 아니기 때문에 이런 경우 환경변수로 변환되지 않는다.(올바른 형식이 아닌 경우 쿠버네티스에서 생략함)spec: containers: - image: some-image envForm: # env 대신 envForm 사용 - prefix: CONFIG_ # 모든 환경변수는 CONFIG_ prefix로 설정됨. configMapRef: # my-config-map 이름의 컨피그맵 참조 name: my-config-map...7.4.5 컨피그맵 항목을 명령줄 인자로 전달 pod.spec.containers.args 필드에서 직접 컨피그맵 항목을 참조할 수는 없지만 컨피그맵 항목을 환경변수로 먼저 초기화하고 이 벼수를 인자로 참조할 수 있다.apiVersion: v1kind: Podmetadata: name: fortune-args-from-configmapspec: containers: - image: luksa/fortune:args # 환경변수가 아닌 첫번째 인자에서 간격을 가져오는 이미지 env: - name: INTERVAL # 컨피그맵에서 환경변수 정의 valueFrom: configMapKeyRef: name: fortune-config key: sleep-interval args: [\"$(INTERVAL)\"] # 인자에 앞에서 정의한 환경변수를 지정 name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {}7.4.6 컨피그맵 볼륨을 사용해 컨피그맵 항목을 파일로 노출 컨피그맵은 모든 설정 파일을 포함한다. 컨피그맵 볼륨을 사용해서도 적용할 수가 있다. 컨피그맵 볼륨은 파일로 컨피그맵의 각 항목을 노출한다.컨피그맵 생성 nginx 서버가 클라이언트로 응답을 gzip 압축해서 보내는 니즈가 있다고 해보자. nginx gzip 압축 옵션을 활성화하고 이에 대한 컨피그맵을 생성해야 한다.# nginx gzip config 정의server { listen 80; server_name www.kubia-example.com; gzip on; # gzip 압축 활성화 gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; }}# configMap 삭제kubectl delete cm fortune-config# configMap dir로 생성kubectl create configmap fortune-config --from-file=configmap-files# configMap 확인kubectl get cm fortune-config -o yaml# 컨피그맵 내용apiVersion: v1data: my-nginx-config.conf: | # 파이프라인(|) 문자는 여러 줄의 문자열이 이어진다는 것을 의미한다. server { listen 80; server_name www.kubia-example.com; gzip on; gzip_types text/plain application/xml; location / { root /usr/share/nginx/html; index index.html index.htm; } } sleep-interval: | 25kind: ConfigMapmetadata: creationTimestamp: \"2020-08-23T07:55:47Z\" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:data: .: {} f:my-nginx-config.conf: {} f:sleep-interval: {} manager: kubectl operation: Update time: \"2020-08-23T07:55:47Z\" name: fortune-config namespace: default resourceVersion: \"3405\" selfLink: /api/v1/namespaces/default/configmaps/fortune-config uid: bc35aea1-618a-4961-a8ef-06136d71825c볼륨 안에 있는 컨피그맵 항목 사용 컨피그맵 항목에서 생성된 파일로 볼륨을 초기화하는 방법apiVersion: v1kind: Podmetadata: name: fortune-configmap-volumespec: containers: - image: luksa/fortune:env env: - name: INTERVAL valueFrom: configMapKeyRef: name: fortune-config key: sleep-interval name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: # 컨피그맵 볼륨 마운트 - name: html mountPath: /usr/share/nginx/html # 컨피그맵 볼륨을 마운트하는 컨테이너 위치 readOnly: true - name: config mountPath: /etc/nginx/conf.d readOnly: true - name: config mountPath: /tmp/whole-fortune-config-volume readOnly: true ports: - containerPort: 80 name: http protocol: TCP volumes: - name: html emptyDir: {} - name: config configMap: # 이 볼륨은 fortune-config 컨피그맵을 참조하는 볼륨 name: fortune-confignginx 서버가 마운트한 설정 파일을 사용하는지 확인# pod 생성kubectl create -f fortune-pod-configmap-volume.yaml# port forward kubectl port-forward fortune-configmap-volume 8080:80 &amp;# reqeuestcurl -H \"Accept-Encoding: gzip\" -I localhost:8080# 마운트된 컨피그맵 볼륨 내용 살펴보기kubectl exec fortune-configmap-volume -c web-server ls /etc/nginx/conf.d볼륨에 특정 컨피그맵 항목 노출 아래와 같이 설정하면 컨테이너 마운트 위치 ‘/etc/nginx/conf.d/’ 디렉터리에는 gzip.conf 파일만 포함된다.apiVersion: v1kind: Podmetadata: name: fortune-configmap-volume-with-itemsspec: containers: - image: luksa/fortune:env name: html-generator volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true - name: config mountPath: /etc/nginx/conf.d/ # 컨테이너 마운트 path readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html emptyDir: {} - name: config configMap: name: fortune-config items: # 볼륨에 포함할 항목을 조회해 선택 - key: my-nginx-config.conf # 해당 키 아래에 항목을 포함 path: gzip.conf # 항목 값이 지정된 파일에 저장디렉터리를 마운트할 때 디렉터리의 기존 파일을 숨기는 것에 대한 이해 리눅스 파일시스템을 비어 있지 않은 디렉터리에 마운트할떄 발생할 수 있는 문제로, 디렉터리는 마운트한 파일시스템에 있는 파일만 포함하고, 원래 있던 파일은 해당 파일시스템이 마운트돼 있는 동안 접근할 수 없게 된다. 중요한 파일을 포함하는 /etc 디렉터리에 볼륨을 마운트한다고 하면 /etc 디렉터리에 있어야 하는 모든 원본 파일이 더 이상 존재하지 않게 되어 전체 컨테이너가 손상될 수 있다.디렉터리 안에 다른 파일을 숨기지 않고 개별 컨피그맵 항목을 파일로 마운트 전체 볼륨을 마운트하는 대신 volumeMount에 subPath 속성으로 파일이나 디렉터리 하나를 볼륨에 마운트할 수 있다.spec: containers: - image: some/image volumeMounts: - name: myvolume mountPath: /etc/someconfig.conf # 디렉터리가 아닌 파일을 마운트 subPath: myconfig.conf # 전체 볼륨을 마운트하는 대신 myconfig.conf 항목만 마운트 subPath 속성은 모든 종류의 볼륨을 마운트할때 사용할 수 있다. 하지만 개별 파일을 마운트하는 이 방법은 파일 업데이트와 관ㄹ녀해 상대적으로 큰 결함을 가지고 있다.컨피그맵 볼륨 안에 있는 파일 권한 수정 기본적으로 컨피그맵 볼륨의 모든 파일 권한은 644(-rw-r-r–)로 설정된다. defaultMode 속성을 설정하여 변경이 가능하다. volumes: - name: html emptyDir: {} - name: config configMap: name: fortune-config defaultMode: 0660 # 모든 파일 권한을 660(-rw-rw---) 로 설정7.4.7 애플리케이션을 재시작하지 않고 설정 업데이트 컨피그맵을 사용해 볼륨으로 노출하면 파드를 다시 만들거나 컨테이너를 다시 시작할 필요 없이 설정을 업데이트할 수 있다. 컨피그맵을 업데이트한 후에 파일이 업데이트되기까지는 생각보다 오랜 시간이 걸릴수 있다.(최대 1분)컨피그맵 편집# configmap 수정kubectl edit configmap fortune-config# web-server 컨테이너 내 설정파일 확인kubectl exec fortune-configmap-volume -c web-server cat /etc/nginx/conf.d/my-nginx-config.conf설정을 다시 로드하기 위해 nginx에 신호 전달kubectl exec fortune-configmap-volume -c web-server -- nginx -s reload파일이 한꺼번에 업데이트되는 방법 이해 쿠버네티스 컨피그맵은 모든 파일이 한번에 업데이트된다. (심볼릭 링크 방식으로 동작하기 때문)# 컨테이너 디렉토리 확인kubectl exec -i -t fortune-configmap-volume -c web-server -- ls -al /etc/nginx/conf.d# 결과drwxrwxrwx 3 root root 4096 Aug 23 08:14 .drwxr-xr-x 3 root root 4096 Aug 14 00:37 ..drwxr-xr-x 2 root root 4096 Aug 23 08:14 ..2020_08_23_08_14_11.042426742lrwxrwxrwx 1 root root 31 Aug 23 08:14 ..data -&gt; ..2020_08_23_08_14_11.042426742lrwxrwxrwx 1 root root 27 Aug 23 08:14 my-nginx-config.conf -&gt; ..data/my-nginx-config.conflrwxrwxrwx 1 root root 21 Aug 23 08:14 sleep-interval -&gt; ..data/sleep-interval이미 존재하는 디렉터리에 파일만 마운트했을 때 업데이트가 되지 않는 것 이해하기 단일 파일만 컨테이너에 마운트한 경우 파일이 업데이트 되지 않는다.(단순 컨피그맵 + 볼륨 기능만 이용하는 경우에 한하여)컨피그맵 업데이트의 결과 이해하기 컨테이너의 가장 주용한 기능은 불변성(immutability)이다. 앱이 설정을 다시 읽는 기능을 지원하지 않는 경우에 심각한 문제가 발생한다. 컨피그맵을 변경한 이후 생성된 파드는 새로운 설정을 사용하지만 예전 파드는 계속 예전 설정을 사용하기 때문. 애플리케이션이 설정을 자동으로 다시 읽는 기능을 가지고 있지 않다면 이미 존재하는 컨피그맵을 수정하는것은 좋은 방법이 아니다.7.5 시크릿으로 민감한 데이터 컨테이너에 전달7.5.1 시크릿 소개 쿠버네티스는 민감한 정보를 보관하고 배포하기 위하여 시크릿이라는 오브젝트를 제공한다. 시크릿은 키-값 쌍을 가진 맵으로 컨피그맵과 매우 비슷하다. 컨피그맵과 마찬가지로 환경변수로 시크릿 항목을 컨테이너에 전달하거나 볼륨 파일로 노출시킬 수 있다. 시크릿에 접근해야 하는 파드가 실행되고 있는 노드에만 개별 시크릿을 배포해 시크릿을 안전하게 유지한다. 노드 자체적으로 시크릿을 항상 메모리에만 저장하게 되고 물리 저장소에는 기록되지 않도록 처리한다. 마스터 노드의 etcd에는 시크릿을 암호화되지 않는 형식으로 저장하므로 시크릿에 저장한 민감한 데이터를 보호하려면 마스터 노드를 보호해야 한다.(쿠버네티스 1.7 이하에서만, 그 이후로는 암호화된 형태로 저장함.)시크릿 vs 컨피그맵 어떤것을 사용해야 할지에 대한 기준 민감하지 않고, 일반 설정 데이터는 컨피그맵을 사용하라. 본질적으로 민감한 데이터는 시크릿을 사용해 키 아래에 보관하는 것이 필요하다. 민감한 데이터와 그렇지 않는 데이터를 모두 가지고 있는 경우 해당 파일은 시크릿 안에 저장해야 한다.7.5.2 기반 토큰 시크릿 모든 파드에는 sercret 볼륨이 자동으로 연결되어 있다.Containers: # default-token (Secret) 마운트 Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-xkr6d (ro)Volumes: default-token-xkr6d: Type: Secret (a volume populated by a Secret) SecretName: default-token-xkr6d Optional: false# pod 정보 조회kubectl describe po kubia-8pw7z# 시크릿 리소스 조회kubectl get secrets# 시크릿 정보 조회kubectl describe secrets 시크릿이 갖고 있는 3가지 항목(ca.crt, namespace, token)은 파드 안에서 쿠버네티스 API 서버와 통신할 때 필요한 것이다. 기본적으로 default-token 시크릿은 모든 컨테이너에 마운트된다. 파드 스펙 안에 auto mountService-AccountToken 필드 값을 false로 지정하거나 파드가 사용하는 서비스 어카운트를 false로 지정해 비활성화할 수 있다.7.5.3 시크릿 생성# 인증서와 개인키 생성openssl genrsa -out https.key 2048openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj /CN=www.kubia-example.com# secret 생성 ( fortune-https 이름을 가진 generic 시크릿을 생성 )kubectl create secret generic fortune-https --from-file=https.key --from-file=https.cert --from-file=foo시크릿의 유형 docker-registry ( 도커 레지스트리 사용을 위한) tls ( TLS 통신을 위한) generic (일반적인 상황)7.5.4 컨피그맵과 시크릿 비교 시크릿 항목의 내용은 base64 인코딩 문자열로 표시되고, 컨피그맵의 내용은 일반 텍스트로 표시된다.# 시크릿 조회kubectl get secret fortune-https -o yaml# 컨피그맵 조회kubectl get configmap fortune-config -o yaml바이너리 데이터 시크릿 사용 base64 인코딩을 사용하는 이유는 일반 텍스트 뿐만 아니라 바이너리 값도 담을 수 있기 때문이다. 민감하지 않은 데이터도 시크릿을 사용할수 있지만 시크릿의 최대 크기는 1MB로 제한된다.stringData 필드 소개 쿠버네티스는 시크릿의 값을 stringData 필드로 설정할 수 있게 해준다. stringData 필드는 쓰기 전용이다.(값을 설정할 때만 사용 가능)apiVersion: v1kind: SecretstringData # 바이너리 데이터가 아닌 시크릿 데이터에 사용할 수 있다. foo: plain text # \"plain text\"는 base64 인코딩되지 않는다.data: https.cert: ... https.key: ...파드에서 시크릿 항목 읽기 secret 볼륨을 통해 시크릿을 컨테이너에 노출하면, 시크릿 항목의 값이 일반 텍스트인지 바이너리 데이터인지에 관계 없이 실제 형식으로 디코딩돼 파일에 기록된다.7.5.5 파드에서 시크릿 사용 configmap의 nignx 설정에 https 인증서와 개인키를 추가해주고 시크릿을 파드에 마운트 참고 : 시크릿도 defaultMode 속성을 통해 볼륨에 노출된 파일 권한을 지정할 수 있음fortune-https 시크릿을 파드에 마운트apiVersion: v1kind: Podmetadata: name: fortune-httpsspec: containers: - image: luksa/fortune:env name: html-generator env: - name: INTERVAL valueFrom: configMapKeyRef: name: fortune-config key: sleep-interval volumeMounts: - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true - name: config mountPath: /etc/nginx/conf.d readOnly: true - name: certs # nginx 서버가 인증서와 키를 /etc/nginx/certs에서 읽을수 있도록 해당 위치에 secret 마운트 mountPath: /etc/nginx/certs/ readOnly: true ports: - containerPort: 80 - containerPort: 443 volumes: - name: html emptyDir: {} - name: config configMap: name: fortune-config items: - key: my-nginx-config.conf path: https.conf - name: certs # 시크릿 볼륨 정의 secret: secretName: fortune-httpsnginx가 시크릿의 인증서와 키를 사용하는지 테스트# https 파드 생성kubectl create -f fortune-pod-https.yaml# port forwardkubectl port-forward fortune-https 8443:443 &amp;# curl callcurl https://localhost:8443 -k -v시크릿 볼륨을 메모리에 저장하는 이유 시크릿 볼륨은 시크릿 파일을 저장하는 데 인메모리 파일시스템(tmpfs)을 사용한다. tmpfs를 사용하는 이유는 민감한 데이터를 노출시킬 수도 있는 디스크에 저장하지 않기 위해서이다.# 컨테이너 확인kubectl exec fortune-https -c web-server -- mount | grep certstmpfs on /etc/nginx/certs type tmpfs (ro,relatime)환경변수로 시크릿 항목 노출 configMapKeyRef 대신 secretKeyRef를 사용해 컨피그맵과 유사한 방식으로 참조가 가능하다. 시크릿을 환경변수로 노출할 수 있게 해주기는 하지만, 이 기능 사용은 권장하지는 않는다. 앱에서 일반적으로 오류 보고서에 환경변수를 기록하거나 로그에 환경변수를 남겨 의도치 않게 시크릿이 노출될 가능성이 있다. 또한 자식 프로세스는 부모 프로세스의 모든 환경변수를 상속받는데, 앱이 타사(third-party) 바이너리를 실행할 경우 시크릿 데이터를 어떻게 사용하는지 알 수 있는 방법이 없다. env: # 변수는 시크릿 항목에서 설정 - name: FOO_SECRET valueFrom: \"30\" secretKeyRef: name: fortune-https # 시크릿 이름 지정 key: foo # 시크릿의 키 이름7.5.6 이미지를 가져올 때 사용하는 시크릿 이해 쿠버네티스에서 자격증명을 전달하는것이 필요할때가 있다.(프라이빗 컨테이너 이미지 레지스트리)도커 허브에서 프라이빗 이미지 사용 도커 레지스트리 자격증명을 가진 시크릿 생성 파드 매니페스트 안에 imagePullSecrets 필드에 해당 시크릿 참조도커 레지스트리 인증을 위한 시크릿 생성# 도커 레지스트리용 시크릿 생성kubectl create secret docker-registry mydockerhubsecret --docker-username=myusername --docker-password=mypassword --docker-email=my.email@providercom파드 정의에서 도커 레지스트리 시크릿 사용apiVersion: v1kind: Podmetadata: name: private-podspec: imagePullSecrets: # 프라이빗 이미지 레지스트리에서 이미지를 가져올 수 있도록 설정 - name: mydockerhubsecret containers: - image: username/private:tag name: main모든 파드에서 이미지를 가져올 때 사용할 시크릿을 모두 지정할 필요는 없다. 이미지를 가져올 때 사용할 시크릿을 서비스어카운트에 추가해 모든 파드에 자동으로 추가되도록 할수도 있다.(12장)7.6 요약 컨테이너 이미지에 정의된 기본 명령어를 파드 정의 안에 재정의 주 컨테이너 프로세스에 명령줄 인자 전달 컨테이너에서 사용할 환경변수 설정 파드 사양에서 설정을 분리해 컨피그맵 안에 넣기 민감한 데이터를 시크릿 안에 넣고 컨테이너에 안전하게 전달 docker-registry 시크릿을 만들고 프라이빗 이미지 레지스트리에서 이미지를 가져올 때 사용Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 6. 볼륨 : 컨테이너에 디스크 스토리지 연결", "url": "/posts/devlog-platform-kubernetes-in-action6/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-17 17:34:00 +0900", "snippet": "6. 볼륨 : 컨테이너에 디스크 스토리지 연결 파드는 내부에 프로세스가 실행되고 CPU, RAM, 네트워크 인터페이스 등의 리소스를 공유한다. 하지만 디스크는 공유되지 않는다. 파드 내부의 각 컨테이너는 고유하게 분리된 파일 시스템을 가지기 때문이다.(컨테이너 이미지로부터 제공되는) 새로 시작한 컨테이너는 이전에 실행했던 컨테이너에 쓰여진 파일 시...", "content": "6. 볼륨 : 컨테이너에 디스크 스토리지 연결 파드는 내부에 프로세스가 실행되고 CPU, RAM, 네트워크 인터페이스 등의 리소스를 공유한다. 하지만 디스크는 공유되지 않는다. 파드 내부의 각 컨테이너는 고유하게 분리된 파일 시스템을 가지기 때문이다.(컨테이너 이미지로부터 제공되는) 새로 시작한 컨테이너는 이전에 실행했던 컨테이너에 쓰여진 파일 시스템의 어떤 것도 볼수 없다. 전체 파일 시스템이 유지될 필요는 없지만 실제 데이터를 가진 디렉터리를 보존하고 싶을 수 있음. 이를 위해 쿠버네티스는 스토리지 볼륨으로 기능을 제공한다. 볼륨은 파드와 같은 최상위 리소스는 아니지만 파드의 일부분으로 정의되며 파드와 일반적으로는 동일한 라이프 사이클을 가진다.6.1 볼륨 소개 쿠버네티스 볼륨은 파드의 구성 요소로 컨테이너와 동일하게파드 스펙에서 정의된다. 볼륨은 독립적인 쿠버네티스 오브젝트가 아니므로 자체적으로 생성, 삭제될 수 없다. 접근하려는 컨테이너에서 각각 마운트 되어야 한다.6.1.1 볼륨 예제 볼륨 2개를 파드에 추가하고, 3개의 컨테이너 내부의 적절한 경로에 마운트 리눅스에서 파일시스템을 파일 트리의 임의 경로에 마운트할 수 있는 방식을 이용 같은 볼륨을 2개의 컨테이너에 마운트하면 컨테이너는 동일한 파일로 동작 가능하다. 마운트되지 않은 볼륨이 같은 파드안에 있더라도 접근할수 없고, 접근하려면 volumeMount를 컨테이너 스펙에 정의해야 한다.6.1.2 사용 가능한 볼륨 유형 소개 emptyDir : 일시적인 데이터를 저장하는 데 사용되는 간단한 빈 디렉터리 hostPath : 워커 노드의 파일시스템을 파드의 디렉터리로 마운트 gitRepo : 깃 리포지터리의 콘텐츠를 체크아웃해 초기화한 볼륨 nft : NFS 공유를 파드에 마운트 gcePersistentDisk, awsElasticBlockStore, azureDisk 등 : 클라우드 제공자의 전용 스토리지 마운트 cinder, cephfs, iscsi, flocker, glusterfs, quobyte, rdb, flexVolume, vsphereVolume, photonPersistentDisk, scaleIO : 다른 유형의 네트워크 스토리지 마운트 configMap, secret, downwardAPI : 쿠버네티스 리소스나 클러스터 정보를 파드에 노출하는 데 사용되는 특별한 유형의 볼륨 persistentVolumeClaim : 사전 혹은 동적으로 프로비저닝된 퍼시스턴트 스토리지를 사용하는 방법6.2 볼륨을 사용한 컨테이너간 데이터 공유6.2.1 emptyDir 볼륨 사용 빈 디렉터리로 시작되며, 볼륨의 라이프사이클이 파드에 묶여 있으므로 파드가 삭제되면 볼륨의 콘텐츠도 같이 사라진다. 컨테이너에서 가용한 메모리에 넣기에 큰 데이터 세트의 정렬 작업을 수행하는 것과 같은 임시 데이터를 디스크에 쓰는 목적인 경우 사용할 수 있다.파드에 emptyDir 볼륨 사용(동일한 볼륨을 공유하는 컨테이너 2개가 있는 파드)apiVersion: v1kind: Podmetadata: name: fortunespec: containers: - image: luksa/fortune name: html-generator # 첫번째 컨테이너 html-generator volumeMounts: # html이라는 이름의 볼륨을 컨테이너 /var/htdocs에 마운트 - name: html mountPath: /var/htdocs - image: nginx:alpine name: web-server # 두번째 컨테이너 web-server volumeMounts: # html이라는 이름의 볼륨을 컨테이너 /usr/share/nginx/html에 마운트 - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: # html이라는 단일 emptyDir 볼륨을 위의 컨테이너 2개에 마운트하기 위한 정의 - name: html emptyDir: {}실행중인 파드 보기# pod 조회kubectl get po# fortune pod 포트 포워딩kubectl port-forward fortune 8080:80# requestcurl http://localhost:8080# html-generator 컨테이너 내부 확인kubectl exec -i -t fortune -c html-generator -- /bin/bash# web-server 컨테이너 index.html 파일 확인kubectl exec -i -t fortune -c web-server -- cat /usr/share/nginx/html/index.htmlemptyDir을 사용하기 위한 매체 지정하기 워커 노드의 실제 디스크에 생성하는 경우 노드 디스크가 어떤 유형인지에 따라 성능이 결정될수 있음. 쿠버네티스에 emptyDir을 디스크가 아닌 메모리를 사용하는 tmpfs 파일시스템으로 생성하도록 요청할수도 있음. volumes: - name: html emptyDir: medium: Memory # 이 emptyDir의 파일들은 메모리에 저장된다.6.2.2 깃 리포지터리를 볼륨으로 사용하기 gitRepo 볼륨은 emptyDir base이고, 파드가 시작되면 깃 리포를 복제하여 데이터를 채운다. 볼륨이 생성된 후에 참조하는 리포지터리와 동기화되지는 않는다. 파드가 삭제되고 새 파드가 생성되면 그 파드는 최신 커밋을 포함하게 된다. 최신 변경사항을 동기화하고 싶은 경우 github web hook 같은 것을 이용하면 될듯복제된 깃 리포지터리 파일을 서비스하는 웹 서버 실행하기apiVersion: v1kind: Podmetadata: name: gitrepo-volume-podspec: containers: - image: nginx:alpine name: web-server volumeMounts: - name: html mountPath: /usr/share/nginx/html readOnly: true ports: - containerPort: 80 protocol: TCP volumes: - name: html gitRepo: # gitRepo 정의하여 볼륨 정의 가능 repository: https://github.com/luksa/kubia-website-example.git revision: master directory: .gitRepo 볼륨에 대한 정리 gitRepo 볼륨은 emptyDir 볼륨과 유사하게 기본적으로 볼륨을 포함하는 파드를 위해 특별히 생성되고 독점적으로 사용되는 전용 디렉터리이다. 파드가 삭제되면 볼륨과 콘텐츠는 모두 삭제된다.6.3 워커 노드 파일시스템의 파일 접근 대부분의 파드는 호스트 노드를 인식하지 못하므로 노드의 파일 시스템에 있는 어떤 파일에도 접근하면 안 된다. 하지만 특정 시스템 레벨의 파드(데몬셋과 같은)는 이런 파일 시스템 접근이 필요할 수 있다. 쿠버네티스는 hostPath 볼륨으로 이 기능을 지원한다.6.3.1 hostPath 볼륨 소개 hostPath 볼륨은 노드 파일 시스템의 특정 파일이나 디렉터리를 가리킨다.(퍼시스턴트 스토리지) gitRepo나 emptyDir 볼륨의 콘텐츠는 파드가 종료되면 삭제되지만, hostPath 볼륨의 콘텐츠는 삭제되지 않는다. 이전 파드와 동일한 노드에서 새롭게 스케줄링 되는 새로운 파드는 이전 파드가 남긴 모든 항목을 볼 수 있다. 다만 데이터베이스의 데이터 디렉터리를 지정할 위치로 사용하기에는 적절하지 않다.(db pod은 다른 노드로 스케줄링 될 가능성이 있으므로) hostPath 볼륨은 파드가 어떤 녿에 스케줄되느냐에 따라 민감하기 때문에 일반적인 파드에서는 사용하지 않는것이 좋다.6.3.2 hostPath 볼륨을 사용하는 시스템 파드 검사하기 노드의 로그 파일이나 kubeconfig(쿠버네티스 구성 파일), CA 인증서를 접근하기 위한 데이터들을 hostPath로 구성되어있음 노드의 시스템 파일에 읽기/쓰기를 하는 경우에만 hostPath 볼륨을 사용해야 한다.(여러 파드에 걸쳐 데이터를 유지하기 위해서는 사용 금지)# kube-system 네임스페이스의 시스템 파드 조회kubectl get pods --namespace kube-system# 시스템 파드 Path 확인kubectl describe po kube-controller-manager-minikube --namespace kube-system# path 내용들ca-certs: Type: HostPath (bare host directory volume) Path: /etc/ssl/certs HostPathType: DirectoryOrCreate flexvolume-dir: Type: HostPath (bare host directory volume) Path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec HostPathType: DirectoryOrCreate k8s-certs: Type: HostPath (bare host directory volume) Path: /var/lib/minikube/certs HostPathType: DirectoryOrCreate kubeconfig: Type: HostPath (bare host directory volume) Path: /etc/kubernetes/controller-manager.conf HostPathType: FileOrCreate6.4 퍼시스턴트 스토리지 사용 파드에서 실행중인 애플리케이션이 디스크에 데이터를 유지해야 하고 파드가 다른 노드로 재스케줄링된 경우에도 동일한 데이터를 사용해야 하는 경우를 NAS 같은 유형에 데이터를 저장해야 한다. 이를 위한 방법을 쿠버네티스가 제공한다. minikube로 연습하는 경우에는 hostPath 볼륨으로 사용하면 된다.6.4.1 GCE 퍼시스턴트 디스크를 파드 볼륨에 사용하기apiVersion: v1kind: Podmetadata: name: mongodbspec: volumes: - name: mongodb-data gcePersistentDisk: # 볼륨의 유형은 GCE 퍼시스턴트 디스크 pdName: mongodb fsType: ext4 # 리눅스 파일시스템 유형 containers: - image: mongo name: mongodb volumeMounts: - name: mongodb-data mountPath: /data/db # 컨테이너 내 마운트 되는 path ports: - containerPort: 27017 protocol: TCP6.4.2 기반 퍼시스턴트 스토리지로 다른 유형의 볼륨 사용하기 awsElasticBlockStore, azureFile이나 azureDisk 볼륨을 사용할 수 있음.AWS Elastic Block Store 볼륨 사용apiVersion: v1kind: Podmetadata: name: mongodb-awsspec: volumes: - name: mongodb-data awsElasticBlockStore: # awsElasticBlockStore volumeID: my-volume fsType: ext4...NFS 볼륨 사용apiVersion: v1kind: Podmetadata: name: mongodb-nfsspec: volumes: - name: mongodb-data nfs: server: 1.2.3.4 path: /some/path...다른 스토리지 기술 사용 쿠버네티스는 왠만한 모든 기술의 다양한 스토리지를 지원한다.6.5 기반 스토리지 기술과 파드 분리 쿠버네티스에 앱을 배포하는 개발자는 기저에 어떤 종류의 스토리지 기술이 사용되는 알 필요 없어야 하고, 동일한 방식으로 파드를 실행하기 위해 어떤 유형의 물리 서버가 사용되는지 알 필요 없어야 한다.(이상적) 파드의 볼륨이 실제 기반 인프라스르럭처를 참조한다는 것은 쿠버네티스가 추구하는 바가 아님 인프라 스트럭처 관련 정보를 파드 정의에 포함한다는 것은 파드 정의가 특정 쿠버네티스 클러슽에 밀접하게 연결됨을 의미한다. 동일한 파드 정의를 다른 클러스터에서는 사용할 수 없다.6.5.1 퍼시스턴트볼륨(PV, PersistentVolume)과 퍼시스턴트볼륨클레임(PVC, PersistentVolumeClaim) 인프라스트럭처의 세부 사항을 처리하지 않고 앱이 스토리지를 요청할 수 있도록 하기 위한 리소스 유형 관리자는 네트워크 스토리지 유형을 서정하고, PV 디스크립터를 게시하여 퍼시스턴트볼륨을 생성한다. 사용자는 퍼시스턴트볼륨클레임(PVC)을 생성하면, 쿠버네티스가 적당한 크기와 접근모드의 PV를 찾아서 PVC를 PV에 바인딩시킨다. 사용자는 이제 PVC를 참조하는 볼륨을 가진 파드를 생성한다.6.5.2 퍼시스턴트볼륨 생성 퍼시스턴트볼륨을 생성할 때 동시에 단일 또는 다수 노드에 읽기나 쓰기가 가능한지 여부 등을 지정해야 하고, 퍼시스턴트볼륨 해제시 어떤 동작을 해야 할지 정의해야 한다. 퍼시스턴트 볼륨을 지원하는 실제 스토리지의 유형, 위치, 그 밖의 속성 정보를 지정 퍼시스턴트 볼륨은 특정 네임스페이스에 속하지 않고, 노드와 같은 수준의 클러스터 리소스이다.정의apiVersion: v1kind: PersistentVolumemetadata: name: mongodb-pvspec: capacity: storage: 1Gi accessModes: - ReadWriteOnce # 단일 클라이언트의 읽기/쓰기용으로 마운트 - ReadOnlyMany # 여러 클라이언트의 읽기 전용으로 마운트 persistentVolumeReclaimPolicy: Retain # 클레임이 해제된 후 퍼시스턴트볼륨을 유지한다. hostPath: # ohstPath 볼륨 (minikube) path: /tmp/mongodbpersistentVolumeReclaimPolicy 현재 NFS 및 HostPath만 재활용을 지원한다. AWS EBS, GCE PD, Azure Disk 및 Cinder 볼륨은 삭제를 지원한다. Retain(보존) – 수동 반환 Recycle(재활용) – 기본 스크럽 (rm -rf /thevolume/*) Delete(삭제) – AWS EBS, GCE PD, Azure Disk 또는 OpenStack Cinder 볼륨과 같은 관련 스토리지 자산이 삭제됨생성 및 조회# hostPath PV 생성kubectl create -f mongodb-pv-hostpath.yaml# pv 조회kubectl get pv6.5.3 퍼시스턴트볼륨클레임 생성을 통한 퍼시스턴트볼륨 요청 파드가 재스케줄링되더라도 동일한 퍼시스턴트볼륨클레임이 사용 가능한 상태로 유지되기를 원하므로 퍼시스턴트 볼륨에 대한 클레임은 파드를 생성하는 것과 별개의 프로세스이다.퍼시스턴트볼륨클레임 생성하기 퍼시스턴트볼륨클레임이 생성되자마자 쿠버네티스는 적절한 퍼시스턴트볼륨을 찾고 클레임에 바인딩한다. 용량은 퍼시스턴트볼륨클레임의 요청을 수용할만큼 충분히 커야하고, 볼륨 접근 모드는 클레임에서 요청한 접근모드를 포함하는 상태여야 바인딩이 이루어진다.apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mongodb-pvcspec: resources: requests: # 1GiB의 스토리지를 요청 storage: 1Gi accessModes: - ReadWriteOnce # 단일 클라이언트를 지원하는 읽기/쓰기 스토리지 storageClassName: \"\"퍼시스턴트볼륨클레임 조회하기# 퍼시스턴트볼륨클레임 생성kubectl create -f mongodb-pvc.yaml# 퍼시스턴트볼륨클레임 조회kubectl get pvc퍼시스턴트볼륨 접근모드 RWO(ReadWriteOnce) : 단일 노드만이 읽기/쓰기용으로 볼륨을 마운트 ROX(ReadOnlyMany) : 다수 노드가 읽읽기용으로 볼륨을 마운트 RWX(ReadWriteMany) : 다수 노드가 읽기/쓰기용으로 볼륨을 마운트6.5.4 파드에서 퍼시스턴트볼륨클레임 사용하기apiVersion: v1kind: Podmetadata: name: mongodbspec: containers: - image: mongo name: mongodb volumeMounts: - name: mongodb-data mountPath: /data/db ports: - containerPort: 27017 protocol: TCP volumes: - name: mongodb-data # 파드 볼륨에서 이름으로 퍼시스턴트볼륨클레임을 참조 persistentVolumeClaim: claimName: mongodb-pvcmongodb test# 퍼시스턴트클레임을 이용하는 mongodb pod 생성kubectl create -f mongodb-pod-pvc.yaml# mongodb 셸 접속kubectl exec -it mongodb mongo# mongodb db 변경use mystore# 데이터 insertdb.foo.insert({name:'foo'})# 데이터 조회db.foo.find()6.5.5 퍼시스턴트볼륨과 퍼시스턴트볼륨클레임 사용의 장점 이해하기 GCE 퍼시스터턴트 디스크를 직접 사용하는 경우와 PVC + PV를 사용하는 경우 비교 개발자가 직접 인프라스트럭처에서 스토리지를 가져오는 방식보다는 PV + PVC을 통해간접적으로 가져오는 방식이 더 간단하다(인프라스트럭처를 몰라도됨) 또한 동일한 파드와 클레임 매니페스트는 인프라스트럭처와는 관련된 어떤것도 참조하지 않으므로 다른 쿠버네티스 클러스터에서도 그대로 사용할 수 있다. “클레임은 x만큼의 스토리지가 필요하고 한 번에 하나의 클라이언트에서 읽기와 쓰기를 할 수 있어야 한다”만 명시한다.6.5.6 퍼시스턴트볼륨 재사용# mongodb pod 삭제kubectl delete pod mongodb# pvc 삭제kubectl delete pvc mongodb-pvc#pvc, pod 재생성 ( 이 경우 pvc는 바로 volume을 할당받지 못하고 Pending 상태가 된다.)kubectl create -f mongodb-pvc.yamlkubectl create -f mongodb-pod-pvc.yaml# pvc 조회kubectl get pvc# pv 조회 ( 퍼시스턴트볼륨의 상태가 Released로 표시되고 Available이 아니다. 그 이유는 이미 볼륨을 사용했기 떄문에 데이터를 가지고 있어서 새로운 클레임을 바인딩할 수 없는 상태)kubectl get pv퍼시스턴트볼륨을 수동으로 다시 클레임하기 persistentVolumeClaimPolicy를 Retain으로 설정하면 퍼시스턴트볼륨클레임이 해제되더라도 데이터가 남아있으면 상태가 Available로 풀리지 않는다.퍼시스턴트볼륨을 자동으로 다시 클레임하기 다른 리클레임 정책인 Recycle과 Delete가 있는데 Recycle은 볼륨의 콘텐츠를 삭제하고 다시 클레임될수 있도록 만드는 옵션이다. Delete 정책은 쿠버네티스에서 퍼시스턴트볼륨 오브젝트와 외부 인프라(예: AWS EBS, GCE PD, Azure Disk 또는 Cinder 볼륨)의 관련 스토리지 자산을 모두 삭제한다. Recycle과 Delete의 차이는 pvc가 삭제될때 pv까지 삭제하느냐 안하느냐에 대한 차이가 있음(Delete는 pvc를 삭제하면 pv까지 삭제함)6.6 퍼시스턴트볼륨의 동적 프로비저닝6.6.2 퍼시스턴트볼륨클레임에서 스토리지 클래스 요청하기특정 스토리지클래스를 요청하는 pvc 정의 클레임을 생성하면 fast 스토리지클래스 리소스에 참조된 프로비저너가 퍼시스턴트볼륨을 생성한다. PVC에서 존재하지 않는 스토리지클래스를 참조하면 PV 프로비저닝은 실패한다. kubectl describe 로 확인해보면 ProvisioningFailed 이벤트 표시됨.apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mongodb-pvcspec: storageClassName: fast # PVC는 사용자 정의 스토리지 클래스를 요청 resources: requests: storage: 100Mi accessModes: - ReadWriteOnce스토리지클래스 정의apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: fastprovisioner: k8s.io/minikube-hostpathparameters: type: pd-ssd동적 프로비저닝된 PV와 생성된 PVC 조회 이렇게 생성된 PV는 리클레임 정책 Delete을 가지며, PVC가 삭제되면 PV도 삭제된다.# 스토리지클래스 생성kubectl create -f storageclass-fast-hostpath.yaml# PVC 생성kubectl create -f mongodb-pvc-dp.yaml# pvc 조회kubectl get pvcNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEmongodb-pvc Bound pvc-b767134f-218a-48cc-b1a4-4787a661fd09 100Mi RWO fast 16m# pv 조회kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-b767134f-218a-48cc-b1a4-4787a661fd09 100Mi RWO Delete Bound default/mongodb-pvc fast 16m스토리지 클래스 사용하는 법 이해하기 스토리지클래스의 좋은 점은 클레임 이름으로 이를 참조한다는 사실. 그래서 다른 클러스터간 스토리지클래스 이름을 동일하게 사용한다면 PVC 정의를 다른 클러스터로 이식도 가능하다.6.6.3 스토리지 클래스를 지정하지 않는 동적 프로비저닝# 스토리지 클래스 조회kubectl get scNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEfast k8s.io/minikube-hostpath Delete Immediate false 18mstandard (default) k8s.io/minikube-hostpath Delete Immediate false 26h# 기본 스토리지 클래스 확인kubectl get sc standard -o yaml스토리지 클래스를 지정하지 않고 퍼시스턴트볼륨클레임 생성하기apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mongodb-pvc2spec: resources: requests: storage: 100Mi accessModes: - ReadWriteOnce storageClassName 속성을 지정하지 않고 PVC를 생성하면 구글 쿠버네티스 엔진에서는 pd-standard 유형의 퍼시스턴트 디스크가 프로비저닝된다.# 스토리지 클래스를 지정하지 않고 pvc 생성kubectl create -f mongodb-pvc-dp-nostorageclass.yaml# pvc 확인kubectl get pvc# pv 확인kubectl get pv퍼시스턴트볼륨클레임을 미리 프로비저닝된 퍼시스턴트볼륨으로 바인딩 강제화하기 storageClassName 속성을 빈 문자열로 지정하지 않으면 미리 프로비저닝된 퍼시스턴트볼륨이 있다고 할지라도 동적 볼륨 프로비저너는 새로운 퍼시스턴트볼륨을 프로비저닝한다. 미리 프로비저닝된 PV에 바인딩하기 위해서는(미리 만들어둔 PV에 바인딩하려면) 명시적으로 storageClassName을 ““로 지정해야 한다.apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mongodb-pvcspec: resources: requests: storage: 1Gi accessModes: - ReadWriteOnce storageClassName: \"\" # 빈 문자열을 스토리지클래스 이름으로 지정하면 PVC가 새로운 PV를 동적 프로비저닝하지 않고 미리 프로비저닝된 PV에 바인딩된다.퍼시스턴트볼륨 동적 프로비저닝의 플로우 클러스터 관리자는 퍼시스턴트볼륨 프로비저너를 설정하고, 하나 이상의 스토리지 클래스를 생성하고 기본값 정의 사용자는 스토리지클래스 중 하나를 참조해 PVC를 생성 PVC는 스토리지클래스와 거기서 참조된 프로비저너를 보고 PVC로 요청된 접근모드, 스토리지 크기, 파라미터를 기반으로 새 PV를 프로비저닝하도록 요청 프로비저너는 스토리지를 프로비저닝하고 PV를 생성한 후 PVC에 바인딩한다. 사용자는 PVC를 이름으로 참조하는 볼륨과 파드를 생성6.7 요약 다중 컨테이너 파드 생성과 파드의 컨테이너들이 볼륨을 파드에 추가하고 각 컨테이너에 마운트해 동일한 파일로 동작하게 할 수 있다. emptyDir 볼륨을 사용해 임시, 비영구 데이터를 저장할 수 있다. gitRepo 볼륨을 사용해 파드의 시작 시점에 깃 리포지터리의 콘텐츠로 디렉터리를 쉽게 채울수 있다. hostpath 볼륨을 사용해 호스트 노드의 파일에 접근한다. 외부 스토리지를 볼륨에 마운트해 파드가 재시작돼도 파드의 데이터를 유지한다. 퍼시스턴트볼륨과 퍼시스턴트볼륨클레임을 사용해 파드와 스토리지 인프라스트럭처를 분리할수 있다. 스토리지클래스를 이용하면 PVC가 원하는 만큼의 PV를 프로비저닝할 수 있다. PVC을 미리 프로비저닝된 PV에 바인딩하고자 할 때 동적 프로비저너가 간섭하는 것을 막을 수도 있다.Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 5. 서비스 : 클라이언트가 파드를 검색하고 통신을 가능하게 함.", "url": "/posts/devlog-platform-kubernetes-in-action5/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-16 17:34:00 +0900", "snippet": "5. 서비스 : 클라이언트가 파드를 검색하고 통신을 가능하게 함. 파드가 다른 파드에게 제공하는 서비스를 사용하려면 다른 파드를 찾는 방법이 필요하다.쿠버네티스에서는 서비스를 제공하는 서버의 정확한 IP 주소나 호스트 이름을 지정해 각 클라이언트 앱을 구성하는것과는 다른 방식이 필요하다.기존의 방식과 다른 방식이 필요한 이유 쿠버네티스 파드는 일시...", "content": "5. 서비스 : 클라이언트가 파드를 검색하고 통신을 가능하게 함. 파드가 다른 파드에게 제공하는 서비스를 사용하려면 다른 파드를 찾는 방법이 필요하다.쿠버네티스에서는 서비스를 제공하는 서버의 정확한 IP 주소나 호스트 이름을 지정해 각 클라이언트 앱을 구성하는것과는 다른 방식이 필요하다.기존의 방식과 다른 방식이 필요한 이유 쿠버네티스 파드는 일시적이고, 언제든 다른 노드로 이동될 수 있기 때문(클러스터 IP는 언제든 변경될수 있다.) 쿠버네티스는 노드에 파드를 스케줄링한 후 파드가 시작되기 바로 직전에 파드의 IP주소를 할당함. 따라서 클라이언트에서 특정 파드의 IP주소를 미리 알 수 없다. 오토 스케일링 등을 위해서는 여러 파드가 동일한 서비스로 제공할 수 있어야 하는데, 각 파드마다 고유한 IP주소가 있고, 클라이언트는 서비스를 지원하는 파드의 수와 IP에 상관없이 단일 IP주소로 모든 파드에 엑세스할수 있어야 한다.5.1 서비스 소개 쿠버네티스의 서비스는 동일한 서비스를 제공하는 파드 그룹에 지속적인 단일 접점을 만들어주는 리소스이다.서비스 설명 서비스를 만들고 클러스터 외부에서 엑세스할 수 있도록 구성하면 외부 클라이언트가 파드에 연결할 수 있는 하나의 고정 IP가 노출 서비스가 관리하는 파드들의 IP가 변경되더라도 서비스의 IP주소는 변경되지 않는다. 내부 클라이언트와 외부 클라이언트 모두 서비스로 파드에 접속5.1.1 서비스 생성 서비스를 지원하는 파드가 한개 혹은 그 이상일 수 있다. 서비스 연결은 뒷단의 모든 파드로 로드밸런싱된다. 서비스에서도 레플리카셋과 동일하게 레이블 셀렉터 매커니즘을 그대로 적용된다.kubectl expose 서비스를 생성하기 가장 쉬운 방법은 kubectl expose 명령어를 사용하는 것이다. expose 명령어를 사용하는 것 대신 kubernetes 명령어를 이용해 서비스 리소스를 생성할 수도 있다.YAML 디스크립터를 통한 서비스 생성apiVersion: v1kind: Servicemetadata: name: kubiaspec: ports: - port: 80 # 서비스가 사용할 포트 targetPort: 8080 # 서비스가 포워드할 컨테이너 포트 selector: app: kubia # app=kubia 레이블이 있는 모든 파드가 이 서비스에 포함된다는것을 의미새 서비스 검사하기# 서비스 생성kubectl create -f kubia-svc.yaml# 서비스 정보 조회 ( 내부 클러스터 IP가 할당되었는지 확인 )kubectl get svc 이렇게 클러스터 IP가 할당되면 클러스터 내부에서는 바로 엑세스할 수 있다.실행인 컨테이너에 원격으로 명령어 실행 kubectl exec 명령어를 사용하면 기존 파드의 컨테이너 내에서 원격으로 임의의 명령어를 실행할 수 있다.# 특정 Pod에 접속하여 서비스의 클러스터 IP로 http 요청kubectl exec kubia-4dkws -- curl -s http://10.102.206.16# pod shell 접근kubectl exec --stdin --tty kubia-4dkws -- /bin/bash더블 대시를 사용하는 이유 명령어의 더블 대시(–)는 kubectl 명령줄 옵션의 끝을 의미함. 더블 대시 뒤의 모든 것은 파드 내에서 실행돼야 하는 명령이다. kubectl exec kubia-4dkws -- curl -s http://10.102.206.16 의 예제에서 더블 대시가 없다면 -s 옵션은 kubectl exec의 옵션으로 해석하여 처리 되지 않는다.서비스의 세션 어피니티 구성 동일한 클라이언트에서 요청하더라도 서비스 프록시가 각 연결을 임의의 파드를 선택해서 연결을 다시 전달(forward)하기 때문에 요청할 때마다 다른 파드가 선택된다. 특정 클라이언트의 모든 요청을 매번 같은 파드로 리디렉션하려면 서비스의 세션 어피니티(sessionAffinity)속성을 기본값 None 대신 ClientIP로 설정하면 된다. 쿠버네티스에서는 None, ClientIP두 가지 유형의 서비스 세션 어피니티만 지원. 서비스 레벨에서는 HTTP 수준에서는 작동하지 않고 TPC / UDP 패킷을 처리하고 그들이 가지고 있는 payload는 신경쓰지 않는다.(쿠키 기반으로 할 수 없음)apiVersion: v1kind: Servicemetadata: name: kubiaspec: sessionAffinity: ClientIP # 동일한 클라이언트 IP의 모든 요청을 동일한 파드로 전달동일한 서비스에서 여러 개의 포트 노출 파드가 2개의 포트(http : 8080, https : 8443)을 수신한다면 하나의 서비스를 사용해 포트 80과 433을 파드의 포트 8080과 8443으로 전달할 수 있음. 하나의 서비스를 사용해 멀티 포트 서비스를 사용하면 단일 클러스터 IP로 모든 서비스 포트가 노출된다.이름이 지정된 포트 사용 포트 번호가 잘 알려진 경우가 아니더라도 서비스 스펙을 좀 더 명확히 할 수 있는 방법. 나중에 서비스 스펙을 변경하지 않고도 pod의 포트 번호를 변경할 수 있다는 큰 장점이 있음.# 파드 정의에 포트 이름 사용kind: Podspec: ports: - name: http containerPort: 8080 - name: https containerPort: 8443# 서비스에 이름이 지정된 포트 참조apiVersion: v1kind: Servicemetadata: name: kubiaspec: ports: - name: http port: 80 targetPort: http # 포트 80은 컨테이너 포트의 이름이 http인 것에 매핑 - name: https port: 443 targetPort: https # 포트 443은 컨테이너 포트의 이름이 https인 것에 매핑 selector: app: kubia5.1.2 서비스 검색 서비스의 파드는 생성되기도 하고 사라지기도 하고, 파드 IP가 변경되거나 파드 수는 늘어나거나 줄어들 수도 있다. 이러한 상황 속에서 항상 서비스의 IP주소로 엑세스할 수 있어야 한다. 쿠버네티스는 클라이언트 파드가 서비스의 IP와 포트를 검색할 수 있는 방법을 제공한다.환경변수를 통한 서비스 검색 파드가 시작되면 쿠버네티스는 해당 시점에 존재하는 각 서비스를 가리키는 환경변수 세트를 초기화한다. 환경변수 네이밍 규칙은 서비스 이름의 대시(-)는 밑줄(_)로 변환되고 서비스 이름이 환경변수 이름의 접두어로 쓰이면서 모든 문자는 대문자로 표시한다.kubectl exec kubia-4dkws env# KUBIA_SERVICE_HOST=10.102.206.16 # 서비스 클러스터 IP# KUBIA_PORT_80_TCP_PORT=80 # 서비스가 제공되는 포트DNS를 통한 서비스 검색 kube-system 네임스페이스에 파드 중 kube-dns라는 게 있었다. kube-dns는 DNS 서버를 실행하며 클러스터에서 실행중인 다른 모든 파드는 자동으로 이를 사용하도록 구성되는 파드이다. 파드에서 실행중인 프로세스에서 수행된 모든 DNS 쿼리는 시스템에서 실행중인 모든 서비스를 알고 있는 쿠버네티스의 자체 DNS 서버로 처리된다. 각 서비스는 내부 DNS 서버에서 항목을 가져오고 서비스 이름을 알고 있는 클라이언트 파드는 환경변수 대신 FQDN(정규화된 도메인 이름)으로 엑세스 할수 있다.FQDN을 통한 서비스 연결 backend-database.default.svc.cluster.local“backend-database” -&gt; 서비스 이름“default” -&gt; 네임스페이스“svc.cluster.local” -&gt; 모든 클러스트의 로컬 서비스 이름에 사용되는 도메인 접미사 클라이언트는 여전히 서비스의 포트번호를 알아야 한다. 표준 포트가 아닌 경우 문제가 될수 있다.(환경변수에서 포트 번호를 얻을수 있어야 함) 접미사와 네임스페이스는 생략이 가능하다. 위 예제에서는 “backend-database” FQDN만으로 서비스에 엑세스할 수 있다.파드의 컨테이너 내에서 셸 실행 파드 컨테이너 내부의 DNS resolver가 구성되어 있기 때문에 네임스페이스와 접미사를 생략할 수 있다.# 파드 컨테이너 냉서 셸 실행kubectl exec -it kubia-4dkws bash# DNS resolver 확인cat /etc/resolv.conf# FQDN을 통한 서비스 호출(모두 같은 결과)curl http://kubia.default.svc.cluster.localcurl http://kubia.defaultcurl http://kubia서비스 IP에 핑을 할 수 없는 이유 서비스로 crul은 동작하지만 핑은 응답이 오지 않는다. 이는 서비스의 클르서트 IP가 가상 IP이므로 서비스 포트와 결합된 경우에만 의미가 있기 때문5.2 클러스터 외부에 있는 서비스 연결 클러스터에서 실행중인 파드는 내부 서비스에 연결하는 것처럼 외부 서비스에 연결할 수 있다.5.2.1 서비스 엔드포인트 소개 서비스는 파드에 직접 연결(link)되지 않는다. 대신 엔드포인트 리소스가 그 사이에 있다. 파드 셀렉터는 서비스 스펙에 정의돼 있지만 들어오는 연결을 전달할 때 직접 사용하지 않고, IP와 포트 목록을 작성하는데 사용되며, 엔드포인트 리소스에 저장된다.# service 정보 조회kubectl describe svc kubia# kubia endpoints 조회kubectl get endpoints kubia5.2.2 서비스 엔드포인트 수동 구성 서비스의 엔드포인트를 서비스와 분리하면 엔드포인트를 수동으로 구성하고 업데이트할수 있다. 수동으로 관리되는 엔드포인트를 사용해 서비스를 만들려면 서비스와 엔드포인트 리소스를 모두 만들어야 한다.셀렉터 없이 서비스 생성apiVersion: v1kind: Servicemetadata: name: external-service # 엔드포인트 오브젝트 이름과 일치해야 함.spec: # spec에 selector를 정의하지 않음. ports: - port: 80셀렉터가 없는 서비스에 관한 엔드포인트 리소스 생성 엔드포인트 오브젝트는 서비스 이름과 같아야 하고, 서비스를 제공하는 대상 IP주소와 포트 목록을 가져야 함.apiVersion: v1kind: Endpointsmetadata: name: external-service # 서비스 이름과 일치시킴.subsets: - addresses: # 서비스가 연결을 전달할 엔드포인트 IP 생성 - ip: 11.11.11.11 - ip: 22.22.22.22 ports: - port: 80 # 엔드포인트의 대상 포트외부 엔드포인트를 가지는 서비스를 만들어야 하는 목적 나중에 외부 서비스를 쿠버네티스 내에서 실행되는 파드로 마이그레이션하기로 한 경우 서비스에 셀렉터를 추가해 엔드포인트를 자동으로 관리 할 수 있다. 이를 통해 서비스의 실제 구현이 변경되는 동안에도 서비스 IP 주소가 일정하게 유지될 수 있다.5.2.3 외부 서비스를 위한 별칭 생성ExternalName 서비스 생성 외부 서비스의 별칭으로 하려는 경우 유형(type) 필드를 ExternalName으로 설정하면 된다.apiVersion: v1kind: Servicemetadata: name: external-servicespec: type: ExternalName # ExternalName 유형 externalName: api.somecompany.com # FQDN(Fully Qualified Domain Name) 이름 지정 ports: - port: 80 여기서 FQDN을 사용하는 대신 external-service.default.svc.clster.local 도메인 이름으로 외부 서비스에 연결할수도 있다. ExternalName 서비스는 DNS 레벨에서만 구현된다. 서비스에 연결하는 클라이언트는 서비스 프록시를 완전히 무시하고 외부 서비스에 직접 연결된다. 이러한 이유로 Cluster IP를 얻을 수 없음.5.3 외부 클라이언트에 서비스 노출 쿠버네티스느 외부에서 서비스를 엑세스할 수 있는 방법을 몇가지 제공해준다.5.3.1 노트포트 서비스 서비스를 생성하고 유형을 노드포트로 설정하는 방법노드포트 서비스 생성 nodePort를 생략할 경우 쿠버네티스가 임의의 포트를 선택한다.apiVersion: v1kind: Servicemetadata: name: kubia-nodeportspec: type: NodePort # 노드포트 서비스 유형 ports: - port: 80 # 서비스 클러스터 IP 포트 targetPort: 8080 # 서비스 대상 파드의 포트 nodePort: 30123 # 각 클러스터 노드의 포트 30123을 통해 서비스에 엑세스 할수 있음. selector: app: kubia노드포트 서비스 확인# 노드포트 서비스 생성kubectl create -f kubia-svc-nodeport.yaml# 노드포트 서비스 확인# kubectl get svc kubia-nodeport# 노드 Ip 조회(minikube에서는 안됨)kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}'# minikube nodeport service(노드포트 서비스를 로컬에서 접근 가능하도록 처리)minikube service kubia-nodeportcrul http://127.0.0.1:53446/|-----------|----------------|-------------|-------------------------|| NAMESPACE | NAME | TARGET PORT | URL ||-----------|----------------|-------------|-------------------------|| default | kubia-nodeport | 80 | http://172.17.0.2:30123 ||-----------|----------------|-------------|-------------------------|🏃 Starting tunnel for service kubia-nodeport.|-----------|----------------|-------------|------------------------|| NAMESPACE | NAME | TARGET PORT | URL ||-----------|----------------|-------------|------------------------|| default | kubia-nodeport | | http://127.0.0.1:53446 ||-----------|----------------|-------------|------------------------|노드포트 서비스의 단점 클라이언트가 하나의 노드에만 요청하는 경우 노드에 장애가 발생할 경우 더 이상 서비스에 엑세스할 수 없으므로, 노드 앞에 로드밸런서를 배치하는 것이 좋다.5.3.2 외부 로드밸런서로 서비스 노출 쿠버네티스 클러스터는 로드밸런서를 자동으로 프로비저닝하는 기능을 제공한다. 쿠버네티스가 로드밸런서 서비스를 지원하지 않는 환경에서 실행중인 경우 로드밸런서는 프로비저닝되지는 않지만 서비스는 여전히 노드포트 서비스처럼 작동한다.로드밸런서 서비스 생성apiVersion: v1kind: Servicemetadata: name: kubia-loadbalancerspec: type: LoadBalancer # 쿠버네티스 클러스터를 호스팅하는 인프라에서 로드밸런서를 얻을수 있다. ports: - port: 80 targetPort: 8080 selector: app: kubia로드밸런서를 통한 서비스 연결# 로드밸런서 서비스 생성kubectl create -f kubia-svc-loadbalancer.yaml# 확인kubectl get svc kubia-loadbalancer세션 어피니티와 웹 브라우저 웹 브라우저에서 세션 어피티니가 None이더라도 같은 파드로 계속 요청하는 현상을 볼수 있음. 그 이유는 브라우저의 http keep-alive header을 사용하기 때문이다.5.3.3 외부 연결의 특성 이해불필요한 네트워크 홉의 이해와 예방 외부 클라이언트가 노드포트로 서비스에 요청할 경우 임의로 선택된 파드가 연결을 수신한 동일한 노드에서 실행중일 수도 있고, 그렇지 않을 수도 있다. 파드에 도달하려면 추가적인 네트워크 홉이 필요할 수 있으며 이것이 항상 바람직한 것은 아니다. 요청을 수신한 노드에서 실행중인 파드로만 외부 트래픽을 전달하도록 서비스를 구성해 추가 홉을 방지할 수 있는 옵션을 제공한다.spec: externalTrafficPolicy: LocalexternalTrafficPolicy을 이용할때 주의사항 서비스 프록시는 로컬에 실행중인 파드를 선택하는데 로컬 파드가 업으면 요청을 중단시킨다.(로드밸런서는 파드가 하나 이상 있는 노드에만 연결을 전달하도록 해야 함) 모든 파드에 균등하게 분산되지 않을 수 있다.클라이언트 IP가 보존되지 않음 인식 노드포트로 연결을 수신하면 패킷에서 소스 네트워크 주소 변환(SNAT)이 수행되므로 패킷의 소스 IP가 변경된다. 웹 서버의 경우 엑세스 로그에 브라우저의 IP를 표시할 수 없다는 것은 의미함.. 로컬 외부 트래픽 정책(Local External Traffic Policy은 연결을 수신하는 노드와 대상 파드를 호스팅하는 노드 사이에 추가 홉이 없기 때문에 클라이언트 IP 보존에 영향을 미친다.5.4 인그레스 리소스로 서비스 외부 노출인그레스가 필요한 이유 인그레스는 한 IP 주소로 수십 개의 서비스에 접근이 가능하도록 지원한다. 네트워크 스택의 애플리케이션 계층(HTTP)에서 작동하며, 서비스가 할 수 없는 쿠키 기반 세션 어피니티 등과 같은 기능 제공이 가능하다.Minikube에서 인그레스 애드온 활성화# minikube addons 확인minikube addons list# ingress addons 활성화( minikube start --vm=true 로 시작해야 가능)minikube addons enable ingress# 기존에 설치된 docker 기반 minikube deleteminikube delete# minikube vm 모드로 시작minikube start --vm=true --driver=hyperkit# 모든 네임스페이스 pods 조회kubectl get po --all-namespaces5.4.1 인그레스 리소스 생성apiVersion: extensions/v1beta1kind: Ingressmetadata: name: kubiaspec: rules: - host: kubia.example.com # 인그레스는 kubia.example.com 도메인 이름으로 서비스에 매핑된다. http: paths: - path: / backend: serviceName: kubia-nodeport # 모든 요청은 kubia-nodeport 서비스의 포트 80으로 전달된다. servicePort: 805.4.2 인그레스 서비스 엑세스# ingress 리소스 생성kubectl create -f kubia-ingress.yaml# ingresses 조회kubectl get ingresses# host 설정sudo vi etc/hosts192.168.64.2\tkubia.example.com# 호출curl http://kubia.example.com인그레스 동작 방식5.4.3 하나의 인그레스로 여러 서비스 노출 규칙과 경로가 모두 배열이라 여러 항목을 가질 수 있다. 여러 호스트(host)와 경로(path)를 여러 서비스(backend.serviceName)에 매핑할 수 있다.동일한 호스트의 다른 경로로 여러 서비스 매핑 - host: kubia.example.com http: paths: - path: /kubia backend: # kubia.example.com/kubia으로의 요청을 kubia 서비스로 라우팅된다. serviceName: kubia servicePort: 80 - path: /bar backend: # kubia.example.com/bar으로의 요청을 bar 서비스로 라우팅된다. serviceName: bar servicePort: 80서로 다른 호스트로 서로 다른 서비스 매핑하기spec: rules: - host: foo.example.com http: paths: - path: / backend: # foo.example.com으로의 요청을 foo 서비스로 라우팅된다. serviceName: foo servicePort: 80 - host: bar.example.com http: paths: - path: / backend: # bar.example.com으로의 요청을 bar서비스로 라우팅된다. serviceName: bar servicePort: 805.4.4 TLS 트래픽을 처리하도록 인그레스 구성인그레스를 위한 TLS 인증서 생성# 개인키 생성openssl genrsa -out tls.key 2048# 인증서 생성openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj /CN=kubia.example.com# 시크릿 리소스 생성kubectl create secret tls tls-secret --cert=tls.cert --key=tls.keyCertificateSigningRequest 리소스로 인증서 서명 인증서를 직접 서명하는 대신 CSR 리소스를 만들어 인증서에 서명할 수 있음.kubectl certificate approve &lt;name of th CSR&gt;tls 트래픽을 처리하는 인그레스 생성apiVersion: extensions/v1beta1kind: Ingressmetadata: name: kubia-tlsspec: tls: # TLS 구성 - hosts: - kubia.example.com secretName: tls-secret # 개인키와 인증서는 위에서 생성한 tls-secret 참조 rules: - host: kubia.example.com http: paths: - path: / backend: serviceName: kubia-nodeport servicePort: 80# https request 호출curl -k -v https://kubia.example.com/kubia5.5 레디니스 프로브 ( 파드가 연결을 수락할 준비가 됐을 때 신호 보내기 ) 파드는 구성에 시간이 걸릴 수 있다. 데이터를 로드하는 데 시간이 걸리거나, 첫번 째 사용자 요청이 너무 오래걸리거나 사용자 경험에 영향을 미치는 것을 방지하고자 준비 절차를 수행해야 할 수도 있다.5.5.1 레디니스 프로브 소개 라이브니스 프로브와 비슷하게 파드에 레디니스 프로브를 정의할 수 있다. 주기적으로 호출되며 특정 파드가 클라이언트 요청을 수신할 수 있는지를 결정한다. 애플리케이션 특성에 따라 상세한 레디니스 프로브를 작성하는 것은 개발자의 몫레디니스 프로브 유형 Exec 프로브 : 프로세스를 실행 HTTP GET 프로브 : HTTP GET 요청 수행 TCP 소켓 프로브 : 컨테이너에 TCP 연결 확인레디니스 프로브의 동작 컨테이너가 시작될 때 쿠버네티스는 첫 번째 레디니스 점검을 수행하기 전에 구성 가능한 시간이 경과하기를 기다릴 수 있도록 구성 가능 주기적으로 프로브를 호출하고 레디니스 프로브의 결과에 따라 작동 파드가 준비되지 않았다고 하면 서비스에서 제거하고, 파드가 준비되면 서비스에 다시 추가한다. 컨테이너가 준비 상태 점검에 실패하더라도 컨에티너가 종료되거나 다시 시작시키지 않는다.라이브니스 프로브 vs 레디니스 프로브 라이브니스 프로브는 상태가 좋지 않은 컨테이너를 제거하고 새롭고 건강한 컨테이너로 교체해 파드의 상태를 정상으로 유지시킨다. 레디니스 프로브는 요청을 처리할 준비가 된 파드의 컨테이너만 요청을 수신하도록 한다.(L7 health check와 유사)레디니스 프로브가 중요한 이유 파드 그룹이 다른 파드에서 제공하는 서비스에 의존한다고 했을때(웹 앱 -&gt; 백엔드 데이터베이스) 웹 앱 파드중 하나만 DB에 연결할 수 없는 경우, 요청을 처리할 준비가 되지 않았다고 신호를 주는게 현명할 수 있다. 레디니스 프로브를 사용하면 클라이언트가 정상 상태인 파드하고만 통신할 수 있다. 그래서 시스템에 문제가 있다는 것을 절대 알아차리지 못한다.5.5.2 파드에 레디니스 프로브 추가# replicaSet 수정kubectl edit rs kubia# yamlapiVersion: apps/v1kind: ReplicaSetmetadata: name: kubiaspec: replicas: 3 selector: matchLabels: app: kubia template: metadata: labels: app: kubia spec: containers: - name: kubia image: sungsu9022/kubia ports: - name: http containerPort: 8080 readinessProbe: # 파드의 각 컨테이너에 레디니스 프로브를 정의 exec: # ls /var/ready 명령어를 주기적으로 수행하여 존재하면 0(성공), 그렇지 않으면 다른 값(실패) command: - ls - /var/ready 이렇게 하면 /var/ready 파일이 없으므로 READY에 준비된 컨테이너가 없다고 표시된다.# /var/ready 파일 생성kubectl exec kubia-4wjqj -- touch /var/ready# 확인(kubia-4wjqj 가 READY 1/1로 변경됨)kubectl get pods# 레디니스 프로브 조회kubectl describe pod kubia-4wjqj# 하나의 READY 파드로 서비스를 호출curl http://kubia.example.com5.5.3 실제 환경에서 레디니스 프로브가 수행해야 하는 기능 서비스에서 파드를 수동으로 추가하거나 제거하려면 파드와 서비스의 레이블 셀렉터에 enabled=true 레이블을 추가한다. 서비스에서 파드를 제거하려면 레이블을 제거하라.레디니스 프로브를 항상 정의하라 파드의 레디니스 프로브를 추가하지 않으면 파드가 시작하는 즉시 서비스 엔드포인트가 된다. 여전히 시작 단계로 수신 연결을 수락할 준비가 되지 않은 상태에서 파드로 전달된다. 따라서 클라이언트가 Connection Refused 유형의 에러를 보게 된다. 기본 URL에 HTTP 요청을 보내더라도 항상 레디니스 프로브를 정의해야 한다.레디니스 프로브에 파드의 종료 코드를 포함하지 마라 파드가 종료할 때, 실행되는 앱은 종료 신호를 받자마자 연결 수단을 중단한다. 쿠버네티스는 파드를 삭제하자마자 모든 서비스에서 파드를 제거하기 때문에 굳이 별도로 이런 처리를 할 필요가 없다.5.6. 헤드리스 서비스로 개별 파드 찾기 클라이언트가 모든 파드에 연결해야 하는 경우 어떻게 할수 있을까? 파드가 다른 파드에 각각 연결해야 하는 경우 어떻게 해야 할까? 클라이언트가 모든 파드에 연결하려면 각 파드의 IP를 알아야 한다. 쿠버네티스는 클라이언트가 DNS 조회로 파드 IP를 찾을 수 있도록 한다. 쿠버네티스 서비스에 클러스터 IP가 필요하지 않다면 ClusterIP 필드를 None으로 설정하여 DNS 서버는 하나의 서비스 IP 대신 파드 IP 목록들을 반환한다. 이때 각 레코는 현재시점 기준으로 서비스를 지원하는 개별 파드의 IP를 가리킨다. 따라서 클라이언트는 간단한 DNS A 레코드 조회를 수행하고 서비스에 포함된 모든 파드의 IP를 얻을 수 있다.5.6.1 헤드리스 서비스 생성 서비스 스펙의 clusterIP필드를 None으로 설정하면 클라이언트가 서비스의 파드에 연결할 수 있는 클러스터 IP를 할당하지 않기 떄문에 서비스가 헤드리스 상태가 된다. 클러스터 Ip가 없고 엔드포인트에 파드 셀렉터와 일치하는 파드가 포함돼 있음을 확인할 수 있다.apiVersion: v1kind: Servicemetadata: name: kubia-headlessspec: clusterIP: None # 헤드리스 서비스로 만드는 spec 옵션 ports: - port: 80 targetPort: 8080 selector: app: kubia5.6.2 DNS로 파드 찾기 실제로 클라이언트 관점에서는 헤드리스 서비스를 사용하나 일반 서비시를 사용하나 관계 없이 서비스의 DNS 이름에 연결해 파드에 연결할 수 있다. 차이점이 있다면 헤드리스 서비스의 경우 클라이언트는 서비스 프록시 대신 파드에 직접 연결한다. 헤드리스 서비스는 여전히 파드간에 로드밸런싱을 제공하지만 서비스 프록시 대신 DNS 라운드 로빈 매커니즘으로 처리된다.# dnsutils pod 생성kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1 --command -- sleep infinity# 헤드리스 서비스를 위해 반환된 DNS A 레코드( 레코드 목록이 표시된다. )kubectl exec dnsutils nslookup kubia-headless# 일반 서비스 nslookup ( 클러스터 IP가 표시된다. )kubectl exec dnsutils nslookup kubia5.6.3 모든 파드 검색 - 준비되지 않은 파드도 포함 쿠버네티스가 파드의 레디니스 상태에 관계 없이 모든 파드를 서비스에 추가되게 하려면 서비스에 다음 어노테이션을 추가해야 한다. tolerate-unready-endpoints는 deprecated되었고, publishNotReadyAddresses를 사용해서 동일한 기능을 처리할 수 있다.( https://github.com/kubedb/project/issues/242 )kind: Service#metadata:# annotations:# service.alpha.kubernetes.io/tolerate-unready-endpoints: \"true\"spec publishNotReadyAddresses: true5.7 서비스 문제 해결 FAQ 서비스로 파드에 엑세스할 수 없는 경우 다음 내용을 확인해보면 도움이 된다. 외부가 아닌 클러스터 내에서 서비스의 클러스터 IP에 연결되는지 확인 서비스에 엑세스할 수 있는지 확인하려고 서비스 IP로 핑을 하는 케이스(핑 동작 안함) 레디니스 프로브를 정의했다면 성공했는지 확인하라, pod의 READY 여부 확인 파드가 서비스의 일부인지 확인하려면 kubectl get endpoints를 사용해 해당 엔드포인트 오브젝트를 확인하라. FQDN이나 그 일부로 서비스에 엑세스하려고 하는데 작동하지 않는 경우 FQDN 대신 클러스터 IP를 사용해 엑세스할수 있는지 확인하라 대상 포트가 아닌 서비스로 노출된 포트에 연결하고 있는지 확인하라 파드 IP에 직접 연결해 파드가 올바른 포틍 연결돼있는지 확인하라 앱이 로컬호스트에만 바인딩하고 있는지 확인하라. 5.8 요약 서비스는 안정된 단일 IP 주소와 포트로 특정 레이블 셀렉터와 일치하는 여러 개의 파드를 노출 기본적으로 클러스터 내부에서 서비스에 엑세스할 수 있지만 유형을 노드포트 또는 로드밸런서로 설정해 클러스터 외부에서 서비스에 엑세스할 수 있다. 파드는 환경변수를 검색해 IP 주소와 포트로 서비스를 검색 할수 있다. 엔드포인트 리소스를 만드는 대신 셀렉터 설정 없이 서비스 리소스를 생성해 클러스터 외부에 있는 서비스를 검색하고 통신할 수 있다. ExternalName 서비스 유형으로 외부 서비스에 대한 DNS CNAME(별칭)을 제공할 수 있다. 단일 인그레스로 여러 HTTP 서비스를 노출할 수 있다. 파드 컨테이너의 레디니스 프로브는 파드를 서비스 엔드포인트에 포함해야 하는지 여부를 결정한다. 헤드리스 서비스를 생성하면 DNS로 파드 IP를 검색할 수 있다.Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 4. 레플리케이션과 그 밖의 컨트롤러 : 관리되는 파드 배포", "url": "/posts/devlog-platform-kubernetes-in-action4/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-07 17:34:00 +0900", "snippet": "4. 레플리케이션과 그 밖의 컨트롤러 : 관리되는 파드 배포4.1 파드를 안정적으로 유지하기 쿠버네티스를 사용하면 얻을 수 있는 주요 이점은 쿠버네티스에 컨테이너 목록을 제공하면 해당 컨테이너를 클러스터 어딘가에서 계속 실행되도록 할수 있는 것이다. 파드가 노드에 스케줄링되면 노드의 Kubelet은 이 파드가 존재하는 한 컨테이너가 계속 실행되도록...", "content": "4. 레플리케이션과 그 밖의 컨트롤러 : 관리되는 파드 배포4.1 파드를 안정적으로 유지하기 쿠버네티스를 사용하면 얻을 수 있는 주요 이점은 쿠버네티스에 컨테이너 목록을 제공하면 해당 컨테이너를 클러스터 어딘가에서 계속 실행되도록 할수 있는 것이다. 파드가 노드에 스케줄링되면 노드의 Kubelet은 이 파드가 존재하는 한 컨테이너가 계속 실행되도록 하고, 컨테이너의 주 프로세스에 크래시가 발생하면 Kubelet이 컨테이너를 다시 시작시킨다. 하지만 JVM 기준으로 OOM이 발생하거나 애플리케이션 무한 루프나 교착상태에 빠져서 응답을 주지 못하는 경우 앱을 재실행하려면 앱 내부의 기능에 의존해서는 안되고, 외부에서 앱의 상태를 체크해야 한다.4.1.1. 라이브니스 프로브 (Liveness probe) 쿠버네티스는 라이브니스 프로브를 통해 컨테이너가 살아 있는지 확인할 수 있다. 파드의 스펙에 각 컨테이너의 라이브니스 프로브를 지정하면 된다. 쿠버네티스는 주기적으로 프로브를 실행하고 프로브가 실패할 경우 컨테이너를 다시 시작한다.쿠버네티스의 프로브 매커니즘1) HTTP GET 프로브 지정한 IP, 포트, 경로에 HTTP GET 요청을 수행 프로브가 응답을 수신하고 응답코드가 오류가 아닌 경우 성공(2xx, 3xx) 오류 응답코드이면 실패로 간주(4xx, 5xx)2) TCP 소켓 프로브 컨테이너의 지정된 포트에 TCP 연결을 수행 연결이 성공하면 성공, 실패하면 실패3) Exec 프로브 컨테이너 내의 임의의 명령을 실행하고 명령의 종료 상태 코드 확인 상태 코드가 0이면 성공, 다른코드는 모두 실패로 간주4.1.2 HTTP 기반 라이브니스 프로브 생성apiVersion: v1kind: Podmetadata: name: kubia-livenessspec: containers: - image: luksa/kubia-unhealthy name: kubia livenessProbe: # 라이브니스 프로브 추가 httpGet: path: / # HTTP 요청 경로 port: 8080 # 프로브가 연결해야 하는 포트4.1.3 동작중인 라이브니스 프로브 확인# 라이브니스 프로브 생성kubectl create -f kubia-liveness-probe.yaml# 라이브니스 프로브 확인kubectl get po kubia-liveness# 크래시된 컨테이너의 애플리케이션 로그 조회kubectl logs {podName} --previous# pod이 다시 시작되는 이유 확인kubectl describe po kubia-livenessPod Last State Exit Code 정상적으로 종료된 경우 0 외부에 의해서 종료된 경우 128 + x의 값을 가진다. x는 프로세스에 전송된 시그널 번호임. SIGKILL 번호인 9로 강제종료된다면 128+9 = 137이 된다. 4.1.4 라이브니스 프로브의 추가 속성 설정kubectl describe을 통해 라이브니스 추가 정보를 확인할 수 있다.Liveness : http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3 delay(지연) - 컨테이너가 시작된 후 바로 프로브 시작된다. timeout(제한시간) - 컨테이너가 제한시간 안에 응답이 와야 한다. period(기간) - 기간마다 프로브를 수행한다 #failure(실패수) - n번 연속 실패시 컨테이너가 다시 시작한다.설정 정의 initialDelaySeconds 옵션을 정의하여 설정할 수 있다.apiVersion: v1kind: Podmetadata: name: kubia-livenessspec: containers: - image: luksa/kubia-unhealthy name: kubia livenessProbe: httpGet: path: / port: 8080 initialDelaySeconds: 15라이브니스 프로브 정의시 주의사항 애플리케이션 시작시간을 고려해서 초기 지연을 설정해야 한다.4.1.5 효과적인 라이브니스 프로브 생성 운영환경이라면 반드시 라이브니스 프로브를 정의하는 것이 좋다.(L7 health check 같은 느낌으로) 정의하지 않으면 쿠버네티스가 앱이 살아 있는지 알 수 있는 방법이 없음.(프로세스가 떠있더라도 실제 문제 상황이 있을 수 있기 때문)라이브니스 프로브가 확인해야 할 사항 URL에 요청하도록 프로브를 구성해 앱 내 실행중인 모든 주요 구성요소가 살아있는지 또는 응답이 없는지 확인하도록 구성할 수도 있다. 특정 가벼운 API를 호출해본다던지 별도의 헬스체크 URL을 백엔드 엔드포인트로 구성한다던지 라이브니스 프로브는 앱 내부만 체크하고 외부 요인의 영향을 받지 않도록 구성해야 한다. 예를 들면 DB 서버에 문제가 있는 경우 이 프로브가 실패하도록 구성하는것은 지양해야 한다. 그 이유는 근본적인 원인이 DB라면 앱 컨테이너를 재시작한다 하더라도 문제가 해결되지 않기 때문이다. 프로브를 가볍게 유지하기 기본적으로 프로브는 비교적 자주 실행되며 1초 내에 완료되어야 한다. 너무 많은 일을 하는 프로브는 컨테이너 속도를 저하 시킬수 있다. 컨테이너에서 JVM 과 같은 기동 절차에 상당한 연산 리소스가 필요한 경우 Exec 프로브보다는 HTTP GET 라이브니스 프로브가 적합하다.프로브에 재시도 루프를 구현하지 마라. 프로브의 실패 임계값을 설정할수 있기도 하고, 실패 임계값이 1이더라도 쿠버네티스는 실패를 한번 했다고 간주하기 전에 여러번 재시도를 한다. 따라서 자체적인 재시도 루프를 구현하지 말아야 한다.라이브니스 프로브 요약 라이브니스 프로브에 대한 로직은 해당 워커 노드의 Kubelet에서 수행이 된다. 만약 워커 노드 자체에 크래시가 발생한 경우 해당 노드의 중단된 모든 파드의 대체 파드를 생성해야 하는것은 컨트롤 플레인의 역할 레플리케이션컨트롤러같은 리소스 외에 직접 생선한 Pod들은 워커 노드 자체가 고장나면 아무것도 할 수 없음.4.2 레플리케이션 컨트롤러 소개 레플리케이션 컨트롤러는 파드가 항상 실행되도록 보장하는 쿠버네티스 리소스이다. 파드의 여러 복제본(레플리카)을 작성하고 관리하기 위한 리소스이다.동작 원리 노드1의 파드 A는 종료된 이후 레플리케이션 컨트롤러가 관리하지 않기 때문에 다시 생성되지 않는다. 레플리케이션컨트롤러는 파드B가 사라진것을 인지하고 새로운 파드 인스턴스를 생성한다.4.2.1 레플리케이션컨트롤러의 동작 실행중인 파드 목록을 지속적으로 모니터링하고, 특정 유형의 실제 파드 수가 의도하는 수와 일치하는지 항상 확인한다.의도하는 수의 복제본수가 변경되는 케이스 파드 유형이란 특정 레비을 셀렉터와 일치하는 파드 세트(실제로 유형이라는 개념은 특별히 없다.) 누군가 같은 유형의 파드를 수동으로 만드는 경우 누군가 기존 파드의 유형을 변경하는 경우 누군가 의도하는 파드 수를 줄이는 경우 컨트롤러 조정 루프 레플리케이션컨트롤러의 역할은 정확한 수의 파드가 항상 레이블 셀렉터와 일치하는지 확인하는 것이다.레플리케이션컨트롤러의 3가지 요소 레이블 셀렉터는 범위에 있는 파드를 결정 레플리카수는 실행할 파드의 의도하는 수를 지정 파드 템플릿은 새로운 파드 레플리카 만들때 사용(파드 정의)컨트롤러의 레이블 셀렉터 또는 파드 템플릿 변경의 영향 이해 레이블 셀렉터와 파드 템플릿을 변경하더라도 기존 파드에는 영향을 미치지 않음. 레플리케이션컨트롤러는 파드를 생성한 후에는 실제 컨텐츠(컨테이너 이미지, 환경변수 등)에 신경 쓰지 않음. 그래서 변경 이후에 새 파드가 생성되는 시점에서만 영향을 미친다.레플리케이션컨트롤러 사용시 이점 기존 파드가 사라지면 새 파드를 시작해 파드가 항상 실행되도록 보장할 수 있다. 클러스터 노드에 장애가 발생하면 장애가 발생한 노드에서 실행중인 모든 파드에 관한 교체 복제본이 생성된다. 수동 또는 자동으로 파드를 쉽게 수평 확장할수 있다.4.2.2 레플리케이션컨트롤러 생성apiVersion: v1kind: ReplicationControllermetadata: name: kubia # 레플리케이션컨트롤러 이름spec: replicas: 3 # 의도한 파드 인스턴스 수 selector: # 관리하는 파드 셀렉터 app: kubia template: # 파드 템플릿 metadata: labels: app: kubia spec: containers: - name: kubia image: luksa/kubia ports: - containerPort: 8080 레플리케이션 spec.selector를 지정하지 않을 수도 있다.(Optional) 셀렉터를 지정하지 않으면 템플릿의 레이블로 자동 설정된다. 레플리케이션컨트롤러를 정의할 때 셀렉터를 지정하지 않는것이 좋다.(쿠버네티스가 자동으로 추출하도록 하는게 간결하고 더 단순하다)레플리케이션 컨트롤러 생성kubectl create -f kubia-rc.yaml4.2.3 레플리케이션 컨트롤러 동작 확인kubectl get pods삭제된 파드에 관한 레플리케이션 컨트롤러의 반응# 삭제kubectl delete pod kubia-2lqjr# 삭제된 팟 확인kubectl get pods# 레플리케이션 정보 확인kubectl get rc# 레플리케이션컨트롤러 추가 정보 확인kubectl describe rc kubia컨트롤러가 새로운 파드를 생성한 원인 정확히 이해하기 레플레키에션컨트롤러가 삭제 액션에 반응한 것이 아니다. 결과적인 상태(부족한 파드수)에 대응한 것을 알아야 한다. 파드 삭제가 일어난 이후 컨트롤러의 실제 파드 수 확인하였고, 이에 대한 적절한 조치로 새로운 파드가 생성된 것노드 장애 대응 쿠버네티스를 사용하지 않는 환경에서 노드에 장애가 발생하면 앱을 수동으로 다른 시스템에 마이그레이션해야 함.(매우 오랜 시간이 걸리고 큰 문제) 레플리케이션컨트롤러는 노드의 다운을 감지하자마자 파드를 대체하기 위해 새 파드를 기동시킨다.# 노드의 eth0 을 다운시킨 후 노드 확인# 이 경우 해당 노드는 NotReady 상태로 변경된다.kubectl get node# 파드 확인# NotReady 상태의 노드에 있던 Pod는 Unknown 상태로 변경되고 삭제된다.(새로운 파드가 다른 노드에서 생성됨)kubectl get pods4.2.4 레플리케이션컨트롤러 범위 안팎으로 파드 이동하기 레플리케이션컨트롤러는 레이블 셀렉터와 일치하는 파드만을 관리한다. 파드의 레이블을 변경하면 범위에서 제거되거나 추가시킬 수 있다. 파드는 metadata.ownerReferences 필드에서 레플리케이션컨트롤러 참조 정보를 확인할 수 있다. 파드의 레이블을 변경하여 범위에서 제거하면 수동으로 만든 다른 파드처럼 변경된다.레플리케이션 컨트롤러가 관리하는 파드에 레이블 추가 관리하지 않는 레이블이라면 아무런 영향이 없다.관리되는 파드의 레이블 변경 레플리케이션컨트롤러에서 관리하지 않는 레이블로 변경하게 되면 범위에서 제거된것으로 간주하고 레플리케이션컨트롤러는 새로운 파드를 생성한다.컨트롤러에서 파드를 제거하는 실제 사례 특정 파드에서만 어떤 작업을 하려는 경우 레이블을 변경하여 범위에서 제거하면 작업이 수월해질 수 있다. 오동작하는 파드가 하나 있을때 이를 범위 밖으로 빼내고(이때 레플리케이션컨트롤러에 의해 레플리카수는 유지될것이다.) 원하는 방식으로 파드를 디버그하거나 문제를 재연해볼 수 있다.레플리케이션컨트롤러의 레이블 셀렉터 변경 컨트롤러의 레이블을 변경하면 모든 파드들이 범위를 벗어나게 되므로 새로운 N개의 파드를 생성하게 된다.4.2.5 파드 템플릿 변경 템플릿을 변경하는것은 쿠키 커터를 다른것으로 교체하는 것과 같다. 나중에 잘라낼 쿠키에만 영향을 줄뿐 이미 잘라낸 쿠키에는 아무런 영향을 미치지 않는다.템플릿을 수정하는 방법kubectl edit rc kubia 기본 텍스트 편집기에서 레플리케이션컨트롤러의 YMAL 정의가 열려서 이를 수정할 수 있다. KUBE_EDITOR 환경변수를 설정해서 텍스트 편집기를 커스텀할 수 있다.4.2.6 수평 파드 스케일링레플리케이션컨트롤러 스케일 업(확장) / 스케일 다운(축소)# 확대kubectl scale rc kubia --replicas=10# 조회kubectl get rc# 축소kubectl scale rc kubia --replicas=3 위와 같이 수정을 하게 되면 레플리케이션컨트롤러가 업데이트되고 즉시 파드 수가 10개로 확장되었다가 다시 3개로 축소된다.스케일링에 대한 선언적 접근 방법 이해 쿠버네티스에게 무엇을 어떻게 하라고 말하는게 아니라 의도하는 상태로 변경하는 것뿐. 쿠버네티스는 상태를 보고 상태에 맞게 조정한다.4.2.7 레플리케이션컨트롤러 삭제 kubectl delete를 통해 컨트롤러를 삭제하면 파드도 같이 삭제된다.# 파드와 컨트롤러 모두 삭제kubectl delete rc kubia# 파드를 삭제하지 않고 레플리케이션 컨트롤러를 삭제하는 방법kubectl delete rc kubia --cascade=false4.3 레플리카셋 레플리카셋은 레플리케이션컨트롤러와 유사한 리소스이고 레플리케이션컨트롤러를 대체하기 위한 나온 리소스이다. 일반적으로는 레플리카셋을 직접 생성하지는 않고 상위 수준의 디플로이먼트 리소스를 생성하면 자동으로 생성된다.4.3.1 레플리카셋과 레플리케이션컨트롤러 비교 레플리카셋이 좀 더 풍부한 파드 셀렉터 표현식을 사용할 수 있다. 특정 레이블이 없는 파드나 레이블의 값과 상관없이 특정 레이블의 키를 갖는 파드를 매칭시킬 수도 있다. 또한 하나의 레플리카셋으로 두 파드 세트를 모두 매칭시켜 하나의 그룹으로 취급하는것도 가능하다. 위 부분을 제외하고는 다르지 않음.4.3.2 레플리카셋 정의하기apiVersion: v1kind: ReplicaSetmetadata: name: kubiaspec: replicas: 3 selector:# matchLabels:# app: kubia matchExpressions: - key: app operator: In values: - kubia template: metadata: labels: app: kubia spec: containers: - name: kubia image: luksa/kubiaAPI 버전 속성 API 그룹, API 버전으로 구분되며 “apps/v1bet2”인 경우 그룹은 apps, 버전은 v1beta2이다. core API 그룹에 속할 경우에는 그룹을 명시하지 않아도 된다. (v1)4.3.3. 레플리카셋 생성 및 검사 레플리카셋 생성은 api v1으로만 해야한다.(버전업되면서 변경됨)# 레플리카셋 생성kubectl create -f kubia-replicaset.yaml# 레플리카셋 조회kubectl get rs4.3.4 레플리카셋의 더욱 표현적인 레이블 셀렉터 In은 레이블의 값이 지정된 값 중 하나와 일치해야 함. NotIn은 레이블의 값이 지정된 값과 일치하지 않아야 함. Exists는 파드의 지정된 키를 가진 레이블이 포함되어야 한다.(값은 관계없기에 value 필드를 지정하지 않는다.) DoesNotExists는 파드에 지정된 키를 가진 레이블이 포함돼 있지 않아야 한다.(value 지정 X)4.3.5 레플리카셋 삭제kubectl delete rs kubia4.4. 데몬셋 데몬셋을 사용하면 각 노드에서 정확히 한 개의 파드만 실행시킬 수 있다. 레플리카셋은 노드는 관계없이 지정된 수만큼 파드를 실행하는데 데몬셋은 이런 수를 지정하는것이 없고 클러스터의 모든 노드에 노드당 하나의 파드만 실행시키는 리소스이다. 시스템 수준의 작업을 수행하는 인프라 관련 파드가 있다고 하면 데몬셋이 가장 적합할것이다. 다른 예는 kube-proxy 프로세스가 데몬셋의 예이다. 서비스를 작동시키기 위해 모든 노드에서 실행되어야 하기 때문이다.4.4.1 데몬셋으로 모든 노드에 파드 실행하기 모든 클러스 노드마다 파드를 하나만 실행하고자 할때 사용하면 되는 리소스이다. 만약 하나의 노드가 다운되더라도 다른곳에서 파드를 생성하지 않고, 새로운 노드가 클러스터에 추가되면 즉시 새 파드 인스턴스를 해당 노드에 배포한다.4.4.2 데몬셋을 사용해 특정 노드에서만 파드 실행하기 파드가 노드의 일부에서만 실행되도록 지정하지 않으면 데몬셋은 클러스터 모든 노드에 파드를 배포한다. 하지만 파드 템플릿에서 node-Selector 속성을 지정하면 특정 노드에만 배포할 수 있다.데몬셋과 파드 스케줄링 쿠버네티스를 이용하면 노드에 스케줄링 되지 않게 해서 파드가 노드에 배포되지 않도록 할수도 있는데 이는 스케줄링 기반으로 동작하는 처리방식이다. 데몬셋이 관리하는 파드의 경우는 스케줄러와는 무관하기 때문에 스케줄링이 되지 않는 노드에서도 파드가 실행된다.데몬셋 적용 예제 노드에 레레이블을 지정하여 노드2를 제외한곳에만 데몬셋 파드가 실행되도록 처리한 예제이다.데몬셋 YAML 정의 apiVerison이 쿠버네티스 업데이트 따라 변경되어서 “apps/v1” 을 이용해야 함.apiVersion: apps/v1kind: DaemonSetmetadata: name: ssd-monitorspec: selector: matchLabels: app: ssd-monitor template: metadata: labels: app: ssd-monitor spec: nodeSelector: disk: ssd containers: - name: main image: luksa/ssd-monitor데몬셋 생성# 데몬셋 생성kubectl create -f ssd-monitor-daemonset.yaml# 데몬셋 조회kubectl get ds노드 레이블 추가 및 삭제# ssd 레이블 노드에 추가kubectl label node minikube disk=ssd# 데몬셋 pod 조회(추가됨)kubectl get po# ssd 레이블 노드를 hdd로 변경kubectl label node minikube disk=hdd --overwrite# 데몬셋 pod 조회(제거됨)kubectl get po4.5 Job 리소스 완료 가능한 단일 태스크를 수행하는 파드를 실행하기 위한 리소스로 Job이 있다. 완료 가능한 단일 태스크에서는 프로세스가 종료된 후에 다시 시작되지 않는다.4.5.1 Job리소스 특징 다른 리소스와 유사하지만 잡은 파드의 컨테이너 내부에서 실행중인 프로세스가 성공적으로 완료되면 컨테이너를 다시 시작하지 않는 파드를 실행시킬 수 있다. 작업이 재대로 완료되는 것이 중요한 임시 작업에 유용하다. 이러한 잡 리소스에 정의하기에 좋은 예로는 데이터를 어딘가에 저장하고 있고, 이 데이터를 변환해서 어딘가로 전송하는 케이스를 들수 있다. 잡에서 과관리하느 파드는 성공적으로 끝날 때까지 다시 스케줄링이 된다.4.5.2 잡 리소스 정의apiVersion: batch/v1 # batchAPI그룹의 버전을 선택해야 한다.kind: Jobmetadata: name: batch-jobspec: template: metadata: labels: app: batch-job spec: restartPolicy: OnFailure # 재시작 정책을 사용할 수 있음.(Always, OnFailure, Naver) containers: - name: main image: luksa/batch-job4.5.3 파드를 실행한 잡 확인# Job 생성kubectl create -f batch-job.yaml# Job 확인kubectl get jods# Job pod 확인 (completed된 job도 표시됨)kubectl get po# 로그 확인kubectl logs batch-job-9dhsc# job 삭제(job을 삭제하면 pod도 삭제된다)kubectl delete job batch-job완료된 파드를 삭제하지 않는 이유 파드가 완료될 때 파드가 삭제되지 않는 이유는 해당 파드의 로그를 검사할 수 있도록 하기 위함이다.4.5.4 잡에서 여러 파드 인스턴스 실행하기 2개 이상의 파드 인스턴스를 생성해 병렬 또는 순차 처리를 구성할 수 있다.(completions, parallelism 속성 이용)순차적으로 잡 파드 실행하기 spec에 completions값 지정 이렇게 처리하면 5개의 파드가 성공적으로 완료될 때까지 과정을 계속한다. 중간에 실패하는 파드가 있다면 잡은 새 파드를 생성하여 실제 5개 이상의 파드가 생성될 수 있다.apiVersion: batch/v1kind: Jobmetadata: name: multi-completion-batch-jobspec: completions: 5 template: metadata: labels: app: batch-job spec: restartPolicy: OnFailure containers: - name: main image: luksa/batch-job병렬로 잡 파드 실행하기 parallelism을 2로 설정하면 동시에 2개의 파드가 생성되어 병렬처리로 실행된다.apiVersion: batch/v1kind: Jobmetadata: name: multi-completion-batch-jobspec: completions: 5 parallelism: 2 template: metadata: labels: app: batch-job spec: restartPolicy: OnFailure containers: - name: main image: luksa/batch-job잡 스케일링 잡이 실행되는 동안 parallelism 속성을 변경하면 동시에 처리되는 파드 수를 조절할 수 있다. kubectl scale job multi-completion-batch-job --replicas 3 4.5.5 잡 파드가 완료되는데 걸리는 시간 제한하기 및 재시도 횟수 설정 activeDeadlineSeconds 속성을 설정하면 파드의 실행 시간에 제한을 두고 시간을 넘어서면 잡을 실패한것으로 처리할수도 있다. backoffLimit 필드를 지정해 실패한것으로 표시되기 전에 잡을 재시도할수 있는 횟수도 설정할 수 있다.(기본값 6)4.6 크론잡(CronJon) 잡을 주기적으로 또는 한번만 실행되도록 스케줄링하기 많은 배치 잡이 미래의 특정 시간 또는 지정된 간격으로 반복 실행해야 한다. 쿠버네티스를 이를 지원하기 위한 크론잡 리소스 기능을 제공한다.4.6.1 크론잡 정의apiVersion: batch/v1beta1kind: CronJobmetadata: name: batch-job-every-fifteen-minutesspec: schedule: \"0,15,30,45 * * * *\" # 매일 매시간 0,15,30,45분에 실행되는 cronJob jobTemplate: spec: template: metadata: labels: app: periodic-batch-job spec: restartPolicy: OnFailure containers: - name: main image: luksa/batch-job크론잡 생성 및 확인# 크론잡 생성kubectl create -f cronjob.yaml# 크론잡 확인kubectl get cronjob# 크론잡 삭제kubectl delete cronjob batch-job-every-fifteen-minutes4.6.2 스케줄된 잡의 실행 방법 이해 예정된 시간을 너무 초과해서 시작되서는 안되는 엄격한 요구사항이 요구될떄도 있는데 이를 위한 옵션을 제공한다. startingDeadlineSeconds 필드를 지정( 초단위) 일반적인 상황에서 크론잡은 스케줄에 설정한 각 실행에 항상 하나의 잡만 생성하지만, 2개의 잡이 동시에 생성되거나 전혀 생성되지 않을수도 있다. 이런 문제를 해결하기 위해 멱등성(한번 실행이 아니라 여러번 실행해도 동일한 결과가 나타나야함)가지도록 개발해야 한다. 이전에 누락된 잡 실행이 있다면 다음번 작업에서 해당 작업을 같이 수행해주도록 개발하는것이 좋다. 4.7 요약 컨테이너가 더 이상 정상적이지 않으면 즉시 쿠버네티스가 컨테이너를 다시 시작하도록 라이브니스 프로브를 지정할 수 있다. 직접 생성한 파드는 실수로 삭제되거나 실행중인 노드에 장애가 발생하거나 노드에서 퇴출되면 다시 생성되지 않기 때문에 직접 생성해서 사용하면 안된다. 레플리케이션컨트롤러는 의도하는 수의 파드 복제본을 항상 실행 상태로 유지해준다. 파드를 수평으로 스케일링(확장)하려면 쉽게 레플리케이션컨트롤러에 의도하는 레플리카 수를 변경하는 것만으로도 가능하다. 파드는 레플리케이션컨트롤러가 소유하지 않으며, 필요한 경우 레플리케이션컨트롤러간에 이동할 수 있다. 템플릿을 변경해도 기존의 파드에는 영향을 미치지 않는다. 레플리카셋과 디폴로이먼트로 교체해야 하며, 레플리카셋과 디폴로이먼트는 동일한 기능을 제공하면서 추가적인 강력한 기능을 제공한다. 레플리카셋은 임의의 클러스턴 ㅗ드에 파드를 스케줄링하는 반면, 데몬셋은 모든 노드에 데몬셋이 정의한 파드의 인스턴스 하나만 실행되도록 한다. 배치 작업을 수행하는 파드는 쿠버네티스의 잡 리소스로 생성해야 한다. 특정 시점에 주기적으로 수행해야 하는 잡은 크론잡 리소스를 통해 생성할 수 있다.Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 3. 파드 : 쿠버네티스에서 컨테이너 실행", "url": "/posts/devlog-platform-kubernetes-in-action3/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-03 17:34:00 +0900", "snippet": "3. 파드 : 쿠버네티스에서 컨테이너 실행3.1 파드 소개 파드는 함께 배치된 컨테이너 그룹이며, 쿠버네티스의 기본 빌딩 블록. 컨테이너를 개별적으로 배포하기보다는 컨테이너를 가진 파드를 배포하고, 운영한다. 무조건 2개 이상을 컨테이너를 포함시키라는 의미는 아니고 일반적으로는 하나의 컨테이너만 포함된다. 파드의 핵심은 파드가 여러 컨테이너를 ...", "content": "3. 파드 : 쿠버네티스에서 컨테이너 실행3.1 파드 소개 파드는 함께 배치된 컨테이너 그룹이며, 쿠버네티스의 기본 빌딩 블록. 컨테이너를 개별적으로 배포하기보다는 컨테이너를 가진 파드를 배포하고, 운영한다. 무조건 2개 이상을 컨테이너를 포함시키라는 의미는 아니고 일반적으로는 하나의 컨테이너만 포함된다. 파드의 핵심은 파드가 여러 컨테이너를 가지고 있을 경우 모든 컨테이너는 항상 하나의 워커 노드에서 실행되고, 여러 워커 노드에 걸쳐 실행되지 않는다는것.3.1.1 파드가 필요한 이유 여러 프로세스를 실행하는 단일 컨테이너보다 다중 컨테이너가 좋기 때문 쿠버네티스에서는 프로세스를 항상 컨테이너에서 실행시키고, 각각의 컨테이너는 격리된 머신과 비슷하다. 그래서 여러 프로세스를 단일 컨테이너 안에서 실행하는것이 타당한것처럼 보일수 있는데. 컨테이너는 단일 프로세스를 실행하는 것을 목적으로 설계되었음. 그래서 각 프로세스를 자체의 개별 컨테이너로 실행해야 한다. 하지만 실제 서비스 상황에서는 여러개의 컨테이너를 포함시킬 수 있는 개념이 필요할 수 있음. 예를 들면 WAS를 쿠버네티스로 서빙한다고 했을떄 쿠버네티스 스케줄러에 의해서 pod이 존재하는 노드는 언제든 수시로 변경될 수 있는데 이 경우 로그가 유실될 수 있기 때문에 이런 로그를 유실되지 않기 위해서는 로그를 중앙저장소로 보내주는 별도의 프로세스가 있어야 함. 이 경우 파드 구성을 WAS 컨테이너 + 로그전송 프로세스 컨테이너로 하여 하기 위함이 아닌가 싶음. 3.1.2 파드 이해하기 여러 프로세스를 단일 컨테이너로 묶지 않기 때문에 컨테이너를 함께 묶고 하나의 단위로 관리할 수 있는 또 다른 상위 구조가 필요하다. (파드가 필요한 이유) 컨테이너가 제공하는 모든 기능을 활용하는 동시에 프로세스가 함께 실행되는것처럼 보이게 할수 있음.같은 파드에서 컨테이너 간 부분 격리 쿠버네티스는 파드 안에 있는 모든 컨테이너가 자체 네임스페이스가 아닌 동일한 리눅스 네임스페이스를 공유하도록 도커를 설정 즉 동일한 네트워크 네임스페이스, UTS(Unix Timesharing System, 호스트 이름을 네임스페이스로 격리) 안에서 실행되므로 같은 파드 내에서 이것들을 서로 공유할 수 있다. 여기서 주의할 점은 파일시스템같은 경우는 컨테이너 이미지에서 나오기 때문에 기본적으로 완전히 분리된다고 보면 된다. 쿠버네티스 볼륨 개념을 이용하면 컨테이너간 파일 디렉터리를 공유하는것도 가능컨테이너가 동일한 IP와 포트 공간을 공유하는 방법 파드 안의 컨테이너가 동일한 네트워크 네임스페이스에서 실행되기 때문에 도일한 IP주소와 포트공간을 공유한다. 그래서 동일한 파드 안 컨테이너에서 실행되는 프로세스들은 같은 포트번호를 사용하지 않도록 해야 한다. 파드 안의 컨테이너들끼리는 로컬호스트를 통해 서로 통신이 가능(동일한 루프백 네트워크 인터페이스를 갖음)파드간 플랫 네트워크 소개 모든 파드는 하나의 플랫한 공유 네트워크 주소 공간에 상주 모든 파드는 다른 파드의 IP주소를 사용해 접근이 가능하다. LAN과 유사한 방식으로 상호 통신이 가능 파드는 논리적인 호스트로서 컨테이너가 아닌 환경에서의 물리적 호스트 혹은 VM과 매우 유사하게 동작함.3.1.3 파드에서 컨테이너의 적절한 구성다계층 애플리케이션을 여러 파드로 분할 프론트엔드와 백엔드 예제에서 파드를 2개로 분리하면 쿠버네티스가 프론트엔드를 한 노드로 백엔드는 다른 노드에 스케줄링해서 인프라스트러겨의 활용도를 향상시킬수 있음. 이게 만약 같은 파드에 있다면 둘중 항상 같은 노드에서 실행되겠지만, 서로 필요한 리소스의 양은 다를수 있기에 분리하는것이 좋다.개별 확장이 가능하도록 여러 파드로 분할 두 컨테이너를 하나의 파드에 넣지 말아야하는 이유 중 하나는 “스케일링” 때문 쿠버네티스 스케줄링 기본 단위는 파드이다. 쿠버네티스에서는 컨테이너 단위로 수평 확장할수 없고 파드 단위로만 수평확장(scale out)이 가능하다. 컨테이너를 개별적으로 스케일링하는 것이 필요하다고 판단되는 경우 별도 파드에 배포하자.파드에서 여러 컨테이너를 사용하는 경우 애플리케이션이 하나의 주요 프로세스와 하나 이상의 보안 프로세스로 구성된 경우 특정 디렉터리에서 파일을 제공하는 웹서버로 예를 들자면 주 컨테이너는 웹서버라고 하고, 추가 컨테이너(사이드카 컨테이너)는 외부 소스에서 주기적으로 콘텐츠를 받아 웹서버 디렉터리에 저장하는 방식을 예로 들수 있다.(ex. sitemap을 받아와서 디렉터리에서 저장하는 것과 같은) 다른 에제로는 로그 로테이터와 수집기, 데이터 프로세서, 통신 어댑터 등이 있을수 있다.파드에서 여러 컨테이너를 사용하는 경우 결정 컨테이너를 함께 실행해야 하는가? 혹은 서로 다른 호스트에서 실행할 수 있는가? 여러 컨테이너가 모여 하나의 구성 요소로 나타내는가? 혹은 개별적인 구성 요소인가? 컨테이너가 함꼐, 혹은 개별적으로 스케일링돼야 하는가? 컨테이너는 여러 프로세스를 실행시키지 말아야하고, 파드는 동일한 머신에서 실행할 필요가 없다면 여러 컨테이너를 포함하지 말아야 한다.3.2 YAML 또는 JSON 디스크립터로 파드 생성 kubectl run 명령어를 이용해서 리소스를 만들수도 있지만, yaml 파일에 모든 쿠버네티스 오브젝트를 정의하면 버전 관리 시스템에 넣는것도 가능하고, 모든 이점을 누릴수 있다.3.2.1 기존 파드의 YAML 디스크립터 살펴보기# 메타 데이터apiVersion: v1kind: Podmetadata: creationTimestamp: \"2020-08-03T14:21:44Z\" generateName: kubia- labels: run: kubia name: kubia-5wsx8 namespace: default ownerReferences: - apiVersion: v1 blockOwnerDeletion: true controller: true kind: ReplicationController name: kubia uid: cb311984-2974-4434-8979-b7a11f07388d resourceVersion: \"4551\" selfLink: /api/v1/namespaces/default/pods/kubia-5wsx8 uid: ed4f042e-f547-4003-9a89-8005bca02f60# 스펙spec: containers: - image: sungsu9022/kubia imagePullPolicy: Always name: kubia ports: - containerPort: 8080 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-zszt7 readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: docker-desktop priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: default-token-zszt7 secret: defaultMode: 420 secretName: default-token-zszt7status: conditions: - lastProbeTime: null lastTransitionTime: \"2020-08-03T14:21:44Z\" status: \"True\" type: Initialized - lastProbeTime: null lastTransitionTime: \"2020-08-03T14:21:49Z\" status: \"True\" type: Ready - lastProbeTime: null lastTransitionTime: \"2020-08-03T14:21:49Z\" status: \"True\" type: ContainersReady - lastProbeTime: null lastTransitionTime: \"2020-08-03T14:21:44Z\" status: \"True\" type: PodScheduled# Status containerStatuses: - containerID: docker://796528475e929b9f1ec28fc5ae3ca722f1279b8f852b262dfef549bbdebdcd7a image: sungsu9022/kubia:latest imageID: docker-pullable://sungsu9022/kubia@sha256:3811562a9c1c45f2e03352bf99cacf4e024f9c3a874a3d862f0208367eace763 lastState: {} name: kubia ready: true restartCount: 0 started: true state: running: startedAt: \"2020-08-03T14:21:48Z\" hostIP: 192.168.65.3 phase: Running podIP: 10.1.0.13 podIPs: - ip: 10.1.0.13 qosClass: BestEffort startTime: \"2020-08-03T14:21:44Z\"파드를 정의하는 주요 부분 소개 Metadata : 이름, 네임스페이스, 레이블 및 파드에 관한 기타 정보를 포함 Spec : 파드 컨테이너, 볼륨, 기타 데이터 등 파드 자체에 관한 실제 명세를 가진다. Status : 파드 상태, 각 컨테이너 설명과 상태, 파드 내부 IP, 기타 기본 정보 등 현재 실행중인 파드에 관한 정보 포함(새 파드를 만들때는 작성하지 않는 내용)3.2.2 파드를 정의하는 간단한 YAML 정의apiVersion: v1kind: Podmetadata: name: kubia-manual # 파드 이름spec: containers: - image: luksa/kubia # 컨테이너 이미 name: kubia # 컨테이너 이름 ports: - containerPort: 8080 # 애플리케이션 수신 포트 protocol: TCP컨테이너 포트 지정 스펙에 포트를 명시하지않아도 해당 파트에 접속할 수 있으나, 명시적으로 정의해주면 클러스터를 사용하는 모든 사람이 파드에 노출한 포트를 빠르게 볼 수 있어서 정의하는 것이 좋음.오브젝트 필드들에 대한 설명 확인(도움말같은)kubectl explain podskubectl explain pod.spec3.2.3 kubectl create 명령으로 파드 만들기 yaml 또는 json 파일로 리소스를 동일하게 만들 수 있음.kubectl create -f kubia-manual.yaml실행중인 파드의 전체 정의 가져오기kubectl get po kubia-manual -o yamlkubectl get po kubia-manual -o json3.2.4 애플리케이션 로그 보기 컨테이너 런타임(도커)은 스트림을 파일로 전달하고 아래 명령을 이용해 컨테이너 로그를 가져온다.docker log &lt;container id&gt; 쿠버네티스 logs를 이용해 파드 로그 가져오기 kubectl logs kubia-manual 파드에 컨테이너가 하나만 있다면 로그를 가져오는 것은 매우 간단함 컨테이너 로그는 하루 단위로, 로그 파일이 10MD 크기에 도달할때마다 로테이션 되기 떄문에 별도로 로그 관리하는 방식이 필요할듯컨테이너 이름을 지정해 다중 컨테이너 파드에서 로그 가져오기kubectl logs kubia-manual -c kubia 현재 존재하는 파드의 컨테이너 로그만 가져올수 있음. 파드가 삭제되면 로그도 같이 삭제된다. 일반적인 서비스 환경에서는 클러스터 전체의 중앙집중식 로깅을 설정하는 것이 좋음3.2.5 파드에 요청 보내기 기존에 kubectl expose 명령으로 외부에서 파드에 접속할 수 있는 서비스를 만들었었는데 “포트 포워딩” 방식을 이용해 테스트와 디버깅 목적으로 연결할수 있음.로컬 네트워크 포트를 파드의 포트로 포워딩 서비스를 거치지 않고 특정 파드와 통신하고 싶을때 쿠버네티스는 해당 파드로 향하나느 포트 포워딩을 구성해준다. 아래와 같인 하면 localhost:8888로 해당 파드의 8080포트와 연결시킬 수 있다.kubectl port-forward kubia-manual 8888:80803.3 레이블을 이용한 파드 구성 파드 수가 증가함에 따라 파드를 부분집합으로 분류할 필요가 있음. MSA에서 배포된 서비스가 매우 많고 여기서 여러 버전 혹은 릴리스(안정, 베타 카나리 등)이 동시에 실행될수도 있는데 이러면 시스템에 수백개의 파드가 생길수 있고 이를 정리할 매커니즘이 필요한데 이것이 레이블이다. 레이블을 통해 파드와 기타 다른 쿰버네티스 오브젝트의 조직화를 할 수 있음(파드 뿐만 아니라 다른 쿠버네티스 리소스 가능)3.3.1 레이블 소개 파드와 모든 쿠버네티스 리소스를 조직화할 수 있는 단순하면서 강력한 기능 리소스에 첨부하는 키-값 쌍 레이블 셀렉터를 사용해 리소스를 선택할때 호라용 레이블 키가 해당 리소스 내에서 고유하다면 하나 이상 원하는 만큼의 레이블을 가질 수 있다.3.3.2 파드를 생성할 때 레이블 지정 kubia-manual-with-labels.yamlapiVersion: v1kind: Podmetadata: name: kubia-manual-v2 labels: # 레이블 2개 지정 creation_method: manual env: prodspec: containers: - image: luksa/kubia name: kubia ports: - containerPort: 8080 protocol: TCP# Pod 생성kubectl create -f kubia-manual-with-labels.yaml# pod label 정보 조회kubectl get po --show-labels# 특정 레이블에만 관심 있는 경우 -L 스위치를 지정해 각 레이블을 자체 열에 표시할 수 있음.kubectl get po -L creation_method,env3.3.3 기존 파드 레이블 수정# 기존 pods 레이블 추가kubectl label po kubia-manual creation_method=manual# pods 레이블 수정kubectl label po kubia-manual-v2 env=debug --overwrite# 레이블 조회kubectl get po -L creation_method,env3.4 레이블 셀렉터를 이용한 파드 부분 집합 나열 레이블이 중요한 이유는 레이블 셀렉터와 함꼐 사용된다는 것이다. 레이블 셀렉터는 리소스 중에서 다음 기준에 따라 리소르를 선택한다. 특정한 키를 포함하거나 포함하지 않는 레이블 특정한 키와 값을 가진 레이블 특정한 키를 갖고 있지만, 다른 값을 가진 레이블 3.4.1 레이블 셀렉터를 사용해 파드 나열# creation_method 레이블 manual을 가지고 있는 파드 나열kubectl get po -l creation_method=manual# env를 가지고 있는 파드 나열kubectl get po -l env# env를 가지고 있지 않은 파드 나열kubectl get po -l '!env'# creation_method 레이블을 가진 것중에 값이 manual이 아닌것kubectl get po -l 'creation_method!=manual'kubectl get po -l 'env in (prod,devel)'kubectl get po -l 'env notin (prod,devel)'3.4.2 레이블 셀렉터에서 여러 조건 사용# 셀렉터는 쉼표로 구분된 여러 기준을 포함하는 것도 가능하다.kubectl get po -l creation_method=manual,env=debug 레이블 셀렉터를 이용해 여러 파드를 한번에 삭제할수도 있음.3.5 레이블과 셀렉터를 이용해 파드 스케줄링 제한 파드 스케줄링할 노드를 결정할 떄 영향을 미치고 싶은 상황이 있을수 있음. SSD를 가지고 있는 워커 노드에 할당해야 하는 경우 이때 특정노드를 지정하는 방법은 좋은것은 아님(아마도 노드이름이나 별도의 정보를 기준으로 하는것을 말하는듯) 노드를 지정하는 대신 필요한 노드 요구사항을 기술하고 쿠버네티스가 요구사항을 만족하는 노드를 직접 선택하도록 해야 하는데 이때 노드 레이블과 레이블 셀렉터를 통해서 할 수 있다. SSD 노드가 2개가 있을떄 A노드로 지정을 해버리면 혹시 A노드에 문제가 생기면 스케줄링을 재대로 못할수 있다는 의미(만약 저런 지정이 없었다면 B로 스케줄링 되었을것) 3.5.1 워커 노드 분류에 레이블 사용 노드를 포함한 모든 쿠버네티스 오브젝트에 레이블을 부착할수 있기 때문에 노드 분류에 레이블을 사용하고 쿠버네티스가 스케줄링할때 활용할 수 있다.# 이런식으로 노드에 label을 붙인다.(로컬에서 docker-desktop밖에 없어서 직접해보지는 ㅇ낳음)kubectl label node gke-kubia-85f6-node-0rrx gpu=true# 노드 조회kubectl get nodes -l gpu=true3.5.2 특정 노드에 파드 스케줄링apiVersion: v1kind: Podmetadata: name: kubia-gpuspec: nodeSelector: gpu: \"true\" # gpu=true 레이블을 포함한 노드에 이 파드라 배포하도록 지시 containers: - image: luksa/kubia name: kubia3.5.3 하나의 특정 노드 스케줄링 nodeSelector에 실제 호스트 이름을 지정할 경우 해당 노드가 오프라인 상태인 경우 파드가 스케줄링 되지 않을수 있어서 레이블 셀렉터를 통해 지정한 특정 기준을 만족하는 노드의 논리적인 그룹으로 처리될 수 있도록 해야 한다. 추가로 노드에 파드를 스케줄링할 때 영향을 줄수 있는 방법은 16장에서 또 나옴3.6 파드에 어노테이션 달기 어노테이션은 키-값 쌍으로 레이블과 거의 비슷하지만 식별 정보로 사용되지 않음. 반면 훨씬 더 많은 정보를 보유할 수 있다. 쿠버네티스 새로운 기능 릴리스시 어노테이션을 사용하곤 한다. 어노테이션을 유용하게 사용하는 방법은 파드나 다른 API 오브젝트에 설명을 추가하는 것을 예를 들수 있음. 이렇게 하면 클러스터를 사용하는 모든 사람이 개별 오브젝트에 관한 정보를 쉽게 찾을수 있음. 3.6.1 오브젝트의 어노테이션 조회 레이블에 넣을 데이터는 보통 짧은 데이터이고, 어노테이션에는 상대적으로 큰 데이터를 넣을 수 있음# kubectl describe {리소스} {name}kubectl get po kubia-xxxxx -o yarml # 1.9부터 이 정보는 yaml안에서 확인할수 없도록 변경됨3.6.2 어노테이션 추가 및 수정# 어노테이션 추가kubectl annotate pod kubia-manual naver.com/someannotation=\"naver foo bar\"# describe을 통해 pod의 어노테이션 정보 조회kubectl describe pod kubia-manual3.7 네임스페이스를 사용한 리소스 그룹화 각 오브젝트는 여러 레이블을 가질 수 있기 때문에 오브젝트 그룹은 서로 겹쳐질 수 있다. 클러스터에서 작업을 수행할 때 레이블 셀렉터를 명시적으로 지정하지 않으면 항상 모든 오브젝트를 보게 된다. 이런 문제를 해결하기 위해 쿠버네티스는 네임스페이스 단위로 오브젝트를 그룹화한다. docker에서 이야기했던 리눅스 네임스페이스와는 별개임3.7.1 네임스페이스의 필요성 네임스페이스를 사용하면 많은 구성 요소를 가진 복잡한 시스템을 좀 더 작은 개별 그룹으로 분리할 수 있음. 멀티테넌트환경처럼 리소르르 분리하는 데 사용된다. 리소스를 프로덕션 개발, QA환경 혹은 원하는 다른 방법으로 나누어 사용할 수 있다. 단 노드의 경우는 전역 리소스이고, 단일 네임스페이스에 얽매이지 않는다.(이러한 이유는 4장에서 다룸)3.7.2 다른 네임스페이스와 파드 살펴보기 명시적으로 지정하지 않았다면 default 네임스페이스를 사용했을 것``` shkubectl get nskubectl get po -n kube-system - 쿠베 시스템에 대한 네임스페이스가 분리되어 있어서 뭔가 유저가 만든 리소스들과 깔끔하게 분리하여 관리할수 있음. - 매우 큰 규모에서 쿠버네티스를 사용한다면 네임스페이스를 사용해서 서로 관계없는 리소스를 겹치지 않는 그룹으로 분리할 수 있다. - 네임스페이스 기준으로 리소스 이름에 관한 접근범위를 제공하므로 리소스 이름 충돌에 대한 걱정도 할 필요 없음. - 이 외에도 특정 사용자에 대한 리솟 ㅡ접근 권한을 관리할수도 있고, 개별 사용자가 사용할 수 있는 컴퓨팅 리소스를 제한하는 데에도 사용된다.### 3.7.3 네임스페이스 생성``` yamlapiVersion: v1kind: Namespacemetadata: name: custom-namespace# yaml 파일을 이용한 네임스페이스 생성kubectl create -f custom-namespace.yaml# 커맨드라인 명령어로 생성kubectl create namespace custom-namespace3.7.4 다른 네임스페이스의 오브젝트 관리 생성한 네임스페이스 안에 리소스를 만들기 위해서는 metadata 섹션에 namespace항목을 넣거나 kubectl create 명령시 네임스페이스를 지정하면 된다.# custom-namespace에 pod 생성kubectl create -f kubia-manual.yaml -n custom-namespace 네임스페이스 context 이동# custom-namespace 네임스페이스로 컨텍스트 변경kubectl config set-context --current --namespace=custom-namespace# default 네임스페이스로 컨텍스트 변경kubectl config set-context --current --namespace=default3.7.5 네임스페이스가 제공하는 격리이해 실행중인 오브젝트에 대한 격리는 제공하지 않음. 네임스페이스에서 네트워크 격리를 제공하는지는 쿠버네티스와 함께 배포하는 네트워킹 솔루션에 따라 다름. 서로 다른 네임스페이스 안에 있는 파드끼리 서로 IP주소를 알고있다면 HTTP 요청과 같은 트래픽을 다른 파드로 보내는데에는 전혀 제약이 없다.3.8 파드 중지와 제거 파드를 삭제하면 쿠버네티스 파드 안에 있는 모든 컨테이너를 종료하도록 지시 이때 SIGTERM 신호를 프로세스에 보내고, 지정된 시간(기본 30초)동안 기다린다. 시간 내에 종료되지 않으면 SIGKILL 신호를 통해 종료시킨다. 프로세스가 항상 정상적으로 종료되게 하기 위해서는 SIGTERM 신호를 올바르게 처리해야 한다.3.8.1 이름으로 파드 제거# 이름으로 삭제kubectl delete po kubia-gpu# 공백을 구분자로 여러개를 한번에 지울수 있음.kubectl delete po kubia-gpu kubia-gpu23.8.2 레이블 셀렉터를 이용한 파드 삭제# 레이블 셀렉터를 이요한 삭제kubectl delete po -l creation_method=manualkubectl delete po -l rel=canary3.8.3 네임스페이스를 삭제한 파드 제거kubectl delete ns custom-namespace3.8.4 네임스페이슬 유지하면서 네임스페이스 안에 모든 파드 삭제 여기서 파드를 삭제하더라도 레플리케이션컨트롤러로 생긴 파드가 있었다면 삭제하더라도 다시 스케줄링 될것(이 경우 파드를 삭제하기 위해서는 리플리케이션컨트롤러도 같이 삭제해야 한다. kubectl delete po --all 3.8.5 네임스페이스에서 (거의) 모든 리소스 삭제 all 키워드를 쓰면 대부분의 리소스는 삭제할 수 있지만, 7장에 나올 시크릿 같은 특정 리소스는 그대로 보존되어 있다. 이 키워드를 통해 kubernetes 서비스도 삭제가 가능하지만 잠시 후에 다시 생성된다. kubectl delete all --all 3.9 요약 특정 컨테이너를 파드로 묶어야 하는지 여부를 결정하는 방법 파드는 여러 프로세스를 실행할 수 있으며 컨테이너가 아닌 세계의 물리적 호스트와 비슷하다. yaml 또는 json 디스크립터를 작성해 파드를 작성하고 파드 정의와 상태를 확인할 수 있다. 레이블과 레이블 셀렉터를 사용해 파드를 조직하고 한번에 여러 파드에서 작업을 쉽게 수행할 수 있다. 노드 레이블과 셀렉터를 사용해 특정 기능을 가진 노드에 파드를 스케줄링 할 수 있다. 어노테이션을 사용하면 사람 또는 도구, 라이브러리에서 더 큰 데이터를 파드에 부착할 수 있다. 네임스페이스는 다른 팀들이 동일한 클러스터를 별도 클러스터를 사용하는 것처럼 이용할 수 있게 해준다. kubectl explain 명령으로 쿠버네티스 리소스 도움말을 볼 수 있음.Reference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 2. 도커와 쿠버네티스 첫걸음", "url": "/posts/devlog-platform-kubernetes-in-action2/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-02 17:34:00 +0900", "snippet": "2. 도커와 쿠버네티스 첫걸음2.1 도커를 사용한 컨테이너 이미지 생성, 실행, 공유하기2.1.1 Hello World 컨테이너 실행하기docker run busybox echp \"Hello world\"백그라운드에 일어난 동작 이해하기 docker run 명령을 수행했을 떄 일어나는 일들2.1.2 간단한 node.js 애플리케이션 생성 예제 코드 ...", "content": "2. 도커와 쿠버네티스 첫걸음2.1 도커를 사용한 컨테이너 이미지 생성, 실행, 공유하기2.1.1 Hello World 컨테이너 실행하기docker run busybox echp \"Hello world\"백그라운드에 일어난 동작 이해하기 docker run 명령을 수행했을 떄 일어나는 일들2.1.2 간단한 node.js 애플리케이션 생성 예제 코드 있음.2.1.3 이미지를 위한 DockerFile 생성FROM node:7ADD app.js /app.jsENTRYPOINT [\"node\", \"app.js\"]2.1.4 컨테이너 이미지 생성 도커 이미지 조회 docker images 도커 이미지 생성 docker buld -t kubia . 2.1.5 컨테이너 이미지 실행 컨테이너 이미지 실행docker run --name kubia-container -p 8080:8080 -d kubiacurl localhost:8080 실행중인 컨테이너 조회docker ps 컨테이너에 관한 추가 정보 얻기docker inspect kubia-contaniner[ { \"Id\": \"d9e72c4f7867bab17b77faaefc0148e3ce430a9335c0578c199a66efd2ba6fc2\", \"Created\": \"2020-08-03T14:04:38.901107275Z\", \"Path\": \"node\", \"Args\": [ \"app.js\" ], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 2660, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2020-08-03T14:04:39.219869415Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" }, \"Image\": \"sha256:5b3381f90920e1c043628736040dca33fefeff5df30a14376e0d387e51439e0d\", \"ResolvConfPath\": \"/var/lib/docker/containers/d9e72c4f7867bab17b77faaefc0148e3ce430a9335c0578c199a66efd2ba6fc2/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/d9e72c4f7867bab17b77faaefc0148e3ce430a9335c0578c199a66efd2ba6fc2/hostname\", \"HostsPath\": \"/var/lib/docker/containers/d9e72c4f7867bab17b77faaefc0148e3ce430a9335c0578c199a66efd2ba6fc2/hosts\", \"LogPath\": \"/var/lib/docker/containers/d9e72c4f7867bab17b77faaefc0148e3ce430a9335c0578c199a66efd2ba6fc2/d9e72c4f7867bab17b77faaefc0148e3ce430a9335c0578c199a66efd2ba6fc2-json.log\", \"Name\": \"/kubia-contaniner\", \"RestartCount\": 0, \"Driver\": \"overlay2\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"\", \"ExecIDs\": null, \"HostConfig\": { \"Binds\": null, \"ContainerIDFile\": \"\", \"LogConfig\": { \"Type\": \"json-file\", \"Config\": {} }, \"NetworkMode\": \"default\", \"PortBindings\": { \"8080/tcp\": [ { \"HostIp\": \"\", \"HostPort\": \"8080\" } ] }, \"RestartPolicy\": { \"Name\": \"no\", \"MaximumRetryCount\": 0 }, \"AutoRemove\": false, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"Capabilities\": null, \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"private\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DeviceRequests\": null, \"KernelMemory\": 0, \"KernelMemoryTCP\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": null, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0, \"MaskedPaths\": [ \"/proc/asound\", \"/proc/acpi\", \"/proc/kcore\", \"/proc/keys\", \"/proc/latency_stats\", \"/proc/timer_list\", \"/proc/timer_stats\", \"/proc/sched_debug\", \"/proc/scsi\", \"/sys/firmware\" ], \"ReadonlyPaths\": [ \"/proc/bus\", \"/proc/fs\", \"/proc/irq\", \"/proc/sys\", \"/proc/sysrq-trigger\" ] }, \"GraphDriver\": { \"Data\": { \"LowerDir\": \"/var/lib/docker/overlay2/a3a88ce5bf878e4f64d8ae9e7db231c4b6e66b32b288ef882a2f105423eb4a74-init/diff:/var/lib/docker/overlay2/64e81c4fc48f121c861dbe5cc5cb5022a2ec2f7574e82e5f8948e04c87746b81/diff:/var/lib/docker/overlay2/5d2ddb4be75ae0be6e92a332006d173a1f20b92ec8688b10d6ac01bee176c542/diff:/var/lib/docker/overlay2/1a703255cdcdd1f16f5a3bd8a4b998be442fdb25c7a45108df017f1bb6c8960f/diff:/var/lib/docker/overlay2/f65dcb07c2b440dd4da92dc5536ee68a0952023878e59b335554a5d4ca9fdc8d/diff:/var/lib/docker/overlay2/57db638f03c99ab27a1426cc52ccc8911b103a5128a81b56fee29852dafe65bd/diff:/var/lib/docker/overlay2/af3ec0c3056b0a7ce999eff9a10084409f532b44e27418e736901749621ce194/diff:/var/lib/docker/overlay2/81250fe4e2e75f440c5aa17df795342744a5e70cd0e90ee4b54f1a20b36504aa/diff:/var/lib/docker/overlay2/5f0ac8d1e4a787d1229a165e4ea65375daa78d49a0ec9ec933ca3e5a2f7f1feb/diff:/var/lib/docker/overlay2/37442cc111e0254803eb97bc76e3e04ed907fa92aeacd37c10cf4b68c9c9ed94/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/a3a88ce5bf878e4f64d8ae9e7db231c4b6e66b32b288ef882a2f105423eb4a74/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/a3a88ce5bf878e4f64d8ae9e7db231c4b6e66b32b288ef882a2f105423eb4a74/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/a3a88ce5bf878e4f64d8ae9e7db231c4b6e66b32b288ef882a2f105423eb4a74/work\" }, \"Name\": \"overlay2\" }, \"Mounts\": [], \"Config\": { \"Hostname\": \"d9e72c4f7867\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"ExposedPorts\": { \"8080/tcp\": {} }, \"Tty\": false, \"OpenStdin\": false, \"StdinOnce\": false, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", \"NPM_CONFIG_LOGLEVEL=info\", \"NODE_VERSION=7.10.1\", \"YARN_VERSION=0.24.4\" ], \"Cmd\": null, \"Image\": \"kubia\", \"Volumes\": null, \"WorkingDir\": \"\", \"Entrypoint\": [ \"node\", \"app.js\" ], \"OnBuild\": null, \"Labels\": {} }, \"NetworkSettings\": { \"Bridge\": \"\", \"SandboxID\": \"b3896d08952f17bf1357487b848f07f3479552c4ccfce4c21a3a4e4f9c0d05b7\", \"HairpinMode\": false, \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": { \"8080/tcp\": [ { \"HostIp\": \"0.0.0.0\", \"HostPort\": \"8080\" } ] }, \"SandboxKey\": \"/var/run/docker/netns/b3896d08952f\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"f07d0019848fb4f71c8be009ca864596dc3ca7a270264a07b0cbc2af9e7e430a\", \"Gateway\": \"172.17.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.17.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:11:00:02\", \"Networks\": { \"bridge\": { \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"316f7a8d294ea277f1380cd6eda453adc349411009ecaa7b9e622c1f0910df78\", \"EndpointID\": \"f07d0019848fb4f71c8be009ca864596dc3ca7a270264a07b0cbc2af9e7e430a\", \"Gateway\": \"172.17.0.1\", \"IPAddress\": \"172.17.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:11:00:02\", \"DriverOpts\": null } } } }]2.1.6 실행중인 컨테이너 내부 탐색하기# 컨테이너 내부 쉘 실행docker exec -it kubia-contaniner bash2.1.7 컨테이너 중지와 삭제# 컨테이너 중지docker stop kubia-contaniner# 컨테이너 삭제docker rm kubia-contaniner2.1.8 이미지 레지스트리에 이미지 푸시# 추가 태그로 이미지 태그 지정docker tag kubia sungsu9022/kubia# 도커 허브에 이미지 푸시docker push sungsu9022/kubia# 다른머신에서 이미지 실행하기docker run -p 8080:8080 -d sungsu9022/kubia2.2 쿠버네티스 설치하기 공식문서에 minikube로 시작하는 가이드가 있음. 그게 가장 간편해보임. docker desktop이 설치되어있다면 거기 옵션으로 kubernetes를 시작할수도 있음.2.2.3 쿠버네티스 자동완성 설정하기 …2.3 쿠버네티스 앱 실행# 파드 실행kubectl run kubia --image=sungsu9022/kubia --port=8080 --generator=run/v1# 파드 조회kubectl get pods# 서비스 오브젝트 생성kubectl expose rc kubia --type=LoadBalancer --name kubia-http# 서비스 조회kubectl get serviceskubectl get svc minikube에서는 서비스 지원하지 않아 외부 포트를 통해 서비스 접근해야 함.# replicationcontrollers 정보 조회kubectl get replicationcontrollers# replicationcontrollers 수늘리기kubectl scale rc kubia --replicas=3# 실행중인 노드까지 표시kubectl get pods -p wide# 파드 세부 정보 살펴보기kubectl describe pod kubia-5wsx8# dashboardminikube dashboardReference kubernetes-in-action kubernetes.io" }, { "title": "[kubernetes-in-action] 1. 쿠버네티스 소개", "url": "/posts/devlog-platform-kubernetes-in-action1/", "categories": "DevLog, kubernetes", "tags": "Infra, kubernetes, kubernetes-in-action", "date": "2020-08-01 17:34:00 +0900", "snippet": "1. 쿠버네티스 소개쿠버네티스 등장 배경 거대한 모놀리스 레거시 애플리케이션은 점차 마이크로 서비스라는 독립적으로 실행되는 더 작은 구성 요소로 세분화되고 있다.마이크로 서비스는 서루 분리돼 있기 떄문에 개별적으로 개발, 배포, 업데이트, 확장할 수 있다. 이로써 오늘날 급변하는 비즈니스 요구사항을 충족시킬 만큼 신속하게 자주 구성 요소를 변경할 수...", "content": "1. 쿠버네티스 소개쿠버네티스 등장 배경 거대한 모놀리스 레거시 애플리케이션은 점차 마이크로 서비스라는 독립적으로 실행되는 더 작은 구성 요소로 세분화되고 있다.마이크로 서비스는 서루 분리돼 있기 떄문에 개별적으로 개발, 배포, 업데이트, 확장할 수 있다. 이로써 오늘날 급변하는 비즈니스 요구사항을 충족시킬 만큼 신속하게 자주 구성 요소를 변경할 수 있게 되었다.하지만 배포 가능한 구성 요소 수가 많아지고 데이터 센터의 규모가 커지면서 전체 시스템을 원활하게 구성, 관리 유지하는 일이 점점 더 어려워졌다.이런 구성 요소의 서버 배포를 자동으로 스케줄링하고 구성, 관리, 장애 처리를 포함하는 자동화가 필요한데, 이것이 바로 “쿠버네티스”가 등장한 이유이다.쿠버네티스 어원 쿠버네티스는 조종사, 조타수(선박의 핸들을 잡고 있는 사람)를 뜻하는 그리스어쿠버네티스 쿠버네티스는 하드웨어 인프라를 추상화하고 데이터 센터 전체를 하나의 거대한 컴퓨팅 리소스로 제공한다. 실제 세세한 서버 정보를 알 필요 없이 애플리케이션 구성요소를 배포하고 실행할 수 있다.쿠버네티스를 이용하여 하드웨어에서 실행되는 수만 개의 애플리케이션을 일일이 알 필요가 없게 되었다.1.1 쿠버네티스와 같은 시스템이 필요한 이유1.1.1 모놀리스 애플리케이션에서 마이크로 서비스 전환 모놀리스 애플리케이션은 앱을 실행하는데 충분한 리소스를 제공할 수 있는 소수의 강력한 서버가 필요하다. 여기서 시스템의 증가하는 부하를 처리할수 있는 방법은 Scale up과 Scale out이 있는데..수직 확장(scale up) 시스템의 증가하는 부하를 처리하려고 하면 CPU, 메모리 등을 추가해 서버를 수직 확장(scale up)하거나 서버를 추가하는 방법 비교적 비용이 많이 들고 실제 확장에 한계가 있음.수평 확장(scale out) 애플리케이션의 복사본을 실행해 전체 시스템을 수평 확장(scale out)하는 방법 상대적으로 저렴하지만, 애플리케이션 코드의 큰 변경이 필요할 수도 있고, 항상 가능하지 않음.(예를 들면 RDBMS 같은 경우에 scale out은 불가하고, HA 구성을 위해서는 다른 방식으로 사용)마이크로서비스로 애플리케이션 분할 하나의 포르세스였던것을 독립적인 프로세스로 나누어서 실행하고, 잘 정의된 API로 상호 통신한다.(Resetful API를 제공하는 HTTP 또는 AMQP와 같은 비동기 프로토콜)마이크로서비스 확장 전체 시스템을 함께 확장해야 하는 모놀리스 시스템과 달리 마이크로 서비스 확장은 서비스별로 수행되므로 리소스가 더 필요한 서비스만 별도로 확장할 수 있으며 다른 서비스는 그대로 둬도 된다.마이크로서비스 배포 마이크로서비스에도 단점이 있는데, 구성요소가 많아지면 배포 조합의 수 뿐만 아니라 구성 요소 간의 상호 종속성 수가 훨씬 더 많아지므로 배포 관련 결정이 점점 더 어려워질 수 있음. 마이크로서비스는 여러 개가 서로 함께 작업을 수행하므로 서로를 찾아 통신해야 하는데, 서비스 수가 증가함에 따라 특히 서버 장애 상황에서 해야 할 일을 생각해봤을 때 전체 아키텍쳐 구성의 어려움과 오류 발생 가능성이 높아진다. 또한 실행 호출을 디버깅하고 추적하기 어려운 단점이 있을수 있는데 Zipkin과 같은 분산 추적 시스템으로 해결은 가능함.환경 요구 사항의 다양성 애플리케이션이 서로 다른 버전의 동일한 라이브러리를 필요로 하는 경우 애플리케이션 구성 요소간 종속성의 차이는 불가피하다. 동일한 호스트에 배포해야 하는 구성 요소 수가 많을수록 모든 요구사항을 충족시키려 모든 종속성을 관리하기가 더 어려워진다.1.1.2 애플리케이션에 일관된 환경 제공 애플리케이션을 실행하는 환경이 다른것은 사실 심각한 문제 중에 하나이다. 개발과 프로덕션 환겨 사이의 큰 차이 뿐만 아니라 각 프로덕션 머신간에도 차이가 있다. 이런 차이는 하드웨어에서 운영체제, 각 시스템에서 사용 가능한 라이브러리에 이르기까지 다양하다. 프로덕션 환경에서만 나타나는 문제를 줄이려면 애플리케이션 개발과 프로덕션이 정확히 동일한 환경에서 실행돼 운영체제, 라이브러리, 시스템 구성, 네트워킹 환경, 기타 모든 것이 동일한 환경을 만들 수 있다면 사실 이상적이다.1.1.3 지속적인 배포로 전환 : 데브옵스와 노옵스 과거 개발 팀의 업무는 애플리케이션을 만들고 이를 배포(CD)하고 관리하며 계속 운영하는 운영팀에 넘기는것(?, 회사마다 다름)이었다고 한다. 개발팀이 애플리케이션을 배포하고 관리하는 것이 모두가 낫다고 생각한다. 개발자, 품질보증(QA), 운영팀이 전체 프로세스에서 협업해야 하는데 이를 “데브옵스”라고 한다.개발자와 시스템 관리자 각자가 최고로 잘하는 것을 하게 하는것! 하드웨어 인프라를 전혀 알지 못하더라도 관리자를 거치지 않고 개발자는 애플리케이션을 직접 배포할 수 있다. 이는 가장 이상적인 방법이며, 노옵스(NoOps)라고 부른다, 쿠버네티스를 사용하면 이런것들을 해결할 수 있음. 하드웨어를 추상화하고 이를 애플리케이션 배포, 실행을 위한 플랫폼으로 제공함으로써, 개발자는 시스템 관리자의 도움 없이도 애플리케이션을 구성, 배포할 수 있으며, 시스템 관리자는 실제 실행되는 애플리케이션을 알 필요 없이 인프라를 유지하고 운영하는 데 집중할 수 있다. 1.2 컨테이너 기술 소개 쿠버네티스는 애플리케이션을 격리하는 기능을 제공하기 위해 리눅스 컨테이너 기술을 사용한다.쿠버네티스 자체를 깊이 파고들기 전에 먼저 컨테이너의 기본에 익숙해져야 한다.또한 도커나 rkt(rock-it)과 같은 컨테이너 기술이 어떤 문제를 해결하는지 이해해야 한다.1.2.1 컨테이너 이해 애플리케이션이 더 작은 수의 커다란 구성요소로만 이뤄진 경우 구성요소에 전용 가상머신을 제공하고 고유한 운영체제 인스턴스를를 제공해 환경을 격리할수 있다. 하지만 MSA로 인해 구성요소가 작아지면서 숫자가 많아지기 시작하면 하뒈어 리소스 낭비가 생길 수 있음. 일반적으로 각각의 가상머신을 개별적으로 구성하고 관리해야 해서 시스템 관리자의 작업량도 상당히 증가한다.리눅스 컨테이너 기술로 구성 요소 격리 가상머신을 사용해 각 마이크로서비스의 환경을 격리하는 대신 개발자들은 리눅스 컨테이너 기술로 눈을 돌림 컨테이너 기술은 가상머신과 유사하게 서로 격리하지만 오버헤드가 훨씬 적음. 컨테이너에서 실행되는 프로세스는 다른 모든 프로세스와 마찬가지로 호스트 운영체제 내에서 실행된다.(가상머신은 별도의 운영체제에서 실행됨)컨테이너와 가상머신 비교컨테이너 컨테이너는 훨씬더 가벼워서 동일한 하드웨어 스펙으로 더 많은 수의 소프트웨어 구성 요소를 실행할 수 있음. 호스트 OS에서 실행되는 하나의 격리된 프로세스일뿐, 앱이 소비하는 리소스만 소비하고 추가 프로세스의 오버헤드가 없음. 호스트 OS에서 실행되는 동일한 커널에서 시스템 콜을 사용한다. 어떠한 종류의 가상화도 필요없음. 각각의 컨테이너가 모두 동일한 커널을 호출함으로 보안 위협이 발생할 수 있다. 컨테이너를 실행할때 VM처럼 부팅할 필요가 없고, 즉시 프로세스를 실행시킬 수 있음.가상머신 구성 요소 프로세스 뿐만 아니라 시스템 프로세스를 실행해야 하기 때문에 추가 컴퓨팅 리소스가 필요하다. 호스트에 가상머신 3개를 실행하면 3개의 완전히 분리된 운영체제가 실행되고 동일한 하드웨어를 공유한다. 시스템콜을 하이퍼 바이저를 통해 받아서 수행 OS를 필요로 하는 하이퍼바이저 타입이 있고, 호스트 타입을 필요로 하지 않는 타입이 있음. 독립적인 커널을 호출하므로 보안 위험이 컨테이너에 비해 적다.컨테이너 격리를 가능하게 하는 매커니즘 첫번째는 리눅스 네임스페이스로 각 프로세스가 시스템(파일, 프로세스, 네트워크 인터페이스, ㅎ스트 이름 등)에 대한 독립된 뷰만 볼 수 있도록 하는것 두번째는 리눅스 컨트롤 그룹(cgroups)으로 프로세스가 사용할 수 있는 리소스(CPU, 메모리, 네트워크 대역폭 등)의 양을 제한하는 것네임 스페이스의 종류 마운트(mnt) 프로세스 ID(pid) 네트워크(net) 프로세스간 통신(ipc) 호스트와 도메인 이름(uts - Unix Time Sharing) 사용자 ID(user)리눅스 네임스페이스로 프로세스 격리 여러 종류의 네임스페이스가 있기 때문에 프로세스는 하나의 네임스페이스에만 속하는 것이 아니라 여러 네임스페이스에 속할 수 있다. 각 네임스페이스는 특정 리소스 그룹을 격리하는데 사용됨. 2개의 서로 다른 UTS 네임스페이스를 프로세스에 각각 지정하면 서로 다른 로컬 호스트 이름을 보게할 수 도 있음.(두 프로세스는 마치 두 개의 다른 시스템에서 실행중인것처럼 보이게 할 수 있음.) 각 컨테이너는 고유한 네트워크 네임스페이슬르 사용하므로 각 컨테이너는 고유한 네트워크 인터페이스 세트를 보수 있음.프로세스의 가용 리소스 제한 프로세스의 리소스 사용을 제한하는 리눅스 커널 기능인 cgroups으로 이뤄진다.1.2.2 도커 컨테이너 플랫폼 소개 도커는 컨테이너를 여러 시스템에 쉽게 이식 가능하게 하는 최초의 컨테이너 시스템 앱, 라이브러리, 여러 종속성, 심지어 전체 OS 파일시스템까지도 도커를 실행하는 다른 컴퓨터에 애플리케ㅣ션을 프로비저닝하는 데 사용할 수 있다. 도커로 패키징된 앱을 실행하면 함께 제공된 파일 시스템 내용을 정확하게 볼수 있다. 앱은 실행중인 서버의 내용은 볼 수 없으므로 서버에 개발 컴퓨터와 다른 설치 라이브러리가 설치돼 있는지는 중요하지 않다. 가상머신에 운영체제를 설치하고 그 안에 앱을 설치한 다음 가상 머신 이미지를 배포하고 실행하는 가상머신 이미지를 만드는것과 유사함. 도커 기반 컨테이너 이미지와 가상머신 이미지의 큰 차이점은 컨테이너 이미지가 여러 이미지에서 공유되고 재사용될 수 있는 레이러로 구성되어 있다는것 동일한 레이러르 포함하는 다른 컨테이너 이미지를 실행할 때 다른 레이어에서 이미 다운로드된 경우 이미지의 특정 레이어만 다운로드 하면됨. 도커 개념 이해 도커는 앱을 패키징, 배포, 실행하기 위한 플랫폼전체 환경과 함꼐 패키지화할 수 있음.도커를 사용하여 패키지를 중앙 저장소로 전송할 수 있고, 이를 도커를 실행하는 모든 컴퓨터에 전송할 수 있음.이미지 애플리케션과 해당 환경을 패키지화한 것 앱에서 사용할 수 있는 파일시스템과 이미지가 실행될때 실행돼야 하는 실행파일 경로와 같은 메타데이터를 포함한다.레지스트리 도커 이미지를 저장하고 다른 사람이나 컴퓨터 간에해당 이미지를 쉽게 공유할 수 있는 저장소 ( push / pull)컨테이너 실행중인 컨테이너는 도커를 실행하는 호스트에서 실행되는 프로세스이지만 호스트와 호스트에서 실행중인 다른 프로세스와는 완전히 격리돼 있음. 리소스 사용이 제한돼 있으므로 할당된 리소스의 양(CPU, RAM 등)만 엑세스하고 사용할 수 있다.도커 이미지의 빌드 및 배포, 실행가상 머신과 도커 컨테이너 비교 가상머신에서 실행될 떄와 두 개의 별도 컨테이너로 실행될 때 앱 A,B가 동일한 바이너리, 라이브러리에 접근할 수 있음. 컨테이너는 격리된 자체 파일시스템이 있는데 이떄 같은 파일을 공유할 수 있음.이미지 레이어의 이해 모든 도커 이미지는 다른 이미지 위에 빌드되고, 2개의 다른 이미지는 동일한 부모 이미지를 사용할 수 있으므로, 서로 정확히 동일한 레이어가 포함될 수 있음. 레이어는 배포를 효율적으로 할 뿐만 아니라 이미지의 스토리지 공간을 줄이는 데에도 도움이 된다. 각 레이어는 동일 호스트에서 한번만 저장됨. 동일한 기본 레이러를 기반으로 한 2개의 이미지에서 생성한 2개의 컨테이너는 동일한 파일을 읽을 수 있지만, 그중 하나가 해당 파일을 덮어쓰면 다른 컨테이너에서는 그 변경 사항을 바라보지 않는다. 파일을 공유하더라도 여전히 서로 격리돼 있는데 이것은 컨테이너 이미지 레이어가 읽기 전용이기 때문 컨테이너가 실행될때 이미지 레이어 위에 새로운 쓰기 가능한 레이어가 만들어진다. 컨테이너 이미지의 제한적인 이식성 이해 이론적으로 컨테이너 이미지는 도커를 실행하는 모든 리눅스 시스템에서 실행될 수 있지만, 호스트에서 실행되는 모든 컨테이너가 호스트의 리눅스 커널을 사용한다는 사실과 관련해 주의해야 한다. 컨테이너화된 앱이 특정 커널 버전이 필요하다면 모든 시스템에서 작동하지 않을수 있음. 머신이 다른 버전의 리눅스 커널로 실행되거나 동일한 커널 모듈을 사용할 수 없는 경우에는 앱이 실행될 수 없다. 컨테이너는 가상머신에 비해 훨씬 가볍지만 컨테이너 내부에서 실행되는 앱은 일정한 제약이 있따. 하드웨어 아키텍처용으로 만들어진 컨테이너화된 앱은 해당 아키텍처 시스템에서만 실행될 수 있다는 점을 분명히 해야 한다.1.2.3 도커의 대안으로 rkt 소개 rkt는 현시점 deprecated됨. 도커와 마찬가지로 컨테이너를 실행하기 위한 플랫폼 2018년 이후로 업데이트 없음.1.3 쿠버네티스 소개1.3.1 쿠버네티스의 기원 구글은 보그(Borg, 이후 오메가(Omega)로 바뀐 시스템)라는 내부 시스템을 개발해 애플리케이션 개발자와 시스템 관리자가 수천 개의 애플리케이션과 서비스를 관리하는데 도움을 주었다. 개발과 관리를 단순화할 뿐만 아니라 인프라 활용률을 크게 높일 수 있었다. 2014년 보그, 오메가, 기타 내부 구글 시스템으로 얻은 경험을 기반으로 하는 오픈소스 시스템인 쿠버네티스를 출시1.3.2 넓은 시각으로 쿠버네티스 바라보기 쿠버네티스는 컨테이너화된 애플리케이션을 쉽게 배포하고 관리할 수 있게 해주는 소프트웨어 시스템 애플리케이션은 컨테이너에서 실행되므로 동일한 서버에서 실행되는 다른 앱에 영향을 미치지 않으며, 이는 동일한 하드웨어에서 완전히 다른 조직의 앱을 실행할때 매우 중요하다. 호스팅된 앱을 완전히 격리하면서 하드웨어를 최대한 활용한다. 모든 노드가 마치 하나의 거대한 컴퓨터인 것처럼 수천대의 컴퓨터 노드에서 스포트웨어 애플리케이션을 실행할 수 있다.쿠버네티스 핵심 이해 시스템은 마스터 노드와 여러 워커 노드로 구성 구성요소가 어떤 노드에 배포되든지 개발자나 시스템 관리자에게 중요하지 않다. 개발자는 앱 디스크립터를 쿠버네티스 마스터에게 게시하면 쿠버네티스는 해당 앱을 워커 노드 클러스터에 배포한다.개발자가 앱 핵심 기능에 집중할 수 있도록 지원 쿠버네티스는 마치 클러스터의 운영체제로 생각할 수 있음. 서비스 디스커버리 ,스케일링, 로드밸런싱, 자가 치유, 리더 선출 같은 것들을 포함하여 지원한다.운영 팀이 효과적으로 리소스를 활용할 수 있도록 지원 각 앱들은 어떤 노드에서 실행되든 상관이 없기 때문에 쿠버네티스는 언제든지 앱을 재배치하고, 조합함으로써 리소스를 수동 스케줄링보다 훨씬 잘 활용할 수 있다.1.3.3 쿠버네티스 클러스터 아키텍쳐 이해 하드웨어 수준에서 쿠버네티스 클러스터는 여러 노드로 구성되며, 2가지 유형으로 나눌 수 있다. 마스터 노드 : 전체 쿠버네티스 시스템을 제어하고 관리하는 쿠버네티스 컨트롤 플레인을 실행워커 노드 : 실제 배포되는 컨테이너 애플리케이션을 실행 컨트롤 플레인(Control Plane) 클러스터를 제어하고 작동시키는 역할 하나의 마스터 노드에서 실행하거나, 여러 노드로 분할되고 복제하여 고가용성을 보장할 수 있는 여러 구성 요소로 구성할 수 있다. API서버는 사용자, 컨트롤 플레인 구성 요소와 통신 스케줄러는 앱의 배포를 담당(앱의 배포 가능한 각 구성요소를 워크 노드에 할당) 컨트롤러 매니저는 구성요소 복제본, 워커 노드 추적, 노드 장애 처리 등과 같은 클러스터단의 기능을 수행 Etcd는 클러스터 구성을 지속적으로 저장하는 신뢰할 수 있는 분산 데이터 저장소노드(Worker Node) 컨테이너화된 애플리케이션을 실행하는 시스템 컨테이너 런타임은 도커, rkt 또는 다른 컨테이너 런타임이 될 수 있다. kebelet은 API 서버와 통힌하고 노드의 컨테이너를 관리한다. kebe-proxy(쿠버네티스 서비스 프록시)는 앱 구성요소간 네트워크 트래픽을 로드밸런싱한다.1.3.4 쿠버네티스에서 애플리케이션 실행 애플리케이션을 하나 이상의 컨테이너 이미지로 패키징하고 해당 이미지를 레지스트리로 푸시한 다음에 쿠버네티스 API 서버에 앱 디스크립션을 게시해야 한다.앱 디스크립션에 포함되는 내용들 컨테이너 이미지 앱 구성요소가 포함된 이미지 해당 구성요소가 서로 통신하는 방법 동일 서버에 함께 배치되어야 하는 구성 요소 실행될 각 구성 요소의 본제본 수 내부 또는 외부 클라이언트에 서비스를 제공하는 구성요소 하나의 IP주소로 노출해 다른 구성 요소에서 검색가능하게 해야 하는 구성요소 등디스크립션으로 컨테이너를 실행하는 방법 이해 1) API 서버가 앱 디스크립션을 처리할때 스케줄러는 각 컨테이너에 필요한 리소스를 계산하고 각 노드에 할당되지 않은 리소스를 기반으로 사용 가능한 워커 노드에 지정된 컨테이너를 할당한다. 2) kebulet은 컨테이너 런타임(도커)에 필요한 컨테이너 이미지를 가져와 컨테이너를 실행하도록 지시한다.실행된 컨테이너 유지 앱이 실행되면 쿠버네티스는 앱의 배포 상태가 사용자가 제공한 디스크립션과 일치하는지 지속적으로 확인한다. 예를 들어 5개의 웹 서버 인스턴스를 실행하도록 지정하면 쿠버네티스는 항상 정확히 5개의 인스턴스를 계속 실행 프로세스 중단 등 인스턴스가 제대로 동작하지 않으면 자동으로 다시 시작 워커 노드 전체가 종료되거나 하면 이 노드에서 실행중인 모든 컨테이너 노드를 새로 스케줄링하고, 새로 선택한 노드에서 실행한다. 복제본 수 스케일링 실행되는 동안 복제본 수를 늘릴지 줄일지 결정할 수 있음. 최적의 본제본 수를 결정하는 작업을 쿠버네티스에게 위힘할 수 있다. 쿠버네티스는 CPU부하, 메모리 사용량, 초당 요청수 등 실시간 메트릭을 기반으로 복제본 수를 자동으로 조정할 수 있다.이동한 애플리케이션에 접근하기 쿠버네티스는 클라이언트가 특정 서비르르 제공하는 컨테이너를 쉽게 찾을 수 있도록 동일한 서비스를 제공하는 컨테이너를 알려주면 하나의 고정 IP주소로 모든 컨테이너를 노출하고 해당 주소를 클러스터에서 실행중인 모든 앱에 노출한다. DNS로 서비스 IP를 조회할 수도 있음. kube-proxy는 서비스를 제공하는 모든 컨테이너에서 서비스 연결이 로드밸런싱되도록 한다. 이런 매커니즘에 의해서 컨테이너들이 클러스터 내에서 이동하더라도 컨테이너에 항상 연결할 수 있음.1.3.5 쿠버네티스 사용의 장점 시스템 관리자는 앱을 배포하고 실행하기 위해 아무것도 설치할 필요가 없음. 개발자는 시스템 관리자의 도움 없이 즉시 앱을 실행할 수 있다.애플리케이션 배포의 단순화 개발자는 클러스트를 구성하는 서버에 관해 알 필요가 전혀 없다. 모든 노드는 단순히 전체 컴퓨팅 리소스일뿐, 앱에 적절한 시스템 리소를 제공할 수 있는 한 어느 서버에서 실행중인지는 신경쓰지 않아도 된다. 특정 앱이 특정 종류의 하드웨어에서 실행해야 하는 경우에도 지원이 가능(예를 들면 SSD를 이용해야 하는 경우)하드웨어 활용도 높이기 인프라와 애플리케이션을 분리해서 생ㅇ각할 수 있다. 쿠버네티스는 요구사항에 대한 디스크립션과 노드에서 사용 가능한 리소스에 따라 앱을 실행한 가장 적합한 노드를 선택할 수 있다. 쿠버네티스는 언제든지 클러스터 간에 앱이 이동할 수 있으모로 수동으로 수행하는것보다 훨씬 더 인프라를 잘 활용할 수 있다.상태 확인과 자가 치유 서버 장애 발생시 언제든지 클러스터 간에 앱을 이동시킬수 있는 시스템이 갖출수 있다. 쿠버네티스는 앱 구성요소와 구동중인 워커 노드를 모니터링하다가 노드 장애 발생시 자동으로 앱을 다른 노드로 스케줄링한다.오토스케일링 급격한 부하 급증에 대응하기 위해 개별 앱의 부하를 운영팀이 지속적으로 모니터링할 필요가 없다. 각 앱이 사용하는 리소스를 모니터링하고, 실행중인 인스턴스 수를 자동으로 조정하도록 지시할수 있다.애플리케이션 개발 단순화 개발과 프로덕션 환경이 모두 동일한 환경에서 실행된다는걸 보장할 수 있어서 버그발견시 큰 효과를 얻을수 있음. 또한 서비스 디스커버리와 같은 일반적으로 구현해야 하는 기능들을 구현할 필요가 없어졌다. 쿠버네티스 API 서버를 직접 쿼리하면 개발자가 리더 선정 같은 복잡한 메커니즘을 구현하지 않아도 된다. 새로운 버전의 앱을 출시할때 신버전이 잘못됐는지 자동으로 감지하고 즉시 롤아웃을 중지할수 있어서 신뢰성을 증가시켜 CD(continuous delivery)을 가속화할수 있음.1.4 요약 모놀리스 애플리케이션은 구축하기 쉽지만 시간이 지남에 따라 유지 관리가 어려워지고 때로는 확장이 불가능할수 있다. 마이크로서비스 기반 애플리케이션 아키텍처는 각 구성요소의 개발을 용이하게 하지만, 하나의 시스템으로 작동하도록 배포하고 구성하기가 어렵다. 리눅스 컨테이너는 가상머신과 동일한 이점을 제공하지만 훨씬 더 가볍고 하드웨어 활용도를 높일 수 있다. 도커는 OS환경과 함꼐 컨테이너화된 애플리케이션을 좀 더 쉽고 빠르게 프로비저닝할 수 있도록 지원해 기존 리눅스 컨테이너 기술을 개선했다. 쿠버네티스는 전체 데이터 센터를 앱 실행을 위한 컴퓨팅 리소스로 제공한다. 개발자는 시스템 관리자의 도움 없이도 쿠버네티스로 앱을 배포할 수 있다. 시스템 관리자는 쿠버네티스가 고장 난 노드를 자동으로 처리하도록 할수 있다.Reference kubernetes-in-action kubernetes.io" }, { "title": "[HackerRank] Java Anagrams", "url": "/posts/devlog-java-algolithm3/", "categories": "DevLog, Algorithm", "tags": "Java, Algorithm", "date": "2020-06-23 17:34:00 +0900", "snippet": "[HackerRank] Java AnagramsPloblemMy solution char별로 카운트해야 된다고 생각해서 Map에 넣고 Count를 한다음에 각 맵끼리 비교하는것으로 문제를 풀었다.private void anagram(String a, String b) {\t\tboolean anagram = isAnagram(a,b);\t\tif(anagr...", "content": "[HackerRank] Java AnagramsPloblemMy solution char별로 카운트해야 된다고 생각해서 Map에 넣고 Count를 한다음에 각 맵끼리 비교하는것으로 문제를 풀었다.private void anagram(String a, String b) {\t\tboolean anagram = isAnagram(a,b);\t\tif(anagram) {\t\t\tSystem.out.println(\"Anagrams\");\t\t} else {\t\t\tSystem.out.println(\"Not Anagrams\");\t\t}\t}\tprivate boolean isAnagram(String a, String b) {\t\tif(isValidParam(a, b)) {\t\t\treturn false;\t\t}\t\tMap&lt;Character, Integer&gt; aCountMap = makeCountMap(a);\t\tMap&lt;Character, Integer&gt; bCountMap = makeCountMap(b);\t\tlog.debug(\"aCountMap : {}\", aCountMap);\t\tlog.debug(\"bCountMap : {}\", bCountMap);\t\treturn aCountMap.equals(bCountMap);\t}\tprivate Map&lt;Character, Integer&gt; makeCountMap(String a) {\t\tMap&lt;Character, Integer&gt; countMap = new HashMap&lt;&gt;();\t\tfor(Character c : a.toLowerCase().toCharArray()) {\t\t\tInteger count = Optional.ofNullable(countMap.get(c))\t\t\t\t.map(cnt -&gt; cnt + 1)\t\t\t\t.orElse(1);\t\t\tcountMap.put(c, count);\t\t}\t\treturn countMap;\t}\tprivate boolean isValidParam(String a, String b) {\t\treturn (a.length() &lt;= 1 &amp;&amp; a.length() &lt;= 50 &amp;&amp; b.length() &lt;=1 &amp;&amp; b.length() &lt;= 50);\t}Simple solution 다른사람들이 손쉽게 풀이한 문제를 공유한다. 공간복잡도도 더 조금쓰면서 코드도 심플..(시무룩)private void othersAnagram(String a, String b) {\t\tif (a.length() != b.length()) {\t\t\tSystem.out.println(\"Not Anagrams\");\t\t\treturn;\t\t}\t\tchar[] a1 = a.toLowerCase().toCharArray();\t\tchar[] a2 = b.toLowerCase().toCharArray();\t\tArrays.sort(a1);\t\tArrays.sort(a2);\t\tif(Arrays.equals(a1, a2)) {\t\t\tSystem.out.println(\"Anagrams\");\t\t} else {\t\t\tSystem.out.println(\"Not Anagrams\");\t\t}\t}Code Repo https://github.com/sungsu9022/study-algorithm/blob/master/src/test/java/com/sungsu/algorithm/hackerrank/Java_Anagrams.javaRefference https://www.baeldung.com/java-strings-anagrams" }, { "title": "Spring AOP 이야기 - AOP는 언제 써야할까?", "url": "/posts/devlog-spring-aop/", "categories": "DevLog, Spring", "tags": "Java, Spring", "date": "2020-06-22 17:34:00 +0900", "snippet": "Spring AOP Spring을 공부하다보면 필연적으로 AOP를 만나게 되는데, 개발자들은 이걸 언제 쓰는게 좋을지 고민하곤 하는데요. 관련해서 사용방법과 언제 써야할지를 한번 알아보겠습니다.1. AOP란? 부가기능 모듈을 객체지향 기술에서 주로 사용하는 오브젝트와는 다르게 특별한 이름으로 부르기 시작한 것 그 자체로 애플리케이션의 핵심 기능을...", "content": "Spring AOP Spring을 공부하다보면 필연적으로 AOP를 만나게 되는데, 개발자들은 이걸 언제 쓰는게 좋을지 고민하곤 하는데요. 관련해서 사용방법과 언제 써야할지를 한번 알아보겠습니다.1. AOP란? 부가기능 모듈을 객체지향 기술에서 주로 사용하는 오브젝트와는 다르게 특별한 이름으로 부르기 시작한 것 그 자체로 애플리케이션의 핵심 기능을 담고 있지는 않지만, 애플리케이션을 구성하는 중요한 한 가지 요소이고, 핵심 기능에 부가되어 의미를 갖는 특별한 모듈 어드바이저는 가장 단순한 형태의 애스팩트AOP(애스펙트 지향 프로그래밍, Aspect Oriented Programming) OOP를 돕는 보조적인 기술일뿐, OOP를 대체하는 새로운 개념이 아니다. 부가기능이 핵심기능 안으로 침투해버리면 핵심기능 설계에 객체지향 기술의 가치를 온전히 부여하기 힘들어짐. AOP는 애스펙트를 분리함으로써 핵심기능을 설계하고 구현할 때 객체지향적인 가치를 지킬 수 있도록 돕는 것.2. 언제 AOP를 사용해야 할까?2.1 매번 반복되는 개발 패턴들\tprivate final SomethingService service;\tprivate final SomethingRepository repository;\t@Transactional\tpublic void doSomething() {\t\tif(isCondition()) {\t\t\t// 무언가를 한다.\t\t}\t\tif(isCondition2()) {\t\t\t// 무언가를 한다.\t\t} else {\t\t}\t\tif(isCondition3()) {\t\t\tif(isCondition4()) {\t\t\t\tfor(Object item : ItemList) {\t\t\t\t\t// 무언가를 한다.\t\t\t\t}\t\t\t} else {\t\t\t\t// 무언가를 한다.\t\t\t}\t\t}\t} Spring에서 제공하고자 했던 그 기술의 철학과 가치에 부합되는 방식으로 개발하자. 객체지향적인 고민(?)을 더하다. 실제로 객체지향의 핵심인 SOLID 원칙 중 단일책임 원칙을 위배하면서 개발하는 경우는 상당수 사실 이렇게 개발하게 된 이유는 현실적으로 이것 하나 떄문에 이렇게까지 해야 하나에 대한 고민도 있을것이고, if문 하나 추가하면 오히려 가독성 측면이나 유지보수 측면에서 더 좋기 떄문이다. 2.2 그러면 언제 AOP를 써야하는가? 반복적으로 동일한 코드를 여기저기에 개발 적용해야 하는 경우(최우선) 자신의 설계에서 A클래스에 부여한 책임을 넘어서는 수준의 일을 처리해야 하는 경우 이 부분은 반복적으로 동일한 코드가 계속 들어가야 하는 경우의 전제조건이 있어야 한다. 만약 하나의 영역에서만 사용되는 코드를 굳이 AOP로 만드는것은 다른사람의 코드 추적을 어렵게 만들지도 모른다. 해당 처리 로직이 클래스에서 처리해야 하는 핵심로직이냐 아니면 부가 기능이냐를 기준으로 잘 판단해서 부가기능인 경우 이 판단이 사실 명확한 근거는 없다. 개인과 팀이 상황에 따라 적절히 판단하는게 좋을것 같다. 2.3 Sevlet Filter vs Interceptor vs AOP AOP에 대해서 자세히 알아보기 전에 먼저 Servlet Filter, Intercpetor, AOP에 대해서 알아볼 필요가 있다. Spring 웹 애플리케이션을 개발하면서 Filter, Interceptor는 심심찮게 접했을 것이다. 동작 방식은 사실 이 3가지 모두 비슷한 점이 있다. 차이가 있다면 처리되는 시점의 차이가 있다. Servlet Filter는 request 최앞단, 최말단에서 동작하고, Interceptor는 Dispatcher Servlet 다음 단계에서 동작하고 AOP는 개발자가 직접 처리시점을 정할수 있다. 3. AOP 사용하기3.1 Spring boot Project 설정 aop 라이브러리 dependency를 추가한다.&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt;3.2 AOP Configration Configuration클래스 혹은 Application Class에 @EnableAspectJAutoProxy 을 추가해준다. 나는 별도의 Configration도 각 목적에 따라 나누어 처리하는것을 선호해서 별도의 RootConfig.class를 두었고, 그쪽에 선언하였다.@Configuration@EnableAspectJAutoProxypublic class RootConfig {}@SpringBootApplication(scanBasePackageClasses = {RootConfig.class})public class Application extends SpringBootServletInitializer {\tpublic static void main(String[] args) {\t\tSpringApplication app = new SpringApplication(Application.class);\t\tapp.addListeners(new ApplicationPidFileWriter());\t\tapp.run(args);\t}3.3 @EnableAspectJAutoProxy @EnableAspectJAutoProxy 는 다른 Enable류와 마찬가지로 auto configration annotation이다. AOP와 관련된 @Aspect와 같은 annotation을 사용할수 있도록 해준다. xml에서는 &lt;aop:aspectj-autoproxy&gt;와 동일한 효과라고 생각하면 된다. @EnableAspectJAutoProxy의 proxyTargetClass attribute가 있는데 이 부분에 대해서도 간략히 알고 있으면 좋다.3.4 EnableAspectJAutoProxy.proxyTargetClass 이 부분은 Spring의 역사 이야기를 조금 해야 하는데 proxy 방식에는 JDK Dynamic Proxy방식과 CGLIB 방식이 있다는 것을 알아야 한다. JDK Dynamic Proxy은 인터페이스를 구현해서 구현된 메소드에만 AOP를 적용할수 있는 방법이었어서 이 방식으로 proxy 처리를 하면 인터페이스에 정의된 메소드에만 proxy를 적용할 수 있다. CDLIB는 대상 클래스를 상속받아서 프록시 객체를 구현하는 방식으로 클래스가 final만 아니라면 proxy를 적용할수 있다. proxyTargetClass 이 옵션은 저 방식을 무엇으로 할건지에 대한 것이다. proxyTargetClass의 기본값은 false인데, 그러면 기본이 JDK Dynamic Proxy인것을 알수 있다. 하지만 가장 최근 문서를 읽어보니면 프록시 대상 클래스가 인터페이스를 구현하지 않으면 기본적으로 CGLIB 프록시가 적용된다고 합니다.(spring 5.2 공식 문서 기준)3.5 AOP Aspect 클래스 정의 Aspect를 정의하고 어느시점에 처리할지에 대한 부분은 @Around, @Before, @After가 있다. @Around는 메소드의 실행 전 / 후의 로직을 컨트롤할수 있다. @Before, @After은 말 그대로 메소드 실행 전 / 후이다. 서비스 비즈니스 로직 개발하는 입장에서는 @Around을 이용해야 하는 케이스가 제일 많았다. 로컬환경에서만 동작해야 하는 스펙이 있다고 가정하고 AOP를 만들어보자.동작하는 스펙은 편의상 String을 return하는 메소드 시그니쳐에 DummyString을 return 한다고 가정한다.@Aspect@Component@Slf4j@RequiredArgsConstructorpublic class MainAspect {\tprivate final EnvConfig envConfig;\t@Around(\"execution(String com.sungsu.boilerplate.app.main.MainService.*(..)) &amp;&amp; @annotation(com.sungsu.boilerplate.app.main.aspect.ReturnDummy)\")\tpublic Object returnDummy(ProceedingJoinPoint joinPoint) throws Throwable {\t\tif (!isLocalEnvironment()) {\t\t\treturn joinPoint.proceed();\t\t}\t\t// local 환경인 경우 본 메소드를 수행하지 않고 dummy value를 return\t\tlog.info(\"returnDummy\");\t\treturn \"returnDummy\";\t}\t/**\t * 로컬 환경인지 확인\t * @return\t */\tprivate boolean isLocalEnvironment() {\t\treturn envConfig.isLocal();\t}}3.6 Aspect 동작 시점 정의 어느 시점에 Aspect를 동작시킬지에 대한 방법은 다양한데, 대표적으로 포인트컷 표현식이 있다. 하지만 위 3.5 예제에서는 보다시피 포인트컷 표현식(@execution) 과 @annotation을 and 조건으로 사용하고 있다. 왜 Annotation을 이용해서 AOP를 적용했을까? 포인트컷 표현식을 이용한 AOP를 적용했을때의 문제@Aspectpublic class TestAspect {\t@Pointcut(\"execution(* transfer(..))\")// 포인트컷 표현식\tprivate void anyOldTransfer() {}// 포인트컷 시그니처} AOP 적용을 위해서 포인트컷 표현식만을 이용한다고 했을 때 AOP가 적용된 메소드에서는 이 메소드에 AOP를 걸었는지 아닌지를 한눈에 파악하기가 어렵다. 물론 IDE에서는 네비게이션을 제공해주기는 하지만 github을 이용해서 코드리뷰를 한다던지 했을때 이를 쉽게 파악할 수 없다. 대표적인 AOP 예인 @Transactional와 같은 경험을 얻기 위해서 위와 같이 처리한 것이다. 자세한 내용은 여기서는 다루지 않고 참조 링크에서 확인 바란다. ( https://blog.outsider.ne.kr/843 )3.7 annotation 정의하기 Meta Annotations     @Retention 어노테이션이 적용되는 범위(?), 어떤 시점까지 어노테이션이 영향을 미치는지 결정(코드, 클래스로 컴파일, 런타임에 반영 등) @Documented 문서에도 어노테이션의 정보가 표현됩니다. @Target 어노테이션이 적용할 위치를 결정합니다. @Inherited 이 어노테이션을 선언하면 자식클래스가 어노테이션을 상속 받을 수 있습니다. @Repeatable 반복적으로 어노테이션을 선언할 수 있게 합니다. import java.lang.annotation.*;@Inherited@Documented@Retention(RetentionPolicy.RUNTIME) // 컴파일 이후에도 JVM에 의해서 참조가 가능합니다.//@Retention(RetentionPolicy.CLASS) // 컴파일러가 클래스를 참조할 때까지 유효합니다.//@Retention(RetentionPolicy.SOURCE) // 어노테이션 정보는 컴파일 이후 없어집니다.@Target({ ElementType.PACKAGE, // 패키지 선언시 ElementType.TYPE, // 타입 선언시 ElementType.CONSTRUCTOR, // 생성자 선언시 ElementType.FIELD, // 멤버 변수 선언시 ElementType.METHOD, // 메소드 선언시 ElementType.ANNOTATION_TYPE, // 어노테이션 타입 선언시 ElementType.LOCAL_VARIABLE, // 지역 변수 선언시 ElementType.PARAMETER, // 매개 변수 선언시 ElementType.TYPE_PARAMETER, // 매개 변수 타입 선언시 ElementType.TYPE_USE // 타입 사용시}) 추가적인 이해를 위해서 @Transactional 을 인용하자면@Target({ElementType.METHOD, ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface Transactional { @AliasFor(\"transactionManager\") String value() default \"\"; @AliasFor(\"value\") String transactionManager() default \"\"; Propagation propagation() default Propagation.REQUIRED; Isolation isolation() default Isolation.DEFAULT; int timeout() default -1; boolean readOnly() default false; Class&lt;? extends Throwable&gt;[] rollbackFor() default {}; String[] rollbackForClassName() default {}; Class&lt;? extends Throwable&gt;[] noRollbackFor() default {}; String[] noRollbackForClassName() default {};} 예제코드에 사용한 annotation은 간단하다. 이정도만으로도 원하는 형태로 충분히 사용 가능하다.@Target({ElementType.METHOD, ElementType.TYPE })@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface ReturnDummy {} annotation을 통해 특정 값을 전달할수도 있다.@Target({ElementType.METHOD, ElementType.TYPE })@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface ReturnDummy {\tString data() default \"\";}\t@ReturnDummy(data = \"test\")\tpublic String method1() {\t\tlog.info(\"method1\");\t\treturn \"method1\";\t} 혹시 필요하다면 el expression으로 전달할수도 있으니 참고만 하도록 하자.3.8 AOP 메소드에 적용하기 애노테이션과 Aspect를 정의했다면 이제 Service Layer 같은곳에 적용할 차례이다.// MainController.java@Controller@RequiredArgsConstructorpublic class MainController {\tprivate final MainService mainService;\t@GetMapping(value = {\"/\", \"/main\"})\tpublic String main() {\t\tmainService.method1();\t\tmainService.method2();\t\tmainService.method3();\t\treturn \"index\";\t}}// MainService.java@Service@RequiredArgsConstructor@Slf4jpublic class MainService {\t@ReturnDummy(data = \"test\")\tpublic String method1() {\t\tlog.info(\"method1\");\t\treturn \"method1\";\t}\t@ReturnDummy(data = \"test\")\tpublic String method2() {\t\tlog.info(\"method2\");\t\treturn \"method2\";\t}\t@ReturnDummy(data = \"test\")\tpublic String method3() {\t\tlog.info(\"method3\");\t\treturn \"method3\";\t}} 이렇게 적용하면 로컬환경에서는 각 method1,2,3을 호출해도 해당 메소드는 호출되지 않고 AOP에 정의한 “dummyReturn” 값만을 return 하게 될 것이다. annotation을 정의할떄 @Target을 METHOD, TYPE 2가지를 보통 정의했는데 위 예제는 메소드 각각에 정의했지만 Class에 바로 적용도 가능한다.@Service@RequiredArgsConstructor@Slf4j@ReturnDummy(data = \"test\")public class MainService {\tpublic String method1() {\t\tlog.info(\"method1\");\t\treturn \"method1\";\t}\tpublic String method2() {\t\tlog.info(\"method2\");\t\treturn \"method2\";\t}\tpublic String method3() {\t\tlog.info(\"method3\");\t\treturn \"method3\";\t}} 다만 클래스에 적용하는것보단 메소드단위로 적용하는것이 개발할때 더 편리하다.(IntelliJ IDEA 기준으로 method에 정의한 annotation에서는 trace가 가능한 마킹이 생기는것을 알수 있다.)3.9 AOP 테스트 코드 작성 AspectJProxyFactory를 이용해서 aspect를 주입해주고 Proxy 객체로 덮어 씌워주면 된다. 예제는 아래를 참고하면 된다.@RunWith(MockitoJUnitRunner.class)public class MainServiceTest {\t@InjectMocks\tprivate MainService mainService;\t@InjectMocks\tprivate MainAspect aspect;\t@Mock\tprivate EnvConfig envConfig;\t@Before\tpublic void setUp() {\t\tAspectJProxyFactory factory = new AspectJProxyFactory(mainService);\t\tfactory.addAspect(aspect);\t\tfactory.setProxyTargetClass(true);\t\tmainService = factory.getProxy();\t}\t@Test\tpublic void method1() {\t\tMockito.when(envConfig.isLocal()).thenReturn(true);\t\tassertTrue(MainAspect.DUMMY_VALUE.equals(mainService.method1()));\t}}4. AOP에 대한 개인 생각 나는 실제 서비스 개발을 하면서 AOP를 꽤 자주 사용하는 편이다. AOP를 써야할지 말지를 판단할떄는 위에 언급한것처럼 각각의 조건을 충족할떄 하는것이 좋다. AOP가 만능은 아니며, 오히려 유지보수성을 떨어뜨릴지도 모른다. 적절한 상황을 잘 판단하여 사용하도록 하자. 내가 AOP를 실무에 적용한 케이스 A/B테스트 적용을 위해 특정 유저군들에만 한시적으로 기능을 오픈하기 위한 방식으로 사용 성능 테스트시에 다른 서비스(서버 단위)에 영향을 주지 않기 위해 더미 처리를 하기 위해 사용 특정 환경에서만 노출되어야 할 부가정보를 추가해주기 위해 모델의 Deprecated된 필드가 있는데(DB에서 제거) 구버전 앱 지원을 위해 상위 레벨에서는 해당 데이터가 필요한 상황이 있어서 이 값을 바인딩해주는 역할로써 AOP 사용 Redis 기반 Lock 적용시 AOP 사용 이 외 임시로 특정기간동안만 추가되는 로직이 있을때 한시적으로 AOP를 적용해서 처리하고, 그 특정기간이 종료되면 Aspect와 Annotation을 제거하여 쉽게 제거하기 위한 목적 등 5. 예제 코드 https://github.com/sungsu9022/spring-boot-boilerplate/commit/bcc91cc94b7f8fe5c233bbf9efe154fb7a00b6e7 https://github.com/sungsu9022/spring-boot-boilerplateReference https://docs.spring.io/spring/docs/5.2.0.M1/spring-framework-reference/core.html#aop https://blog.outsider.ne.kr/843" }, { "title": "[HackerRank] Java String Reverse", "url": "/posts/devlog-java-algolithm2/", "categories": "DevLog, Algorithm", "tags": "Java, Algorithm", "date": "2020-06-21 17:34:00 +0900", "snippet": "[HackerRank] Java String ReversePloblemMy solution 예전에 동일한 문제 해결 방법으로 Stack에 넣었다가 pop()으로 reverse 한다는 이야기를 들은적이 있어서 그렇게 구현private void solution(String s) {\t\tif(isPalindrome(s)) {\t\t\tSystem.out.prin...", "content": "[HackerRank] Java String ReversePloblemMy solution 예전에 동일한 문제 해결 방법으로 Stack에 넣었다가 pop()으로 reverse 한다는 이야기를 들은적이 있어서 그렇게 구현private void solution(String s) {\t\tif(isPalindrome(s)) {\t\t\tSystem.out.println(\"Yes\");\t\t} else {\t\t\tSystem.out.println(\"No\");\t\t}\t}\tprivate boolean isPalindrome(String s) {\t\tStack&lt;Character&gt; stack = new Stack&lt;&gt;();\t\tfor (char c : s.toCharArray()) {\t\t\tstack.push(c);\t\t}\t\tStringBuilder sb = new StringBuilder();\t\twhile(!stack.isEmpty()) {\t\t\tsb.append(stack.pop());\t\t}\t\treturn sb.toString().equals(s);\t}Simple solution StringBuilder 객체로 만든 뒤 reverse() 호출private void solution2(String s) {\t\tSystem.out.println( s.equals( new StringBuilder(s).reverse().toString()) ? \"Yes\" : \"No\" );\t}추가 내용 내가 풀이한 방법대로 했을때 공간복잡도를 생각했을때 더 많이 사용될 수 있음. Stack 인스턴스를 만들어야 하기 떄문.. 만약 StringBulder 없이 하더라면 비슷할것으로 생각된다. StringBuilder.reverse() 코드를 보면 for-loop를 돌면서 처리한다.Code Repo https://github.com/sungsu9022/study-algorithm/blob/master/src/test/java/com/sungsu/algorithm/hackerrank/Java_String_Reverse.javaRefference https://www.hackerrank.com/challenges/java-string-reverse/forum" }, { "title": "[HackerRank] Java Loops II", "url": "/posts/devlog-java-algolithm1/", "categories": "DevLog, Algorithm", "tags": "Java, Algorithm", "date": "2020-06-13 17:34:00 +0900", "snippet": "[HackerRank] Java Loops IIPloblemMy Solutionimport java.util.*;import java.io.*;import java.lang.Math;class Solution{ public static void main(String []argh){ Scanner in = new Scanner(Syst...", "content": "[HackerRank] Java Loops IIPloblemMy Solutionimport java.util.*;import java.io.*;import java.lang.Math;class Solution{ public static void main(String []argh){ Scanner in = new Scanner(System.in); int t=in.nextInt(); for(int i=0;i&lt;t;i++){ int a = in.nextInt(); int b = in.nextInt(); int n = in.nextInt(); for(int j=1;j&lt;=n;j++) { int value = a; for(int k=0;k&lt;j;k++) { value += Math.pow(2, k) * b; } System.out.printf(\"%d \", value); } System.out.println(\"\"); } in.close(); }}Best Solution 내가 문제를 보면서 푸는것 말고 더 좋은 방법이 없나 검색해보니 더 좋은 방법이 있었다. 2^0 + 2^1 + ... 2^j = 2^(j+1) - 1 수학 공식을 이용해 푸는 방법인데 저는 재귀함수를 써야하나 loop 한번 더 돌아야할것 같다고만 생각했네요class Solution{ public static void main(String []argh){ Scanner in = new Scanner(System.in); int t=in.nextInt(); StringBuilder sb = new StringBuilder(); for(int i=0;i&lt;t;i++){ int a = in.nextInt(); int b = in.nextInt(); int n = in.nextInt(); sb.setLength(0); for(int j=0; j&lt;n; ++j) { // 2^0 + 2^1 + ... 2^j = 2^(j+1) - 1 sb.append((int) (a + b*(Math.pow(2, j+1) - 1))).append(\" \"); } System.out.println(sb.toString()); } in.close(); }}Code Repo https://github.com/sungsu9022/study-algorithm/blob/master/src/test/java/com/sungsu/algorithm/hackerrank/Java_Loops_II.javaRefference https://www.hackerrank.com/challenges/java-loops/forum/comments/214175" }, { "title": "OSX에서 MongoDB 설치하기", "url": "/posts/devlog-mongodb/", "categories": "DevLog, MongoDB", "tags": "Infra, MongoDB", "date": "2020-06-08 17:34:00 +0900", "snippet": "OSX에서 MongoDB 설치하기1) brew를 통한 mongoDB 설치 OSX에 Mongo DB를 설치하는 방법은 다양합니다. 공식 사이트( https://www.mongodb.com/download-center/community ) 에서 tgz 파일을 다운받아서 설치할수도 있는데요. OSX를 사용한다면 brew와 같은 패키지 매니저를 통해 여러 ...", "content": "OSX에서 MongoDB 설치하기1) brew를 통한 mongoDB 설치 OSX에 Mongo DB를 설치하는 방법은 다양합니다. 공식 사이트( https://www.mongodb.com/download-center/community ) 에서 tgz 파일을 다운받아서 설치할수도 있는데요. OSX를 사용한다면 brew와 같은 패키지 매니저를 통해 여러 애플리케이션을 설치하고 관리하는것이 조금더 일반적이라 brew를 이용한 방법으로 작성하였습니다.brew install mongodb-community mongodb로 설치하는 경우 설치가 되지 않는 문제가 있는데요. homebrew PR-43770( https://github.com/Homebrew/homebrew-core/pull/43770 ) 에 의해 제거되었기 떄문입니다.brew install mongodb 추가로 혹시 인스톨상에 문제가 있다면 아래와 같이 시도해보세요.brew services stop mongodbbrew uninstall mongodbbrew tap mongodb/brewbrew install mongodb-community2) mongoDB 간략 설정 brew를 통해 mongodb가 설치되었다면 /usr/local/etc/mongod.conf 파일이 생성된것을 알수 있습니다. 해당 파일의 log와 dbPath를 적절한 위치로 수정합니다.# 적절히 설정 변경systemLog: destination: file path: /Users/user/logs/mongodb/mongo.log logAppend: truestorage: dbPath: /Users/user/data/mongodbnet: bindIp: 127.0.0.13) mongoDB 실행버전 확인mongo -version(3.1) mongoDB server 실행 brew를 이용해 실행시키는 방법(이 경우는 /usr/local/etc/mongod.conf 설정을 들고 뜨도록 되어있습니다.)brew services start mongodb-community mongod를 직접 사용하여 server를 띄워도 됩니다.mongod --config /usr/local/etc/mongod.conf &amp;ps -ef | grep mongo(3.2) mongoDB server 종료brew services stop mongodb-community mongod를 직접 사용하여 프로세스를 올린 경우에는 아래와 같이 프로세스를 종료시킬수 있습니다.# commend parsing error 발생mongod --shutdownkill &lt;mongod process ID&gt; mongod --shutdown은 document 업데이트가 안된것인지 작동하지 않아 그냥 kill을 이용해서 프로세스를 종료하시면 됩니다.(3.3) mongoDB clientmongoRefference https://docs.mongodb.com/manual/tutorial/manage-mongodb-processes/#stop-mongod-processes https://stackoverflow.com/questions/57856809/installing-mongodb-with-homebrew" }, { "title": "jenv를 활용한 JDK 버전 관리", "url": "/posts/devlog-jenv/", "categories": "DevLog, Java", "tags": "Java", "date": "2020-05-30 17:34:00 +0900", "snippet": "jenv를 활용한 JDK 버전 관리jenv란 무엇인가? jenv는 rbenv에서 사용하는 방식을 본떠서 만든 java version 관리 도구입니다. 다양한 버전의 java application 관리 하는 경우에 적절히 손쉽게 version 변경할 수 있습니다.jenv 설치하기brew install jenvsh 설정 추가 jenv를 설치하면 아...", "content": "jenv를 활용한 JDK 버전 관리jenv란 무엇인가? jenv는 rbenv에서 사용하는 방식을 본떠서 만든 java version 관리 도구입니다. 다양한 버전의 java application 관리 하는 경우에 적절히 손쉽게 version 변경할 수 있습니다.jenv 설치하기brew install jenvsh 설정 추가 jenv를 설치하면 아래와 같은 문구가 나옵니다.# for bash(terminal, iTerm)echo 'export PATH=\"$HOME/.jenv/bin:$PATH\"' &gt;&gt; ~/.bash_profileecho 'eval \"$(jenv init -)\"' &gt;&gt; ~/.bash_profilesource ~/.bash_profile# for zshecho 'export PATH=\"$HOME/.jenv/bin:$PATH\"' &gt;&gt; ~/.zshrcecho 'eval \"$(jenv init -)\"' &gt;&gt; ~/.zshrcsource ~/.zshrc 여기서 eval \"$(jenv init -)\" 이 부분은 사실 .bash_profile에까지 추가할 필요는 없다. 한번만 initialize 해주면 되기 때문에 profile에 넣지 말고 커맨드라인에서 한번만 실행시켜주셔도 됩니다.jenv에 java version 추가하기 Java를 개인이 원하는 별도의 위치에 설치하고 쓰는 경우도 있겠지만 자동 설치파일을 통해 설치한 경우“/Library/Java/JavaVirtualMachines/” 하위에 들어가게 되는데요. 이를 추가해주시면 됩니다. 만약 별도의 디렉토리에서 관리하고 있다면 해당 디렉토리 Home을 기준으로 추가해주면 됩니다.jenv add /Library/Java/JavaVirtualMachines/jdk-11.0.2.jdk/Contents/Homejenv add /Library/Java/JavaVirtualMachines/jdk-14.jdk/Contents/Homejenv add /Library/Java/JavaVirtualMachines/jdk1.8.0_201.jdk/Contents/Homejenv 등록된 버전 확인jenv versionsjenv 사용하기 jenv를 사용하는 방법은 global, local 단위로 설정하는 방법이 있다. global은 말그대로 전체에 사용할 버전을 명시한것이고, local을 디렉토리 단위에서 사용할 java version을 지정하여 관리하는 방법이다. global을 지정하게 되면 ~/.jenv/version 파일에 사용자가 설정한 버전이 입력되어 관리된다. jenv global 14 local을 지정한 경우에는 지정한 디렉토리의 .java-version 파일이 생성되고 local java version이 관리된다. local 지정한 버전를 제거하고 싶다면 해당 파일을 삭제하면 된다. jenv local 14 Reference https://github.com/jenv/jenv" }, { "title": "이펙티브 자바 3판 - 3. 모든 객체의 공통 메서드", "url": "/posts/devlog-effective-java-3e-3/", "categories": "DevLog, Java", "tags": "Effective Java", "date": "2020-05-17 17:34:00 +0900", "snippet": "이펙티브 자바 3판 - 3. 모든 객체의 공통 메서드 Item10. equals 는 일반 규약을 지켜 재정의하라 Item11. equals를 재정의하려거든 hashCode도 재정의하라 Item12. toString을 항상 재정의하라 Item13. clone 재 정의는 주의해서 진행해라. Item14. Comparable을 구현할지 고려하라....", "content": "이펙티브 자바 3판 - 3. 모든 객체의 공통 메서드 Item10. equals 는 일반 규약을 지켜 재정의하라 Item11. equals를 재정의하려거든 hashCode도 재정의하라 Item12. toString을 항상 재정의하라 Item13. clone 재 정의는 주의해서 진행해라. Item14. Comparable을 구현할지 고려하라.Item10. equals 는 일반 규약을 지켜 재정의하라 equals 재정의하는데 있어서 곳곳에 합정이 있으며, 자칫 잘못하면 끔찍한 결과를 초래할 수 있음. 이를 회피하는 가장 쉬운 길은 아예 재정의하지 않는 것이지만 실제로 개발하다보면 재정의가 필요한 경우도 있을수 있다.(물론 lombok을 사용하면 조금 더 쉬워진다.)재정의를 하지 않아야 하는 상황 각 인스턴스가 본질적으로 고유한 경우(값을 표현하는게 아니라 동작하는 개체를 표현하는 클래스 Thread 클래스 같은것, 추가로 우리가 개발하는 Spring bean용 클래스의 경우를 생각해도 좋을 것 같다. 인스턴스의 논리적 동치성(logical equality)을 검사할 일이 없는 경우 Pattern클래스의 인스턴스 2개가 같은 정규표현식을 나타내는지 검사하는것과 같은 행위(논리적 동치성 검사) 설계자는 이 방식을 원하지 않거나 애초에 필요하지 않다고 판단할 수도 있음. 상위 클래스에서 재정의한 equals가 하위 클래스에도 딱 들어맞는 경우 Collection 클래스류와 같은.. 클래스가 private이거나 package-private이고 equals 메서드를 호출할 일이 없는 경우equals를 재정의를 해야 하는 상황 객체 식별성(object identity, 두 객체가 물리적으로 같은지?)이 아니라 논리적 동치성을 확인해야 하는데, 상위 클래스의 equals가 논리적 동치성을 비교하도록 재정의되지 않은 경우 위에 정의된 말이 조금 이해가 어려운데 쉽게 풀이하면 값 클래스 (Interger, String ..) 값이 같은 인스턴스가 둘 이상 만들어지지 않음을 보장하는 인스턴스 통제 클래스 Boolean.TRUE, Boolean,FALSE, Enum류 등 비즈니스 로직에서 사용할 Entity Class 적절히 PK로 활용될 수 있는 데이터셋을 기준으로 equals를 구현할수 있음 equals 재정의할때 따라야할 일반 규약equals 메서드는 동치 관계(equivalence relation)을 구현하며, 다음을 만족 반사성(reflexivity) : null이 아닌 모든 참조 값 x에 대해 x.equals(x)는 true 대칭성(symmetry) : null이 아닌 모든 참조 값 x,y에 대해 x.equals(y)가 true면 y.equals(x)도 true 추이성(transitivity) : null이 아닌 모든 참조 값 x,y,z에 대해 x.equals(y)가 true이고, y.equals(z)가 true면 x.equals(z)도 true다. 일관성(consistency) : null이 아닌 모든 참조 값 x,y에 대해 x.equals(y)를 반복해서 호출해도 항상 true 또는 false만을 반환 null-아님 : null이 아닌 모든 참조값 x에 대해 x.equals(null)은 false이다.대칭성(symmetry)public final class CaseInsenstiveString{ private final String s; public CaseInsensitiveString(String s){ this.s = Objects.requireNorNull(s); } }@Overridepublic boolean equals(Object o) { if (o instanceof CaseInsensitiveString) return s.equalsIgnoreCase( ((CaseInsensitiveString) o).s); if (o instanceof String) // 한 방향으로만 작동한다! return s.equalsIgnoreCase((String) o); return false;} CaseInsensitiveString.equals(String) 에서는 동작하지만 String.equals(CaseInsensitiveString)에서는 동작하지 않고 한방향으로만 작동하기 때문에 대칭성을 명백히 위반하게 됨.추이성(transitivity) null이 아닌 모든 참조 값 x,y,z에 대해 x.equals(y)가 true이고, y.equals(z)가 true면 x.equals(z)도 true다.public class Point { private final int x; private final int y; public Point(int x, int y) { this.x = x; this.y = y; } @Override public boolean equals(Object o) { if (!(o instanceof Point)) return false; Point p = (Point)o; return p.x == x &amp;&amp; p.y == y; }}public class ColorPoint extends Point { private final Color color; public ColorPoint(int x, int y, Color color) { super(x, y); this.color = color;}// 대칭성 위배@Overridepublic boolean equals(Object o) { if (!(o instanceof ColorPoint)) return false; return super.equals(o) &amp;&amp; ((ColorPoint) o).color == color;}// 추이성 위배@Overridepublic boolean equals(Object o) { if (!(o instanceof Point)) { return false; } // o가 일반 Point면 색상을 무시하고 비교한다. if (!(o instanceof ColorPoint)) { return o.equals(this); } ColorPoint cp = (ColorPoint) o; return cp.point.equals(point) &amp;&amp; cp.color.equals(color);} 대칭성 위배 코드를 살펴보면 Point의 equals는 색상을 무시하고, ColorPoint의 equals는 입력 매개변수의 클래스 종류가 다르다며 매번 false를 반환하게 된다. 추이성 위배 코드를 살펴보면 대칭성은 지켜주지만 추이성을 깨버린다. 구체 클래스를 확장해 새로운 값을 추가하면서 equals 규약을 만족시킬 방법은 존재하지 않는다…(객체 지향적 추상화의 이점을 포기하지 않는 한) 리스코프 치환 원칙(Liskov substitution principle) 어떤 타입에 잇어 중요한 속성이라면 그 하위 타입에서도 마찬가지로 중요. 타입의 모든 메서드가 하위 타입에서도 똑같이 잘 동작해야 함. Point를 기준으로 풀이하면 Point의 모든 하위 클래스는 정의상 모두 Point이므로 어디서든 Point 타입으로 활용될 수 있어야 한다는 말 // 리스코프 치환법칙 위배public boolean equals(Object o) { if (o == null || o.getClass() != getClass()) return false; Point p = (Point) o; return p.x == x &amp;&amp; p.y == y;} 위와 같이 equals를 구현해둔 경우에 Point의 하위 클래스를 정의하고 이를 Set과 같은 컬렉션에 넣어서 처리를 하고자하면 재대로 동작하지 않는걸 확인할 수 있다. 이 원인은 Colletion.contains 동작방식에 있음 Colletion.contains(Object o) 동작방식 대부분의 Collection interface 구현체 대부분은 contains을 판단하는데 있어서 객체의 equals 메서드를 사용하여 검증하므로 이 구현이 의도된대로 정의되어있어야 함. 이 문제를 해결할 수 있는 대표적인 방법은 “상속 대신 컴포지션을 사용하는 것”이다. 상위 클래스 타입의 변수를 멤버로 두고 그것을 반환하는 뷰 메소드를 public으로 작성한다. 아무 값도 갖지 않는 클래스를 베이스로 두고 확장한다.(베이스 클래스를 인스턴스하지 않기 때문에 위배되지 않는다) public class ColorPoint { private final Point point; private final Color color; public ColorPoint(int x, int y, Color color) { point = new Point(x, y); this.color = Objects.requireNonNull(color); } /** * 이 ColorPoint의 Point 뷰를 반환한다. */ public Point asPoint() { return point; } @Override public boolean equals(Object o) { if (!(o instanceof ColorPoint)) return false; ColorPoint cp = (ColorPoint) o; return cp.point.equals(point) &amp;&amp; cp.color.equals(color); }}일관성(consistency) null이 아닌 모든 참조 값 x,y에 대해 x.equals(y)를 반복해서 호출해도 항상 true 또는 false만을 반환 두 객체가 같다면 앞으로도 영원히 같아야 한다는 의미 클래스를 작성할때 불변 클래스로 만드는게 나을지를 심사숙고하자. equals의 판단에 신뢰할 수 없는 자원이 끼어들게 해서는안된다. java.net.URL의 equals가 예다. URL과 ip의 호스트를 비교하는데 이때 네트웍을 통하게 된다. null-아님 동치성 검사를 위해 적절한 형변환 후 값을 비교한다. instanceof는 비교하는 객체가 null 인지 검사한다.(그래서 ==null 을 할 필요 없다)// 명시적 null 검사 필요 없음.@Overridepublic boolean equals(Object o) { if(!(o instanceof MyType)) { return false; } MyType type = (MyType) o; ...}equals 메서드 단계별 구현 방법 1) == 연산자를 사용해 입력이 자기 자신의 참조인지 확인. 2) instanceof 연산자로 입력이 올바른 타입인지 확인. 3) 입력을 올바른 타입으로 형변환 4) 입력 객체와 자기 자신의 대응되는 핵심 필드들이 모두 일치하는지 하나씩 검사equals 관련 팁 primitive type (float, double 제외) 는 “==” 연산자로 비교하고, 레퍼런스 타입 필드는 equals 메서드로 비교한다. float,double은 wrapper class 의 static method인 compare 메서드로 비교한다. 이렇게 특별취급하는 이유는 NaN, -0.0f, 특수한 부정소수 값 등을 다뤄야 하기 때문 wrapper class의 equals를 사용할수도 있는데 오토박싱을 수반할 수 있어서 성능상 좋지 않을 수 있음 성능이 걱정된다면 cost가 적을 것 같은 필드부터 비교한다. 동기화용 락(lock) 필드 같은 논리적 상태와 관련 없는 필드는 연산하지 말자. (논리적 상태만을 비교하자) 캐쉬된 값을 저장하는 파생클래스 변수가 있을 경우 활용하자. 완벽한 equals 를 보여주는 코드 public final class PhoneNumber { private final short areaCode, prefix, lineNum; public PhoneNumber(int areaCode, int prefix, int lineNum) { this.areaCode = rangeCheck(areaCode, 999, \"지역코드\"); this.prefix = rangeCheck(prefix, 999, \"프리픽스\"); this.lineNum = rangeCheck(lineNum, 9999, \"가입자 번호\"); } private static short rangeCheck(int val, int max, String arg) { if (val &lt; 0 || val &gt; max) throw new IllegalArgumentException(arg + \": \" + val); return (short) val; } @Override public boolean equals(Object o) { if (o == this) return true; if (!(o instanceof PhoneNumber)) return false; PhoneNumber pn = (PhoneNumber)o; return pn.lineNum == lineNum &amp;&amp; pn.prefix == prefix &amp;&amp; pn.areaCode == areaCode; }} 추가 코멘트 Java Platform 내에서 equlas의 역할, 어떻게 동작하는가 이런 개념적인 내용은 Java 개발자라면 필수로 알고 있어야 하는 개념들이라고 생각합니다. 실제로 실무에서 이렇게 equlas 메서드를 직접 구현하는케이스는 상당히 드믈것이라 생각되는데요. 이미 Lombok 등 다른대안들이 많이 있으니깐요. 하지만 그렇다고 이 개념들을 알 필요가 없는것은 아닙니다.Item11. equals를 재정의하려거든 hashCode도 재정의하라 equals를 재정의한 클래스 모두에서 hashCode도 재정의해야 한다. (Lombok의 annotation이 @EqulasAndHashCode 로 묶어둔 이유기도 함.)hashCode 관련 규약(Object 명세) 1) equals 비교에 사용되는 정보가 변경되지 않았다면 해당 객체의 hashCode는 몇 번을 호출해도 일관된 값을 반환해야 한다. 2) equals(Object)가 두 객체를 같다고 판단했다면 hashCode도 같은 값을 반환해야 한다. 3) equals(Object)가 두 객체를 다르다고 판단했더라도, 두 객체의 hashCode가 서로 다른 값을 반환할 필요는 없다. 단, 다른 객체에 대해서는 다른 값을 반환해야 해시 테이블의 성능이 좋아진다는 것을 알아야 한다.hashCode 일반 규약을 어기는 경우 HashMap, hashSet등에서 문제가 발생한다. 위 2번 조항의 내용과 연관성이 있는데 같은 객체라면 같은 해시코드를 반환해야 hashMap, hashSet에 유일하게 저장될 수 있다.HashMap&lt;PhonNumber,String&gt; m = new Hashmap&lt;&gt;();m.put(new PhoneNumber(707, 867, 5309),\"jenny\");m.get(new PhoneNumber(707, 867, 5309)); jenny가 나올것 같지만, 실제 결과는 null 넣을때 한번, 꺼낼때 한번 객체를 생성했다. 이들은 논리적 동치이나. 둘의 hashCode가 다르기 때문에 HashMap에서 찾을 수 없음.이 케이스로 꼭 해야 겠다면 put한 객체를 로컬변수로 저장한 뒤 해당 객체를 get()의 파라메터로 넘겨야 한다.절대로 하지 말아야 할 hashCode 구현 행위@overridepublic int hashcode(){ return 42;} 학창시절 HashTable의 시간복잡도를 배우면서 O(1)의 엄청난 성능을 보이는것으로 배웠을텐데, 위처럼 모든 객체가 같은 hashCode를 return하도록 한다면 O(n)의 성능을 만들어낼 수 있다. 모든 해쉬가 같기 때문에 결국 Linked List처럼 동작하게 된다. hash 관련된 내용이잘 설명되어 있는 블로그일반적으로 hashCoed method를 만드는 방법@Override// PhoneNumber.java의 예public int hashCode() { int result = Short.hashCode(areaCode); result = 31 * result + Short.hashCode(prefix); result = 31 * result + Short.hashCode(lineNum); return result;} 31을 곱하는 이유는 31이 홀수이면서 소수(prime)이기 떄문 이렇게 복잡한 연산으로 hashCode를 만드는 이유는 hash conflict를 최대한 막아 hash 분포를 잘되게 하여 좋은 성능을 유지시키기 위함이다. hash conflict을 더욱 적은 방법을 쓰려고 할떄 고려사항 구아바의 com.google.common.hash.Hashing 참조 조금 살펴봤는데 충돌을 피하기 위한 여러 알고리즘을 적용된 HashFunction을 생성해주는 Util Class네요. 실제로 매우 많은 데이터를 JVM 위에 올려놓고 하나의 Hash Collection을 이용하는 경우 특정 모델의 성능 향상을 위해서 저런 Util을 써볼수 있을 것 같은데 일반적인 엔터프라이즈 웹 애플리케이션 개발에서는 사용하지 않을 가능성이 높아 보이긴 합니다. 성능을 고려하는 목적으로 핵심필드를 빼고 hashcode를 정의하지 말자.(해시테이블 성능을 놓치게된다)Item12. toString을 항상 재정의하라. default 동작은 “클래스_이름@16진수로_표시한_해시코드”가 반환됨. 위 정보는 아주 쓸모가 없음..항상 적합한 문자열을 반환하지 않는다. PhoneNumber@adbbb (클래스이름@16진수해쉬코드) 를 반환한다. 하위 클래스에서는 간결하고 읽기 쉬운(핵심필드 들을) 형태로 toString을 정의해주는 것이 좋다. 문제(디버깅)에 용이하게 만든다. 모든 핵심 필드들을 출력하는 것이 좋다.``` /** 이 전화번호의 문자열 표현을 반환한다. 이 문자열은 “XXX-YYY-ZZZZ” 형태의 12글자로 구성된다. XXX는 지역 코드, YYY는 프리픽스, ZZZZ는 가입자 번호다. 각각의 대문자는 10진수 숫자 하나를 나타낸다. * 전화번호의 각 부분의 값이 너무 작아서 자릿수를 채울 수 없다면, 앞에서부터 0으로 채워나간다. 예컨대 가입자 번호가 123이라면 전화번호의 마지막 네 문자는 “0123”이 된다. */@Overridepublic String toString() { return String.format(“%03d-%03d-%04d”, areaCode, prefix, lineNum);}``` 하위 클래스에서 상위클래스의 적절한 toString이 있다면 말고 없다면 꼭 구현해주는것이 좋다.코멘트 사실 여기에 대한 생각은 log를 남기거나 하는 경우에 해당 클래스의 정보를 표시해주어야 하므로 그런 경우에만 toString을 구현해주는것이 맞는듯 하기도 하다. Lombok을 사용하는 경우에 @ToString만 붙여주면 아주 예쁘게 노출되기 떄문에 실제로 어려운일이 아니기도 하고 말이다. 사실 ToString 관련해서는 책에서 다루는 개념적으로 아주 중요한 내용은 없는듯 하다.Item13. clone 재 정의는 주의해서 진행해라.Cloneable interface 복제해도 되는 클래스임을 명시하는 용도의 믹스인 인터페이스 아쉽게도 의도한 목적을 제대로 이루지 못했음.. 가장 큰 문제는 clone 메서드가 선언된 곳이 Cloneable이 아닌 Object라는 것(심지어 protected ) 이런 내용을 보면 Cloneable 인터페이스가 하는일이 없어보이지만 동작방식을 살펴보면 용도가 있긴 있다.Object.clone 동작 방식 Object.clone은 Cloneable을 구현한 클래스의 인스턴스에서 호출하면 해당 객체의 필드들을 하나하나 복사한 객체를 반환 그렇지 않은 클래스의 인스턴스에서 호출하면 CloneNotSupportedExpcetion을 반환한다.clone으로 파생되는 문제 Cloneable을 구현한 클래스는 clone 메서드를 public으로 제공하며, 사용자는 당연히 복제가 재대로 이뤄지리라 기대한다. 하지만 모순적인 매커니즘이 탄생할수 있고, 생성자를 호출하지 않고도 객체를 생성하면서 모순적인 매커니즘이 탄생한다. 상속 구조에 있는 클래스에서 clone 메서드가 super.clone이 아닌 생성자를 호출해 얻은 인스턴스를 반환하더라도 컴파일 에러는 없을텐데, 이 경우 하위 클래스에서 super.clone()을 호출한 경우 잘못된 클래스의 객체가 만들어져서 결국 하위 클래스의 clone 메서드가 재대로 동작하지 않을수 있음. 쓸데없는 복사를 지양한다는 관점에서는 굳이 clone 메서드를 제공하지 않는게 좋다.공변 반환 타이핑(convariant return typing) 재정의한 메서드의 반환 타입은 상위 클래스의 메서드의 반환하는 타입의 하위 타입일 수 있다는 의미 이 주제를 벗어나기도 해서 일단은 간략하게 이야기하면 클래스 상속 구조가 아래와 같다고 할때 return type이 Animal이더라도 Dog 객체는 Animal을 구현한 하위 클래스의 객체를 반환할 수 있다고 쉽게 생각해도 좋을 것 같다.private static class Dog extends Animal { ...}private static class Animal { ...}클래스가 가변 객체를 참조하느 경우의 clone// Stack의 복제 가능 버전 (80-81쪽)public class Stack implements Cloneable { private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { this.elements = new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public Object pop() { if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; // 다 쓴 참조 해제 return result; } public boolean isEmpty() { return size ==0; } // 원소를 위한 공간을 적어도 하나 이상 확보한다. private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} clone 메서드가 단순히 super.clone의 결과를 그대로 반환한다면 size 필드는 올바른 값을 갖겠지만, elements 필드는 원본 Stack 인스턴스와 똑같은 배열을 참조하고 있다. 원본이나 복제본 중 하나를 수정하면 다른 하나도 수정되어 불변식을 해칠수 있고, 프로그램이 이상하게 동작하거나 NPE가 발생할 수 있다.(생성자를 호출해서 만든 객체였다면 이런일은 벌어지지 않았을것이지만, clone은 사실상 생성자와 같은 효과를 냄) clone은 원본 객체에 아무런 해를 끼치지 않는 동시에 복제된 객체의 불변식을 보장해야 함. // 코드 13-2 가변 상태를 참조하는 클래스용 clone 메서드 @Override public Stack clone() { try { Stack result = (Stack) super.clone(); result.elements = elements.clone(); return result; } catch (CloneNotSupportedException e) { throw new AssertionError(); } } 이렇게 처리한다면 위 문제를 해결할수 있겠지만, Cloneable 아키텍처는 ‘가변 객체를 참조하는 필드는 final로 선언하라’는 일반 용법과 충돌이 발생함.(여기서 elemtns와 같은 가변 객체 참조 필드는 실제로 final로 선언되어있어야 하는데 그렇다면 위와 같은 clone 방식으로 처리할 수가 없는것임)clone 정의시 주의사항 생성자에서는 재정의될 수 있는 메서드를 호출하지 않아야 함. 상속용 클래스는 Cloneable을 구현해서는 안된다. Cloneable을 구현한 스레드 안전 클래스를 작성할 때는 clone 메서드 역시 적절히 동기화해줘야 한다.clone 정의 방법 Clonealbe을 구현하는 모든 클래스는 clone을 재정의 해야 하고, 접근제한자는 public으로 반환 타입은 클래스 자신으로 변경한다. super.clone() 호출 후 필요한 필드를 전부 적절히 수정(모든 가변 객체를 복사해야함.)언제 clone을 구현해야 할까? 위에 나열되있는것처럼 모든 가변 객체를 copy해주는 등 clone 구현시에는 복잡한 작업들이 뒤따른다. 그러면 꼭 이렇게까지 구현을 해야하는것일까? Cloneable을 이미 구현한 클래스를확장하는 경우 어쩔 수 없이 clone을 잘 작동하도록 구현해야 한다. 그렇지 않은 상황에서는 더 나은 객체복사 방식을 사용하자(복사 생성자, 복사 팩토리와 같은)// 복사 생성자(conversion constructor)public Yum(Yum yum) { ... };// 복사 팩터리(conversion factory)public static Yum newInstance(Yum yum) { ... };복사 생성자와 복사 팩터리가 Cloneable /clone 방식보다 나은 이유 언어 모순적이고 위험천만한 객체 생성 매니즘(생성자를 쓰지 않는 방식)을 사용하지 않음. 정상적인 final 필드 용법과 충돌하지 않음 불필요한 검사 예외를 던지지 않고 형변환도 필요 없음 해당 클래스가 구현한 인터페이스 타입의 인스턴스도 인수로 받을수 있음.핵심 정리 새로운 인터페이스를 만들 때는 절대 Cloneable을 확장해서는 안되며, 새로운 클래스도 이를 구현해서는 안된다. (final class라면 위험이 크지는 않음) 성능 최적화 관점에서 검토한 후 별다른 문제가 없을 때만 드믈게 허용해야 한다. 복제 기능은 생성자와 팩토리를 이용하는게 최고이나 배열만은 clone 메서드 방식이 가장 깔끔한, 이 규칙의 합당한 예외Item14. Comparable을 구현할지 고려하라. Comparable interface의 유일무이한 메서드인 compareTocompareTo는 Object의 메서드가 아님.위 2가지를 제외하고는 Object의 equals와 같지만, compareTo는 단순 동치성 비교에 더해 순서까지 비교할 수 있으며, 제네릭한 특징을 가진다. Comparable을 구현한다는 것은 그 클래스 인스턴스간에 자연적인 순서(natural order)가 있음을 뜻한다.pulbic interface Comparable&lt;T&gt; { int compareTo(T t);}Comparable.compareTo 메서드 일반 규약 이 객체가 주어진 객체보다 작으면 음의 정수, 같으면 0, 크면 양의 정수 반환(비교불가한 경우 ClassCastException 발생) Comparable을 구현한 클래스는 모든 x,y에 대해 sgn(x.compareTo(y)) == -sgn(y.compareTo(x))여야 한다. 추이성을 보장해야 한다. ( x.compareTo(y) &gt; 0 &amp;&amp; y.compareTo(z) &gt; 0 이면 x.compareTo(z) &gt; 0 모든 z에 대해 x.compareTo(y) == 0이면 sgn(x.compareTo(z)) == sgn(y.compareTo(z))이다. (x.compareTo(y) ==0) == (x.equals(y)) =&gt; 필수는 아니지만 권고사항정렬된 컬렉션에서의 동치성 비교 정렬된 컬렉션들은 일반적으로 equals 대신 compareTo를 사용하여 동치성 검증을 한다. compareTo, equals가 일관되지 않은 BigDecimal 클래스 예제 HashSet&lt;BigDecimal&gt; hashSet = new HashSet(); TreeSet&lt;BigDecimal&gt; treeSet = new TreeSet&lt;&gt;(); BigDecimal a = new BigDecimal(\"1.0\"); BigDecimal b = new BigDecimal(\"1.00\"); hashSet.add(a); hashSet.add(b); // [1.0, 1.00] System.out.println(hashSet); treeSet.add(a); treeSet.add(b); // [1.0] System.out.println(treeSet); hashSet에서는 equals 메서드로 비교하여 2개의 객체 인스턴스가 다르다고 판단한다. TreeSet에서는 compareTo 메서드로 비교하기 떄문에 BigDecimal 인스턴스를 같다고 판단한다.compareTo 작성시 주의사항 Comparable은 타입을 인수로 받는 제네릭 인터페이스이므로 compareTo 메서드 인수 타입은 컴파일 타임에 정해진다.(인수 타입 확인 및 형변환 필요 없음) 각 필드가 동치인지 비교하는게 아니라 순서를 비교, 객체 참조 필드를 비교하려면 compareTo 메서드를 재귀적으로 호출한다. Comparable을 구현하지 않은 필드나 표준이 아닌 순서로 비교해야 한다면 Comparator을 대신 사용Java8 이후의 Compare Comparator 인터페이스가 일련의 비교자 생성 메서드(com-parator construction method)와 팀을 꾸려 메서드 연쇄 방식으로 비교자를 생성할 수 있게 됨. d이 방식이 간결하기는 하나 약간의 성능저하가 뒤따를수 있음.private static final Comparator&lt;PhoneNumber&gt; COMPARATOR = comparingInt((PhoneNumber pn) -&gt; pn.areaCode) .thenComparingInt(pn -&gt; pn.prefix) .thenComparingInt(pn -&gt; pn.lineNum);public int compareTo(PhoneNumber pn) { return COMPARATOR.compare(this, pn); } comparingInt((PhoneNumber pn) -&gt; pn.areaCode)에서 명시적 Casting한 부분을 주목해보면, 자바의 타입 추론 능력이 이 람다에서 타입을 알아낼만큼 강력하지 않기 때문에 프로그램이 컴파일되도록 명시적 Casting을 처리해준 것.hashCode 값을 기준으로 하는 비교자 // 이 방식은 사용해서는 안됨. 정수 오버플로우를 일으키거나 부동수점 계산 방식에 따른 오류를 낼수 있음. ( 추이성 위배) static Comparator&lt;Object&gt; hashCodeOrder = new Comparator&lt;&gt;() { public int compare(Object o1, Object o2) { return o1.hashCode() - o2.hashCode() }}// 정적 compare 메서드를 활용한 비교자 static Comparator&lt;Object&gt; hashCodeOrder = new Comparator&lt;&gt;() { public int compare(Object o1, Object o2) { return Interger.compare(o1.hashCode(), o2.hashCode()); }}// 정적 compare 메서드를 활용한 비교자 static Comparator&lt;Object&gt; hashCodeOrder = Comparator.comparingInt(o -&gt; o.hashCode());}핵심정리 순서를 고려해야 하는 값 클래스를 작성한다면 꼭 Comparable 인터페이스를 구현하여 그 인스턴스들을 쉽게 정렬하고, 검색하고, 비교 기능을 제공하는 컬렉션과 어우러지도록 해야 한다. compareTo 메서드에서 필드의 값을 비교할때는 “&lt;”, “&gt;” 사용하지 말아야 한다. 박싱된 기본 타입 클래스에서 제공하는 정적 compare 메서드나 Comparator인터페이스가 제공하는 비교자 생성 메서드를 사용하자추가 코멘트 equals, hashCode, Comparable 같은 경우 자바를 이용하는 개발자라면 필수적으로 개념을 확실히 알고 있어야 하지만, 간혹 놓치고 개발하다가 치명적인 버그로 이어지는 경우가 있다. 이런 케이스로 발생하는 버그는 코드를 읽으면서 쉽게 발견되지 않기 때문에 잘 알고 사용하는것이 중요하다. equals, hashCode는 사실상 Lombok같은 라이브러리를 통해서 이용한다면 실수는 거의 없을 것이라고 생각된다. clone은 실제로 직접 개발한 코드에서 정의해서 사용해본 적은 없다. 책에서도 정리된 것처럼 가급적이면 해당 메서드를 구현해야 할 필요성을 못느끼는데 레거시 코드에서 혹시 clone이 정의되어있고, 내가 추가로 개발한 부분이 연관성이 있다면 그떄 관련 개념을 좀 알고 있는것이 도움이 될 것 같다." }, { "title": "Spring Boot JSP 자동 reload 되지 않는 문제 해결 방법", "url": "/posts/devlog-spring-boot-tools/", "categories": "DevLog, Spring", "tags": "Java, Spring, Back-end", "date": "2020-05-03 17:34:00 +0900", "snippet": "Spring Boot JSP 자동 reload 되지 않는 문제 해결 방법내용 legacy Spring Project에서 Spring boot로 전환되었을때 즉시 발견할수 있는 문제중 하나는 IDE에서 jsp파일을 수정해도 auto reload되지 않는 문제입니다.이 경우 JSP에 수정사항을 반영하기 위해서는 Application을 재시작해야 하는데...", "content": "Spring Boot JSP 자동 reload 되지 않는 문제 해결 방법내용 legacy Spring Project에서 Spring boot로 전환되었을때 즉시 발견할수 있는 문제중 하나는 IDE에서 jsp파일을 수정해도 auto reload되지 않는 문제입니다.이 경우 JSP에 수정사항을 반영하기 위해서는 Application을 재시작해야 하는데요. 매번 이런 번거로움을 감수하면서 개발하는 것은 말도 안되죠.이를 해결할수 있는 방안이 이미 마련 되어 있습니다.적용 방법1. Dependency 추가 maven 을 사용하는 경우&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; gradle을 사용하는 경우dependencies { compile(\"org.springframework.boot:spring-boot-devtools\")}2. spring application.yml 또는 application.properties에 아래 설정을 추가spring.devtools.livereload.enabled=truespring: devtools: livereload: enabled: trueReference https://docs.spring.io/spring-boot/docs/current/reference/html/using-spring-boot.html#using-boot-devtools-livereload" }, { "title": "Spring Boot + JPA + H2 Application 간단하게 띄우기", "url": "/posts/devlog-spring-boot-application/", "categories": "DevLog, Spring", "tags": "Java, Spring, Back-end", "date": "2020-05-01 01:34:00 +0900", "snippet": "Spring Boot + JPA + H2 Application 간단하게 띄우기 1. 프로젝트 만들기 2. 애플리케이션 기본 설정 3. Web MVC Configuration 4. JSP Configration 5. JPA Configration1. 프로젝트 만들기1.1 spring initializr을 통해 프로젝트 뼈대 만들기-https:/...", "content": "Spring Boot + JPA + H2 Application 간단하게 띄우기 1. 프로젝트 만들기 2. 애플리케이션 기본 설정 3. Web MVC Configuration 4. JSP Configration 5. JPA Configration1. 프로젝트 만들기1.1 spring initializr을 통해 프로젝트 뼈대 만들기-https://start.spring.io/ 에서 손쉽게 dependency 가 추가된 프로젝트 틀을 만들수 있다. 하지만 이렇게 만든다고 끝은 아니고.. 추가로 이것저것 설정 등을 해주어야 한다.2. 애플리케이션 기본 설정 아래와 같이 @SpringBootApplication만 붙이면 바로 Application을 구동시킬수 있다. @SpringBootApplication(scanBasePackageClasses = {RootConfig.class})public class Application extends SpringBootServletInitializer { public static void main(String[] args) { SpringApplication app = new SpringApplication(Application.class); app.addListeners(new ApplicationPidFileWriter()); app.run(args); } @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) { return application.sources(Application.class); }} 하지만 일반적으로 만들고자 하는 웹 애플리케이션을 만들려면 추가적인 설정들이 필요한데 필자는 scanBasePackageClasses 속성을 이용해서 RootConfig Configuration class를 만들고 이를 기준으로 처리하도록 하였다.@Configuration@ComponentScan(basePackageClasses = ApplicationPackageRoot.class)public class RootConfig {} 여기서 특정 패키지 루트 하위에 있는 모든 bean 컴포넌트들을 스캔시키기 위해 최상단에 ApplicationPackageRoot 라는 inteface를 만들고 이 패키지 위치를 기준으로 @ComponentScan 처리하도록 했음. 참고로 @Configuration annotation을 사용하면 우리가 일반적으로 AutoConfigration을 위해 사용하는 @Enable{XXX} 류의 설정용 Bean을 생성하여 사용할 수 있다. 이런식으로 프로젝트 구조를 잡고 설정을 했을떄 패키지 구조를 살펴보면 다음과 같다.3. Web MVC Configuration 먼저 dependency를 추가하자.&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; EnableWebMvc를 통해 기본적인 mvc 설정은 모두 커버가 된다. 자세한 내용이 알고 싶다면 DelegatingWebMvcConfiguration.class 파일을 참조하도록 하자@EnableWebMvcpublic class WebMvcConfig implements WebMvcConfigurer { @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\"/static/bundle/**\") .addResourceLocations(\"/static/bundle/\").setCachePeriod(3600) .resourceChain(true).addResolver(new PathResourceResolver()); }}4. JSP Configration jsp를 사용하려면 일단 pom.xml에 dependancy를 추가해주어야 한다.&lt;!-- jsp --&gt;&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt;\t&lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;\t&lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt;\t&lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt;\t&lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;\t&lt;groupId&gt;javax.servlet&lt;/groupId&gt;\t&lt;artifactId&gt;jstl&lt;/artifactId&gt;&lt;/dependency&gt; 위 디펜던시를 추가하지 않으면 jsp 파일을 찾을수 없다고 404 Page Not Found를 맞이할 수 있을 것이다. 그리고 템플릿 엔진으로 jsp를 사용하기 위해 필자가 추가로 한 부분은 internalResourceViewResolver Bean을 추가한 것이다. @Configuration@EnableWebMvcpublic class WebMvcConfig implements WebMvcConfigurer { @Override public void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\"/static/bundle/**\") .addResourceLocations(\"/static/bundle/\").setCachePeriod(3600) .resourceChain(true).addResolver(new PathResourceResolver()); } @Bean public ViewResolver internalResourceViewResolver() { final InternalResourceViewResolver viewResolver = new InternalResourceViewResolver(); viewResolver.setPrefix(\"/WEB-INF/templates/\"); viewResolver.setSuffix(\".jsp\"); return viewResolver; }} 이를 통해 jsp web 페이지를 랜더링할 수 있다. 여기서 이런 방식 말고 다른 방법으로 처리할수도 있는데 그 방법은 application.yml 파일에 설정을 추가해주는 것이다. spring: mvc: view: prefix: /WEB-INF/templates/ suffix: .jsp 5. JPA Configration5.1 DataSource 실제 웹 애플리케이션을 운영하는 것이 아니라 샘플 프로젝트를 만들고 연습하기에 가장 쉬운 방법은 H2 in memory DB를 사용하는 것일 것이다. 이를 사용하기란 매우 간단하다. 먼저 h2 dependency를 pom.xml에 추가한다.&lt;!-- db --&gt;&lt;dependency&gt;\t&lt;groupId&gt;com.h2database&lt;/groupId&gt;\t&lt;artifactId&gt;h2&lt;/artifactId&gt;\t&lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; PersistenceConfigration에서 DataSource Bean을 정의한다. @Beanpublic DataSource dataSource() { EmbeddedDatabaseBuilder builder = new EmbeddedDatabaseBuilder(); return builder.setType(EmbeddedDatabaseType.H2).build();} 이제 H2를 사용할 준비는 끝났다. 이제 다음으로 넘어가서 JPA 설정을 해보자5.2 JPA 여기서 JPA라고 이야기했지만, 실제로는 JPA 구현체 중에서 유명한 것 중 하나인 hibernate를 사용할 것이다. 이를 사용하기 위해서는 먼저 dependency를 추가해주어야 한다.&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;\t&lt;groupId&gt;org.hibernate&lt;/groupId&gt;\t&lt;artifactId&gt;hibernate-ehcache&lt;/artifactId&gt;\t&lt;version&gt;5.4.14.Final&lt;/version&gt;&lt;/dependency&gt; 그리고 이와 관련된 configuration 값들을 정의해준다. spring: datasource: driver-class-name: org.h2.Driver url: jdbc:h2:mem:test;DB_CLOSE_DELAY=-1 username: password: h2: console: enabled: true path: /h2 jpa: database-platform: org.hibernate.dialect.H2Dialect database: H2 generate-ddl: false open-in-view: false show-sql: true hibernate: ddl-auto: create naming: physical-strategy: org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl implicit-strategy: org.hibernate.boot.model.naming.ImplicitNamingStrategyLegacyJpaImpl properties: hibernate: format_sql: true use_sql_comments: true cache.use_second_level_cache: true cache.use_query_cache: false generate_statistics: true cache.region.factory_class: org.hibernate.cache.ehcache.EhCacheRegionFactory 이 외에도 hibernate 설정은 아주 다양하게 많이 있을텐데, 자세한 것은 공식 문서를 참조하도록 하자. ( https://hibernate.org/orm/documentation/5.4/ ) 사실 위에서 가장 중요한 부분은 “hibernate.ddl-auto” 설정 값인데 이 값은 만약 실제 서비스에서 사용하고자 한다면 “validate” 정도의 설정값을 사용하는게 좋다. 하지만 예제에서는 인메모리 DB를 사용하는 특성상 create를 사용한것이니 참고하자. application.yml에 설정값을 넣어두었다면 이제 Java Configration 을 정의해보자 @Configuration@EnableJpaRepositories(basePackages = \"com.sungsu.boilerplate.app\")@EnableTransactionManagementpublic class PersistenceConfig { @Bean public DataSource dataSource() { EmbeddedDatabaseBuilder builder = new EmbeddedDatabaseBuilder(); return builder.setType(EmbeddedDatabaseType.H2).build(); } @Bean public PlatformTransactionManager transactionManager(EntityManagerFactory entityManagerFactory) { JpaTransactionManager txManager = new JpaTransactionManager(); txManager.setEntityManagerFactory(entityManagerFactory); return txManager; }} @EnableJpaRepositories의 basePackages 하위에 있는 @Entity 와 @Repository를 자동 등록이 된다. EnableJpaRepositories를 보면 default로 “transactionManager” 를 사용하기 때문에 PlatformTransactionManager Type의 클래스로 bean 등록을 해주면 된다. 이렇게 하면 H2 + JPA application이 완성되었다.ResultconsolewebComment 작성된 내용 중 혹시 제가 잘못 알고 있는 내용이 있다면 언제든지 코멘트 부탁드립니다. 이 포스팅을 쓰면서 만든 샘플코드는 github에 올려두었다. 혹시 신규 프로젝트를 시작하고자할떄 필요하다면 언제든지 사용하셔도 됩니다. https://github.com/sungsu9022/sprping-boot-boilerplate " }, { "title": "이펙티브 자바 3판 - 2. 객체 생성과 파괴 - 2", "url": "/posts/devlog-effective-java-3e-2-2/", "categories": "DevLog, Java", "tags": "Effective Java", "date": "2020-04-28 01:34:00 +0900", "snippet": "이펙티브 자바 3판 - 2. 객체 생성과 파괴 - 2 Item5. 자원을 직접 명시하지 말고 의존 객체 주입을 사용하라. Item6. 불필요한 객체 생성을 피하라 Item7. 다 쓴 객체 참조를 해제하라 Item8. finalizer 와 cleaner 의 사용을 피하라 Item9. 생성자에 매개변수가 많다면 빌더를 고려하라Item5. 자원을 ...", "content": "이펙티브 자바 3판 - 2. 객체 생성과 파괴 - 2 Item5. 자원을 직접 명시하지 말고 의존 객체 주입을 사용하라. Item6. 불필요한 객체 생성을 피하라 Item7. 다 쓴 객체 참조를 해제하라 Item8. finalizer 와 cleaner 의 사용을 피하라 Item9. 생성자에 매개변수가 많다면 빌더를 고려하라Item5. 자원을 직접 명시하지 말고 의존 객체 주입을 사용하라. 사용하는 자원에 따라 동작이 달라지는 클래스에는 정적 유틸리티 클래스나 싱글턴 방식이 적합하지 않음. 인스턴스를 생성할 때 생성자에 필요한 자원을 넘겨주는 방식을 사용 의존 객체 주입은 유연성과 테스트에 용이하다. public class SpellChecker { private final Lexicon dictionary; public SpellChecker(Lexicon dictionary) { this.dictionary = Objects.requireNonNull(dictionary); } public boolean isVlaid(String word) { ...\t} public List&lt;String&gt; suggestions(String type) { ...\t}} 의존 객체 주입을 생성자, 정적 팩터리, 빌더 등 아무런 방법에 적용하면 됨.팩터리 메서드 패턴 의존 객체 주입의 쓸만한 변형 방식 생성자에 자원 팩터리 객체를 넘겨주는 방식 Supplier 인터페이스를 사용하면 됨. 한정적 와일드카드 타입(bounded wildcard type)을 사용해 팩터리의 타입 매개변수를 제한 Mosaic create(Supplier&lt;? extends Tile&gt; titleFactory) { ... } 의존 객체 주입이 유연성과 테스트 용이성을 개선해주긴 하지만, 의존성이 많아지면 코드를 어렵게 만들기도 함.핵심 정리자원이 클래스 동작에 영향을 준다면 싱글턴과 정적 유틸리티 클래스는 사용하지 않는 것이 좋다.(의존 객체 주입을 통해 하자)Item6. 불필요한 객체 생성을 피하라똑같은 기능의 객체를 매번 생성하기 보다는 객체 하나를 재사용하는 편이 나을 때가 많다.(당연한 이야기)String 인스턴스 관련String s = new String(\"bikini\"); // 따라하지 말것.String s = \"bikini\"; 생성자로 생성하는 케이스는 매번 새로운 String 인스턴스를 생성한다. 2번쨰 방식을 사용하면 하나의 String 인스턴스를 사용하고, 가상 머신 안에서 이와 똑같은 문자열 리터럴을 사용하는 모든 코드가 같은 객체를 재사용함이 보장된다. 참조 : https://docs.oracle.com/javase/specs/jls/se7/html/jls-3.html#jls-3.10.5 Boolean(String) 생성자 «« Boolean.valueOf(String)생성비용이 비싼 객체 처리 비싼 객체가 반복해서 필요하다면 캐싱하여 재사용한다. String.matches 메소드를 쓰면 간편하지만, 성능이 중요한 상황에서 반복해서 사용하기엔 적합하지 않음. 해당 메소드에서 생성하는 Pattren 객체는 한번 쓰고 버려짐. Pattren 유한 상태 머신(finite sate machine)을 만들기 때문에 인스턴스 생성 비용이 높음. Regular Expression -&gt; Pattren 객체를 이용 String.matches vs Pattern.matchers 성능 비교 1.1마이크로s / 0.17 마이크로s 오토박싱 오토박싱이란 기본타입과 박싱된 기본 타입을 섞어 쓸때 자동으로 상호 변환해주는 기술. 불필요한 객체를 만들어내는 예 중 하나이다. 오토 박싱이 기본 타입과 그에 대응하는 박싱된 타입의 구분을 흐려주지만, 완전히 없애주는 것은 아님. private static long sum() { Long sum = 0L; for (long i = 0; i &lt;= Integer.MAX_VALUE; i++) sum += i; return sum; } sum 변수의 long이 아닌 Long으로 선언해서 불필요한 Long 인스턴스가 생성됨. 박싱된 타입보다는 기본 타입을 사용하고, 의도치 않은 오토박싱이 숨어들지 않도록 주의하자.객체 생성이 비싸니 피해야 한다(?) -&gt; 객체 풀 아주 무거운 객체가 아닌 다음에야 단순히 객체 생성을 피하고자 객체 풀을 만들다던지는 안하는 것이 좋음. DB 연결같은 경우 생성비용이 비싸니 재사용하는 편이 낫지만 그렇지 않은 경우가 많음. 객체 풀은 코드를 헷갈리게 만들고 메모리 사용량을 늘리고, 성능을 떨어뜨린다.방어적 복사 vs 불필요한 객체 생성 방어적 복사가 필요한 상황에서 객체를 재사용했을 때의 피해는 필요 없는 객체를 반복 생성했을 때의 피해보다 훨씬 크다. 잘 모르면 차라리 불필요한 객체 생성은 여러번 하는게 나을수도 있음. Item7. 다 쓴 객체 참조를 해제하라메모리 누수public class Stack { private Object[] elements; private int size = 0; private static final int DEFAULT_INITIAL_CAPACITY = 16; public Stack() { elements = new Object[DEFAULT_INITIAL_CAPACITY]; } public void push(Object e) { ensureCapacity(); elements[size++] = e; } public Object pop() { if (size == 0) throw new EmptyStackException(); return elements[--size]; } /** * 원소를 위한 공간을 적어도 하나 이상 확보한다. * 배열 크기를 늘려야 할 때마다 대략 두 배씩 늘린다. */ private void ensureCapacity() { if (elements.length == size) elements = Arrays.copyOf(elements, 2 * size + 1); }} 위 스택을 오래 수행하다보면 점차 가비지 컬렉션 활동과 메모리 사용량이 늘어나 성능 저하 발생 메모리 누수의 원인? 객체들의 다 쓴 참조(obsolete reference)을 여전히 가지고 있기 때문. ( elements 배열의 활성 영역 밖 ) 해법 해당 참조를 다 썼을 때 null 처리(참조 해제) - pop 시점에 null 처리 public Object pop() { if (size == 0) throw new EmptyStackException(); Object result = elements[--size]; elements[size] = null; // 다쓴 참조 해제 return result;} 객체 참조를 null 처리 해야 하는 경우 객체 참조를 null 처리하는 일은 예외적인 경유여야 한다. 가장 좋은 참조 해제 방법 참조를 담은 변수를 유효 범위(scope) 밖으로 밀어내는 것 자기 메모리를 직접 관리하는 클래스 인 경우 프로그래머가 항상 메모리 누수에 주의해야 함. 캐시 역시 메모리 누수를 일으키는 주범 WeakHashMap http://blog.breakingthat.com/2018/08/26/java-collection-map-weakhashmap/ key의 참조가 사라지면 자동으로 GC 대상이 됨 정확히 이런 케이스에서만 유용```public class WeakHashMapTest { public static void main(String[] args) { WeakHashMap&lt;Integer, String&gt; map = new WeakHashMap&lt;&gt;(); Integer key1 = 1000; Integer key2 = 2000; map.put(key1, \"test a\"); map.put(key2, \"test b\"); key1 = null; System.gc(); //강제 Garbage Collection map.entrySet().stream().forEach(el -&gt; System.out.println(el)); }}``` LinkedHashMap http://javafactory.tistory.com/735 리스너(Listener) 혹은 콜백(Callback) 클라이언트 코드에서 콜백을 등록만 하고 명확히 해제하지 않는 경우에 발생할 수 있음. 콜백을 약한 참조(weak reference)로 저장하면 즉시 수거 ( WekHashMap에 키로 저장) 핵심 정리 메모리 누수는 겉으로 잘 드러나지 않음. 철저한 코드리뷰 힙 프로파일러 같은 디버깅 도구를 동원해야만 발견되기도 함. 즉 발견하기 어렵기 때문에 예방법을 잘 익히자!Item8. finalizer 와 cleaner 의 사용을 피하라GC는 컨트롤 가능한가? 내가 원할때 소멸시키는가 / 아니다. finalizer의 대안 cleaner 역시 문제가 많다. try with resource(auto closable) vs finalize gc 성능이 50배(12ns vs 550ns) 차이 난다. 그럼 언제 저것들을 쓰고 있나? / 효과있나? 닫지 않은 파일/커넥션등을 아주~늦게 나마 회수해준다.(FileInputStream, ThreadPoolExecutor) 네이티브피어(jni 같이 c 등 다른 언어 메소드를 연결하는 것) 객체(자바 객체가 아니니 알지 못해서) //이때는 성능저하가 불가피 할 듯 보이고 close()를 꼭 해야할 것 같다. 이 대안은 그럼 무엇? AutoCloseable을 구현한다.``` javapublic class Room implements AutoCloseable { private static final Cleaner cleaner = Cleaner.create(); // 청소가 필요한 자원. 절대 Room을 참조해서는 안 된다! private static class State implements Runnable { int numJunkPiles; // Number of junk piles in this room State(int numJunkPiles) { this.numJunkPiles = numJunkPiles; } // close 메서드나 cleaner가 호출한다. @Override public void run() { System.out.println(\"Cleaning room\"); numJunkPiles = 0; } } // 방의 상태. cleanable과 공유한다. private final State state; // cleanable 객체. 수거 대상이 되면 방을 청소한다. private final Cleaner.Cleanable cleanable; public Room(int numJunkPiles) { state = new State(numJunkPiles); cleanable = cleaner.register(this, state); } @Override public void close() { cleanable.clean(); }}``` 사실 위의 코드는 정확한 흐름을 모르겠다.(수정필요) 그럼 힙 메모리 세팅, gc 종류 선택 등에 대해 공유(난 경험이 없다..)Item9. try - finally 보다 try-with-resource를 사용하라 자원(파일, 커넥션) 을 닫는 것을 클라이언트가 놓칠 수 있다.(커넥션이 계속 열고 안닫아지면..) 일반적으로 finally에 close를 많이 하는게 대다수이지만 JAVA7에서 추가된 try-with-resource를 사용하는것이 좋음. static String firstLineOfFile(String path) throws IOException { BufferedReader br = new BufferedReader(new FileReader(path)); try { return br.readLine(); } finally { br.close(); } } 만약 한번더 오픈을 한다면? static void copy(String src, String dst) throws IOException { InputStream in = new FileInputStream(src); try { OutputStream out = new FileOutputStream(dst); try { byte[] buf = new byte[BUFFER_SIZE]; int n; while ((n = in.read(buf)) &gt;= 0) out.write(buf, 0, n); } finally { out.close(); } } finally { in.close(); } } 이 경우 어떤 문제에 의해서 close에서도 문제가 생기다면? 두번째(close)예외의 메시지만 준다. 그래서 문제 파악을 힘들게 만든다. 아래는 try with resource 로 고친 코드``` java static String firstLineOfFile(String path) throws IOException { try (BufferedReader br = new BufferedReader( new FileReader(path))) { return br.readLine(); }}static void copy(String src, String dst) throws IOException { try (InputStream in = new FileInputStream(src); OutputStream out = new FileOutputStream(dst)) { byte[] buf = new byte[BUFFER_SIZE]; int n; while ((n = in.read(buf)) &gt;= 0) out.write(buf, 0, n); } }```" }, { "title": "이펙티브 자바 3판 - 2. 객체 생성과 파괴 - 1", "url": "/posts/devlog-effective-java-3e-2-1/", "categories": "DevLog, Java", "tags": "Effective Java", "date": "2020-04-25 01:34:00 +0900", "snippet": "이펙티브 자바 3판 - 2. 객체 생성과 파괴 - 1 Item1. 생성자 대신 정적 팩터리 메서드를 고려하자. Item2. 생성자에 매개변수가 많다면 빌더를 고려하라 Item3. private 생성자나 열거 타입으로 싱글턴임을 보증하라. Item4. 인스턴스화를 막으려거든 private 생성자를 사용하라. 객체를 만들어야 할 때와 만들지 말아...", "content": "이펙티브 자바 3판 - 2. 객체 생성과 파괴 - 1 Item1. 생성자 대신 정적 팩터리 메서드를 고려하자. Item2. 생성자에 매개변수가 많다면 빌더를 고려하라 Item3. private 생성자나 열거 타입으로 싱글턴임을 보증하라. Item4. 인스턴스화를 막으려거든 private 생성자를 사용하라. 객체를 만들어야 할 때와 만들지 말아야 할 때를 구분하는 법올바른 객체 생성 방법과 불필요한 생성을 피하는 방법제때 파괴됨을 보장하고 파괴 전에 수행해야 할 정리 작업Item1. 생성자 대신 정적 팩터리 메서드를 고려하자. 클래스는 생성자와 별도로 정적 팩터리 메서드(static factory method)를 제공할 수 있다. public static Boolean valueOf(boolean b) { return b ? Boolean.TRUE : BOolean.FALSE;} 정적 팩터리 메서드가 생성자보다 좋은 장점 5가지 1 이름을 가질 수 있다. BigInteger(int, int, Random)과 정적 팩터리 메서드인 BigInteger.probablePrime 중 ‘어느 쪽이 값이 소수인 BigInteger를 반환한다’는 의미를 더 잘 설명할 것 같은지? 각 목적에 맞는 생성자가 여러개 있다고 했을때, 각각의 생성자가 어떤 역할을 하는지 정확히 기억하기 어려워 엉뚱한 것을 호출하는 실수를 할 수 있다. 2 호출될 때마다 인스턴스를 새로 생성하지는 않아도 된다. 불변 클래스(immutable class)는 인스턴스를 미리 만들어 놓거나 새로 생성한 인스턴스를 캐싱하여 재활용하는 식으로 불필요한 객체 생성을 피할 수 있다. Boolean.valueOf(boolean) 메서드는 객체를 아예 생성하지 않음.(성능 향상에 기여) 반복되는 요청에 같은 객체를 반환하는 식으로 인스턴스를 철저히 통제할 수 있음 - 통제(instance-controlled) 클래스 이렇게 통제하는 이유 싱글턴(singleton)으로 만들수도, 인스턴스화 불가(noninstantiable)로 만들 수 있음 동치인 인스턴스가 단 하나뿐임을 보장 가능(열거타입과 같이) 플라이웨이트 패턴의 근간( 나중에 보는걸로..) 3 반환 타입의 하위 타입 객체를 반환할 수 있는 능력이 있다. 추상화된 구조인 경우 인터페이스를 정적 팩터리 메서드의 반환 타입으로 해서 구현 클래스를 공개하지 않고 객체를 반환할 수 있음. API를 작게 유지할 수 있다. Collection 프레임워크는 45개 클래스를 공개하지 않기 때문에 API 외견을 훨씬 작게 만들수 있었음. 자바8부터는 인터페이스가 정적 메서드를 가질 수 없다는 제한이 풀려서 인스턴스화 불가 동반 클래스를 둘 이유가 별로 없다. 자바9에서는 private 정적 메서드까지 허락하지만, 정적 필드와 정적 멤버 클래스는 여전히 public이어야 한다. 4 입력 매개변수에 따라 매번 다른 클래스의 객체를 반환할 수 있음. EnumSet 클래스는 public 생성자 없이 오직 정적 팩터리만 제공. 원소가 64개 이하면 ㅣong변수 하나로 관리 -&gt; RegularEnumSet 원소가 65개 이상이면 long 배열로 관리하는 JumboEnumSt 클라이언트는 위와 같은 사실을 몰라도 됨 5 정적 팩터리 메서드를 작성하는 시점에는 반환할 객체의 클래스가 존재하지 않아도 된다. 이러한 유연함은 Service Provider 프레임워크의 근간이 된다. 대표적인 예로 JDBC 있다. DriverManager.registerDriver() 메서드로 각 DBMS별 Driver를 설정한다. (제공자 등록 API) DriverManager.getConnection() 메서드로 DB 커넥션 객체를 받는다. (service access API) Connection Interface는 DBMS 별로 동작을 구현하여 사용할 수 있다. (service Interface) 위와 같이 동작하게 된다면 차후에 다른 DBMS가 나오더라도 같은 Interface를 사용하여 기존과 동일하게 사용이 가능하다. 정적 팩터리 메서드 단점 1 상속을 하려면 public이나 protected 생성자가 필요하니 정적 팩터리 메서드만 제공하면 하위 클래스를 만들 수 없다. 상속보단 컴포지션을 사용하도록 유도하고 불변 타입으로 만들려면 이 제약을을 지켜야 하는건 오히려 장점 2 정적 팩터리 메서드는 프로그래머가 찾기 어렵다. 생성자처럼 잘 드러나지 않음. API 문서를 잘 써놓고 메서드 이름도 널리 알려진 규약을 따라 지어야 함. 명명 규칙 설명 예시 from 매개변수를 하나를 받아서 생성 Date d = Date.from(instant) of 여러 매개변수를 받아서 생성 Set&lt;Rank&gt; faceCards = EnumSet.of(JACK, QUEEN, KING); valueOf from과 of의 더 자세한 버전 BigInteger prime = BigInteger.valueOf(Integer.MAX_VALUE); instace 혹은 getInstance 매개변수로 명시한 인스턴스를 반환, 같은 인스턴스 보장(x) StackWalker luke = StackWalker.getInstnace(options); create 혹은 newInstance 매번 새로운 인스턴스를 생성해 반환 Object newArray = Array.newInstace(classObject, arrayLen); get”Tpye” 생성할 클래스가 아닌 다른 클래스에 팩터리 메서드를 정의할 때 FileStore fs = Files.getFileStore(path); new”Type” 매번 새로운 인스턴스를 생성해 반환하지만 다른 클래스에 팩터리 메서드를 정의할 때 BufferedReader br = Files.newBufferedReader(path); “type” getType과 newType의 간결한 버전 List&lt;Complaint&gt; litany = Collections.list(legacyLitany); 핵심 정리 정적 팩터리 메서드와 public 생성자는 각자의 쓰임새가 있으니 상대적인 장단점을 이해하고 사용하는 것이 좋다.Item2. 생성자에 매개변수가 많다면 빌더를 고려하라점층적 생성자 패턴 과거에 주로 사용했던 방식 public NutritionFacts(int servingSize, int servings) { this(servingSize, servings, 0); } public NutritionFacts(int servingSize, int servings, int calories) { this(servingSize, servings, calories, 0); } public NutritionFacts(int servingSize, int servings, int calories, int fat) { this(servingSize, servings, calories, fat, 0); } public NutritionFacts(int servingSize, int servings, int calories, int fat, int sodium) { this(servingSize, servings, calories, fat, sodium, 0); } public NutritionFacts(int servingSize, int servings, int calories, int fat, int sodium, int carbohydrate) { this.servingSize = servingSize; this.servings = servings; this.calories = calories; this.fat = fat; this.sodium = sodium; this.carbohydrate = carbohydrate; } 점층적 생성자 패턴을 쓸수도 있지만, 매개변수 개수가 많아지면 클라이언트 코드를 작성하거나 읽기 어렵다. 타입이 같은 매개변수가 연달아 늘어서 있으면 찾기 어려운 버그로 이어질 수 있다.NutritionFacts cocaCola = new NutritionFacts(240, 8, 100, 0, 35, 27); // 각각의 값이 어떤걸 뜻하는지 한눈에 파악할 수 없음.자바빈즈 패턴(JavaBeans pattern) 매개변수가 없는 생성자로 객체를 만든 후, 세터(setter) 메서드들을 호출해주는 방식 NutritionFacts cocaCola = new NutritionFacts();cocaCola.setServingSize(240);cocaCola.setServings(8);cocaCola.setCalories(100);cocaCola.setSodium(35);cocaCola.setCarbohydrate(27); 자바빈즈 패턴에서는 객체 하나를 만들려면 메서드를 여러개 호출해야 하고, 객체가 완전히 생성되기 전까지는 일관성(consistency)이 무너진 상태에 놓이게 된다. 클래스를 불변으로 만들 수 없는 것도 문제 이러한 단점을 완화하고자 생성이 끝난 객체를 수동으로 freezing해서 변경할 수 없도록 하기도 한다.(그래도 별로)빌더 패턴(Builder pattern) 실제로 실무에서 많이 사용됨 빌더 패턴을 고려한다면 lombok을 사용하자 lombok@Entity@Getter@Builderpublic class Category { @Id @GeneratedValue private int id; @Column private String name; @Column private String icon; private Level level; // @OneToMany(mappedBy = \"id\", cascade = CascadeType.ALL) private List&lt;Category&gt; subCagetoryList;} @Test public void builderTest() { Category.builder() .id(1) .level(Level.FIRST) .name(\"팬션의류/잡화\") .icon(\"패션의류 아이콘.png\") .subCagetoryList(new ArrayList&lt;&gt;()) .build(); } 빌더패턴은 파이썬과 스칼라에 있는 명명된 선택적 매개변수(named optional parameters)를 흉내낸 것. 잘못된 매개변수를 최대한 일찍 발견하려면 빌더의 생성자와 메서드에서 입력 매개변수를 검사하고 build 메소드가 호출하는 생성자에서 여러 매개변에 걸친 불변식(invariant)을 검사하자. lombok을 사용하더라도 bulderMethod를 지정한다던지 해서 검증 가능. 불변(immutable 혹은 immutability)과 불변식(invariant) 불변(immutable) - 어떠한 변경도 허용하지 않겠다는 뜻으로 가변(mutable) 객체와 구분하는 용도로 쓰인다. 대표적으로 String 객체는 한번 생성되면 절대 바꿀 수 없는 불변 객체 불변식(invariant) - 프로그램이 실행되는 동안, 혹은 정해진 기간동안 반드시 만족해야 하는 조건(리스트의 크기는 반드시 0 이상) 가변(mutable) 객체에서도 불변식(invariant)가 존재할 수 있음. 계층적으로 설계된 클래스와 함께 쓰기에도 좋음. Builder도 상속해서 처리하는데 lombok을 사용하는 것이 더 좋은 방식이라고 생각됨. @NoArgsConstructor@Getter@Builderpublic class BaseFashionItem implements FashionItem { private String brand; private Category category; private String name; private long price;}public class Shirt extends BaseFashionItem { private ColorType colorType; private SizeType sizeType; @Builder public Shirt(String brand, Category category, String name, long price, ColorType colorType, SizeType sizeType) { super(brand, category, name, price); this.colorType = colorType; this.sizeType = sizeType; }} @Test public void builderTest() { Shirt shirt = Shirt.builder() .category(Category.builder().build()) .brand(\"brand\") .name(\"name\") .price(15000) .colorType(ColorType.BLACK) .sizeType(SizeType.M) .build(); } 빌더 패턴의 단점 객체를 만들려면 그에 앞서 빌더부터 만들어야 하는 점.(성능에 민감한 상황에서는 문제가 될 수도 있다.) API는 시간이 지날수록 매개변ㄴ수가 많아지기 때문에 애초에 빌더를 고려하는 편이 나을 때가 많다.Item3. private 생성자나 열거 타입으로 싱글턴임을 보증하라.싱글턴이란? 인스턴스를 오직 하나만 생성할 수 있는 클래스 싱글턴의 전형적인 예로는 함수와 같은 무상태(stateless) 개체나 설계상 유일해야 하는 시스템 컴포넌트를 들수 있다. spring bean 객체를 보통 singleton scope를 이용해서 사용함. 클래스를 싱글턴으로 만들면 이를 사용하는 클라이언트를 테스트하기가 어려워 짐. 싱글턴 인스턴스를 mock 구현으로 대체할 수 없음. 인터페이스를 구현해서 만든 싱글턴이면 가능 private 생성자를 만들면 클라이언트에서는 객체 생성 할수 있는 방법이 존재하지 않아 인스턴스가 전체에서 하나뿐임을 보장할 수 있음. 리플렉션 API를 이용해서 호출 가능한데, 이를 방어하기 위해서 생성자에서 두번째 객체가 생성되려 할떄 예외를 던지게 하면 됨.싱글턴을 만드는 방법 1 public static final 필드 방식 싱글턴임이 API에 명백히 드러나고, 간결하다는 장점이 있음. public class Elvis {public static final Elvis INSTANCE = new Elvis();private Elvis() { }public void leaveTheBuilding() { System.out.println(\"Whoa baby, I'm outta here!\");}// 이 메서드는 보통 클래스 바깥(다른 클래스)에 작성해야 한다!public static void main(String[] args) { Elvis elvis = Elvis.INSTANCE; elvis.leaveTheBuilding();}} 2 정적 팩터리 메서드 방식 API를 바꾸지 않고도 싱글턴이 아니게 변경할 수 있음.(getInstnace 메소드를 변경하면 됨) 정적 팩터리를 제네릭 싱글턴 팩터리로 변경할 수 있음. 메서드 참조를 공급자로 사용할 수 있음. public class Elvis {private static final Elvis INSTANCE = new Elvis();private Elvis() { }public static Elvis getInstance() { return INSTANCE; }public void leaveTheBuilding() { System.out.println(\"Whoa baby, I'm outta here!\");}// 이 메서드는 보통 클래스 바깥(다른 클래스)에 작성해야 한다!public static void main(String[] args) { Elvis elvis = Elvis.getInstance(); elvis.leaveTheBuilding();}} 3 Enum 더 간결하고, 추가 노력 없이 직렬화 할 수 있음. 복잡한 직렬화 상황, 리플렉션 공격에서도 완벽히 방어 대부분 상황에서는 원소가 하나뿐인 Enum이 싱글턴을 만드는 가장 좋은 방법 Enum에서 Enum을 참조하는 경우 순환 참조에 주의 실글턴 클래스를 직렬화하는 방법 모든 인스턴스 필드를 일시적(transient)이라고 선언하고 readResolve 메소드를 제공(Item 89) private Object readResolve() {return INSTNACE;} Item4. 인스턴스화를 막으려거든 private 생성자를 사용하라. 정적 메서드와 정적 필드만을 담은 클래스를 만들고 싶을때가 있다. 객체지향적이지는 않지만 나름의 쓰임새가 있음.(ex java.lang.Math, Utils 시리즈) 추상클래스를 만들어서 인스턴스화를 금지하는 것은 안됨(이렇게 해본적도 없음.) private 생성자를 추가해서 클래스의 인스턴스화를 막자. lombok을 이용하면 편함. @NoArgsConstructor(access = AccessLevel.PRIVATE)public class ContentLanguageConfig {public static final List&lt;Language&gt; SERVICE_LIST;public static final List&lt;Language&gt; EMAIL_PUSH_SUPPORT_LIST;public static final List&lt;Language&gt; EMAIL_JOIN_SUPPORT_LIST;static { SERVICE_LIST = Arrays.asList(Language.ENGLISH, Language.SIMPLIFIED_CHINESE, Language.TRADITIONAL_CHINESE, Language.THAI, Language.INDONESIAN, Language.JAPANESE); EMAIL_PUSH_SUPPORT_LIST = Arrays.asList(Language.ENGLISH, Language.SIMPLIFIED_CHINESE, Language.TRADITIONAL_CHINESE, Language.JAPANESE); EMAIL_JOIN_SUPPORT_LIST = Arrays.asList(Language.ENGLISH, Language.SIMPLIFIED_CHINESE, Language.TRADITIONAL_CHINESE, Language.JAPANESE);}/** * 서비스중인 언어인지 확인 * @param language * @return */public static boolean isService(Language language) { return SERVICE_LIST.contains(language);}} 생성자 내에서 Exception을 던지도록 한다면 주석을 달아두자 이 방식을 적용했을때 상속을 불가능하게 하는 효과도 가져옴.Comment 위에 있는 내용들은 어쩌보면 자바의 기본적인 내용임에도 불구하고 실제로는 놓치는 경우가 많을수 있어서 해당 작업을 직접 해보는 것이 좋다. sonarqube 정적 분석을 하는 경우에 Util Class에서 Instance화를 하지 않도록 가이드 해주기도 하므로 Sonarqube 같은 정적 분석툴을 도입하고 분석결과를 잘 적용하는것도 코드 퀄리티를 향상시키는데 도움이 된다." }, { "title": "Hello World", "url": "/posts/essay-1/", "categories": "Essay", "tags": "Hello World, Test", "date": "2020-04-24 08:32:00 +0900", "snippet": "Hello Worldprintln(\"hello world\")", "content": "Hello Worldprintln(\"hello world\")" } ]
